<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="http://www.streamxhub.com/streamx-docs/rss.xml" rel="self" type="application/rss+xml"/>
    <title>StreamX</title>
    <link>http://www.streamxhub.com/streamx-docs/</link>
    <description>StreamX 是一个 Apache Flink &amp; Spark 极速开发框架,项目的初衷是 ——让 Flink &amp; Spark 更简单。其定位是 Flink &amp; Spark 开发脚手架 + 流批一体大数据平台,一站式大数据平台,Flink 大数据平台,Flink 可视化数据平台, LowCode 平台, 流批一体, 一站式, Flink, Spark, FlinkSQL, Application, sqlSubmit, SpringBoot, Vue, flink-connectors</description>
    <language>en-US</language>
    <pubDate>Wed, 22 Dec 2021 00:46:10 GMT</pubDate>
    <lastBuildDate>Wed, 22 Dec 2021 00:46:10 GMT</lastBuildDate>
    <generator>@mr-hope/vuepress-plugin-feed</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <copyright>Copyright by benjobs</copyright>
    <item>
      <title>Document</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Document</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
    </item>
    <item>
      <title>高级扩展</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/advanced/advanced/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/advanced/advanced/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">高级扩展</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Clickhouse Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/clickhouse/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/clickhouse/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Clickhouse Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Elasticsearch Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/es/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/es/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Elasticsearch Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Http Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/http/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/http/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Http Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Jdbc Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/jdbc/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/jdbc/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Jdbc Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>Flink 官方 提供了<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/jdbc.html" target="_blank" rel="noopener noreferrer">JDBC</a>的连接器,用于从 JDBC 中读取或者向其中写入数据,可提供 <mark>AT_LEAST_ONCE</mark> (至少一次)的处理语义</p>
<p><code>StreamX</code>中基于两阶段提交实现了 <mark>EXACTLY_ONCE</mark> (精确一次)语义的<code>JdbcSink</code>,并且采用<a href="https://github.com/brettwooldridge/HikariCP" target="_blank" rel="noopener noreferrer"><code>光 HikariCP</code></a>为连接池,让数据的读取和写入更简单更准确</p>
<h2 id="jdbc-信息配置"> Jdbc 信息配置</h2>
<p>在<code>StreamX</code>中<code>Jdbc Connector</code>的实现用到了<a href="https://github.com/brettwooldridge/HikariCP" target="_blank" rel="noopener noreferrer"><code>光 HikariCP</code></a>连接池,相关的配置在<code>jdbc</code>的namespace下,约定的配置如下:</p>
<div><pre><code><span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>connectionTimeout</span><span>:</span> <span>30000</span>
  <span>idleTimeout</span><span>:</span> <span>30000</span>
  <span>maxLifetime</span><span>:</span> <span>30000</span>
  <span>maximumPoolSize</span><span>:</span> <span>6</span>
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><h3 id="semantic-语义配置"> semantic 语义配置</h3>
<p><code>semantic</code>这个参数是在<code>JdbcSink</code>写时候的语义,仅对 <mark><code>JdbcSink</code></mark> 有效,<code>JdbcSource</code>会自动屏蔽该参数,有三个可选项</p>
<div>
<ul>
<li>EXACTLY_ONCE</li>
<li>AT_LEAST_ONCE</li>
<li>NONE</li>
</ul>
</div>
<h4 id="exactly-once"> EXACTLY_ONCE</h4>
<p>如果<code>JdbcSink</code>配置了 <code>EXACTLY_ONCE</code>语义,则底层采用了两阶段提交的实现方式来完成写入,此时要flink配合开启<code>Checkpointing</code>才会生效,如何开启checkpoint请参考第二章关于<a href="">checkpoint</a>配置部分</p>
<h4 id="at-least-once-none"> AT_LEAST_ONCE &amp;&amp; NONE</h4>
<p>默认不指定会采用<code>NONE</code>语义,这两种配置效果一样,都是保证 <mark>至少一次</mark> 语义</p>
<div><p>提示</p>
<p>开启<code>EXACTLY_ONCE</code>精确一次的好处是显而易见的,保证了数据的准确性,但成本也是高昂的,需要<code>checkpoint</code>的支持,底层模拟了事务的提交读,对实时性有一定的损耗,如果你的业务对数据的准确性要求不是那么高,则建议采用<code>AT_LEAST_ONCE</code>语义</p>
</div>
<h3 id="其他配置"> 其他配置</h3>
<p>除了特殊的<code>semantic</code> 配置项之外,其他的所有的配置都必须遵守 <mark><code>光 HikariCP</code></mark> 连接池的配置,具体可配置项和各个参数的作用请参考<code>光 HikariCP</code><a href="https://github.com/brettwooldridge/HikariCP#gear-configuration-knobs-baby" target="_blank" rel="noopener noreferrer">官网文档</a>.</p>
<h2 id="jdbc-读取数据"> Jdbc 读取数据</h2>
<p>在<code>StreamX</code>中<code>JdbcSource</code>用来读取数据,并且根据数据的<code>offset</code>做到数据读时可回放,我们看看具体如何用<code>JdbcSource</code>读取数据,假如需求如下</p>
<div>
<ul>
<li>从<code>t_order</code>表中读取数据,以<code>timestamp</code>字段为参照,起始值为<code>2020-12-16 12:00:00</code>往后抽取数据</li>
<li>将读取到的数据构造成<code>Order</code>对象返回</li>
</ul>
</div>
<p>jdbc配置和读取代码如下</p>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>jdbc</span><span>:</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>JdbcSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> MySQLSourceApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>

    JdbcSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span>Order<span>]</span><span>(</span>lastOne <span>=></span> <span>{</span>
      <span>//防止抽取过于密集,间隔5秒抽取一次数据                          </span>
      Thread<span>.</span>sleep<span>(</span><span>5000</span><span>)</span><span>;</span>
      <span>val</span> laseOffset <span>=</span> <span>if</span> <span>(</span>lastOne <span>==</span> <span>null</span><span>)</span> <span>"2020-12-16 12:00:00"</span> <span>else</span> lastOne<span>.</span>timestamp
      s<span>"select * from t_order where timestamp > '$laseOffset' order by timestamp asc "</span>
    <span>}</span><span>,</span>
      _<span>.</span>map<span>(</span>x <span>=></span> <span>new</span> Order<span>(</span>x<span>(</span><span>"market_id"</span><span>)</span><span>.</span>toString<span>,</span> x<span>(</span><span>"timestamp"</span><span>)</span><span>.</span>toString<span>)</span><span>)</span>
    <span>)</span><span>.</span>print<span>(</span><span>)</span>

  <span>}</span>

<span>}</span>

<span>class</span> Order<span>(</span><span>val</span> marketId<span>:</span> <span>String</span><span>,</span> <span>val</span> timestamp<span>:</span> <span>String</span><span>)</span> <span>extends</span> Serializable
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>SQLQueryFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>SQLResultFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>JdbcSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>
<span>import</span> <span>java<span>.</span>util<span>.</span></span><span>ArrayList</span><span>;</span>
<span>import</span> <span>java<span>.</span>util<span>.</span></span><span>List</span><span>;</span>

<span>public</span> <span>class</span> <span>MySQLJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>new</span> <span>JdbcSource</span><span><span>&lt;</span><span>Order</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span>
                        <span>(</span><span>SQLQueryFunction</span><span><span>&lt;</span><span>Order</span><span>></span></span><span>)</span> lastOne <span>-></span> <span>{</span>
                            <span>//防止抽取过于密集,间隔5秒抽取一次数据                          </span>
                            <span>Thread</span><span>.</span><span>sleep</span><span>(</span><span>5000</span><span>)</span><span>;</span>
                            
                            <span>Serializable</span> lastOffset <span>=</span> lastOne <span>==</span> <span>null</span> 
                            <span>?</span> <span>"2020-12-16 12:00:00"</span> 
                            <span>:</span> lastOne<span>.</span>timestamp<span>;</span>
                            
                            <span>return</span> <span>String</span><span>.</span><span>format</span><span>(</span>
                                <span>"select * from t_order "</span> <span>+</span>
                                <span>"where timestamp > '%s' "</span> <span>+</span>
                                <span>"order by timestamp asc "</span><span>,</span>
                                lastOffset
                            <span>)</span><span>;</span>
                        <span>}</span><span>,</span>
                        <span>(</span><span>SQLResultFunction</span><span><span>&lt;</span><span>Order</span><span>></span></span><span>)</span> iterable <span>-></span> <span>{</span>
                            <span>List</span><span><span>&lt;</span><span>Order</span><span>></span></span> result <span>=</span> <span>new</span> <span>ArrayList</span><span><span>&lt;</span><span>></span></span><span>(</span><span>)</span><span>;</span>
                            iterable<span>.</span><span>forEach</span><span>(</span>item <span>-></span> <span>{</span>
                                <span>Order</span> <span>Order</span> <span>=</span> <span>new</span> <span>Order</span><span>(</span><span>)</span><span>;</span>
                                <span>Order</span><span>.</span>marketId <span>=</span> item<span>.</span><span>get</span><span>(</span><span>"market_id"</span><span>)</span><span>.</span><span>toString</span><span>(</span><span>)</span><span>;</span>
                                <span>Order</span><span>.</span>timestamp <span>=</span> <span>Long</span><span>.</span><span>parseLong</span><span>(</span>item<span>.</span><span>get</span><span>(</span><span>"timestamp"</span><span>)</span><span>.</span><span>toString</span><span>(</span><span>)</span><span>)</span><span>;</span>
                                result<span>.</span><span>add</span><span>(</span><span>Order</span><span>)</span><span>;</span>
                            <span>}</span><span>)</span><span>;</span>
                            <span>return</span> result<span>;</span>
                        <span>}</span><span>)</span>
                <span>.</span><span>returns</span><span>(</span><span>TypeInformation</span><span>.</span><span>of</span><span>(</span><span>Order</span><span>.</span><span>class</span><span>)</span><span>)</span>
                <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p>以<code>java</code> api为例,这里要传入两个参数</p>
<div>
<ul>
<li><code>SQLQueryFunction&lt;T&gt; queryFunc</code></li>
<li><code>SQLResultFunction&lt;T&gt; resultFunc</code></li>
</ul>
</div>
<h3 id="queryfunc获取一条sql"> queryFunc获取一条sql</h3>
<p><code>queryFunc</code>是要传入一个<code>SQLQueryFunction</code>类型的<code>function</code>,该<code>function</code>用于获取查询sql的,会将最后一条记录返回给开发者,然后需要开发者根据最后一条记录返回一条新的查询<code>sql</code>,<code>queryFunc</code>定义如下:</p>
<div><pre><code><span>/**
 * @author benjobs
 */</span>
<span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>SQLQueryFunction</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>extends</span> <span>Serializable</span> <span>{</span>
    <span>/**
     * 获取要查询的SQL
     *
     * @return
     * @throws Exception
     */</span>
    <span>String</span> <span>query</span><span>(</span><span>T</span> last<span>)</span> <span>throws</span> <span>Exception</span><span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><p>所以上面的代码中,第一次上来<code>lastOne</code>(最后一条记录)为null,会判断一下,为null则取需求里默认的<code>offset</code>,查询的sql里根据<code>timestamp</code>字段正序排,这样在第一次查询之后,会返回最后的那条记录,下次直接可以使用这条记录作为下一次查询的根据</p>
<div><p>注意事项</p>
<p><code>JdbcSource</code>实现了<code>CheckpointedFunction</code>,即当程序开启 <mark><code>checkpoint</code></mark> 后,会将这些诸如<code>laseOffset</code>的状态数据保存到<code>state backend</code>,这样程序挂了,再次启动会自动从<code>checkpoint</code>中恢复<code>offset</code>,会接着上次的位置继续读取数据,
一般在生产环境,更灵活的方式是将<code>lastOffset</code>写入如<code>redis</code>等存储中,每次查询完之后再将最后的记录更新到<code>redis</code>,这样即便程序意外挂了,再次启动,也可以从<code>redis</code>中获取到最后的<code>offset</code>进行数据的抽取,也可以很方便的人为的任意调整这个<code>offset</code>进行数据的回放</p>
</div>
<h3 id="resultfunc-处理查询到的数据"> resultFunc 处理查询到的数据</h3>
<p><code>resultFunc</code>的参数类型是<code>SQLResultFunction&lt;T&gt;</code>,是将一个查询到的结果集放到<code>Iterable&lt;Map&lt;String, ?&gt;&gt;</code>中返回给开发者,可以看到返回了一个迭代器<code>Iterable</code>,迭代器每次迭代返回一个<code>Map</code>,该<code>Map</code>里记录了一行完整的记录,<code>Map</code>的<code>key</code>为查询字段,<code>value</code>为值,<code>SQLResultFunction&lt;T&gt;</code>定义如下</p>
<div><pre><code><span>/**
 * @author benjobs
 */</span>
<span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>SQLResultFunction</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>extends</span> <span>Serializable</span> <span>{</span>
    <span>/**
     * 将查下结果以Iterable&lt;Map>的方式返回,开发者去实现转成对象.
     *
     * @param map
     * @return
     */</span>
    <span>Iterable</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>result</span><span>(</span><span>Iterable</span><span><span>&lt;</span><span>Map</span><span>&lt;</span><span>String</span><span>,</span> <span>?</span><span>></span><span>></span></span> iterable<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><h2 id="jdbc-读取写入"> Jdbc 读取写入</h2>
<p><code>StreamX</code>中<code>JdbcSink</code>是用来写入数据,我们看看具体如何用<code>JdbcSink</code>写入数据,假如需求是需要从<code>kakfa</code>中读取数据,写入到<code>mysql</code></p>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>pattern</span><span>:</span> user
  <span>group.id</span><span>:</span> user_02
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>...</span>
  
<span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><div><p>注意事项</p>
<p>配置里<code>jdbc</code>下的 <mark><code>semantic</code></mark> 是写入的语义,在上面有介绍,该配置只会在<code>JdbcSink</code>下生效,<code>StreamX</code>中基于两阶段提交实现了 <mark>EXACTLY_ONCE</mark> 语义,
这本身需要被操作的数据库(<code>mysql</code>,<code>oracle</code>,<code>MariaDB</code>,<code>MS SQL Server</code>)等支持事务,理论上所有支持标准Jdbc事务的数据库都可以做到EXACTLY_ONCE(精确一次)的写入</p>
</div>
</CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span>TypeInformation
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span>TypeExtractor<span>.</span>getForClass
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span>KafkaDeserializationSchema

<span>object</span> JdbcSinkApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span>
          <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
          <span>.</span>map<span>(</span>x <span>=></span> JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>x<span>.</span>value<span>)</span><span>)</span>
          
        JdbcSink<span>(</span><span>)</span><span>.</span>sink<span>[</span>User<span>]</span><span>(</span>source<span>)</span><span>(</span>user <span>=></span>
          s<span>"""
          |insert into t_user(`name`,`age`,`gender`,`address`)
          |value('${user.name}',${user.age},${user.gender},'${user.address}')
          |"""</span><span>.</span>stripMargin
        <span>)</span>  
  <span>}</span>

<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span><span>String</span><span>,</span>age<span>:</span><span>Int</span><span>,</span>gender<span>:</span><span>Int</span><span>,</span>address<span>:</span><span>String</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span><span>KafkaDeserializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span><span>ConsumerRecord</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>import</span> <span>static</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span><span>TypeExtractor</span><span>.</span>getForClass<span>;</span>

<span>public</span> <span>class</span> <span>JdbcSinkJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>

        <span>DataStream</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span>
                    mapper<span>.</span><span>readValue</span><span>(</span>value<span>.</span><span>value</span><span>(</span><span>)</span><span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>)</span><span>;</span>

        <span>new</span> <span>JdbcSink</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>sql</span><span>(</span><span>(</span><span>SQLFromFunction</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> <span>JavaUser</span><span>::</span><span>toSql</span><span>)</span>
                <span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>
                
        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
    <span>public</span> <span>String</span> <span>toSql</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>format</span><span>(</span>
                <span>"insert into t_user(`name`,`age`,`gender`,`address`) value('%s',%d,%d,'%s')"</span><span>,</span>
                name<span>,</span>
                age<span>,</span>
                gender<span>,</span>
                address<span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="根据数据流生成目标sql"> 根据数据流生成目标SQL</h3>
<p>在写入的时候,需要知道具体写入的<code>sql</code>语句,该<code>sql</code>语句需要开发者通过<code>function</code>的方式提供,在<code>scala</code> api中,直接在<code>sink</code>方法后跟上<code>function</code>即可,<code>java</code> api 则是通过<code>sql()</code>方法传入一个<code>SQLFromFunction</code>类型的<code>function</code></p>
<p>下面以<code>java</code> api为例说明,我们来看看<code>java</code>api 中提供sql的<code>function</code>方法的定义</p>
<div><pre><code><span>/**
 * @author benjobs
 */</span>
<span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>SQLFromFunction</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>extends</span> <span>Serializable</span> <span>{</span>
    <span>/**
     * @param bean
     * @return
     */</span>
    <span>String</span> <span>from</span><span>(</span><span>T</span> bean<span>)</span><span>;</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><p><code>SQLFromFunction</code>上的泛型<code>&lt;T&gt;</code>即为<code>DataStream</code>里实际的数据类型,该<code>function</code>里有一个方法<code>form(T bean)</code>,这个<code>bean</code>即为当前<code>DataStream</code>中的一条具体数据,会将该数据返给开发者,开发者来决定基于这条数据,生成一条具体可以往数据库中插入的<code>sql</code></p>
<h3 id="设置写入批次大小"> 设置写入批次大小</h3>
<p>在 非 <code>EXACTLY_ONCE</code>(精确一次的语义下)可以适当的设置<code>batch.size</code>来提高Jdbc写入的性能(前提是业务允许的情况下),具体配置如下</p>
<div><pre><code><span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
  <span>batch.size</span><span>:</span> <span>1000</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><p>这样一来就不是来一条数据就立即写入,而是积攒一个匹配然后执行批量插入</p>
<div><p>注意事项</p>
<p>这个设置仅在非<code>EXACTLY_ONCE</code>语义下生效,带来的好处是可以提高Jdbc写入的性能,一次大批量的插入数据,缺点是数据写入势必会有延迟,请根据实际使用情况谨慎使用</p>
</div>
<h2 id="多实例jdbc支持"> 多实例Jdbc支持</h2>
<h2 id="手动指定jdbc连接信息"> 手动指定Jdbc连接信息</h2>
]]></content:encoded>
    </item>
    <item>
      <title>Apache Kafka Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/kafka/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/kafka/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Apache Kafka Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html" target="_blank" rel="noopener noreferrer">Flink 官方</a>提供了<a href="http://kafka.apache.org" target="_blank" rel="noopener noreferrer">Apache Kafka</a>的连接器,用于从 Kafka topic 中读取或者向其中写入数据,可提供 <mark>精确一次</mark> 的处理语义</p>
<p><code>StreamX</code>中<code>KafkaSource</code>和<code>KafkaSink</code>基于官网的<code>kafka connector</code>进一步封装,屏蔽很多细节,简化开发步骤,让数据的读取和写入更简单</p>
<h2 id="依赖"> 依赖</h2>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html" target="_blank" rel="noopener noreferrer">Apache Flink</a> 集成了通用的 Kafka 连接器，它会尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。 当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker。 有关 Kafka 兼容性的更多细节，请参考 <a href="https://kafka.apache.org/protocol.html#protocol_compatibility" target="_blank" rel="noopener noreferrer">Kafka</a> 官方文档。</p>
<div><pre><code>    <span>&lt;!--必须要导入的依赖--></span>
    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>com.streamxhub.streamx<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>Streamx-flink-core<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${project.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

    <span>&lt;!--flink-connector--></span>
    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-connector-kafka_2.11<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></div></div><p>同时在开发阶段,以下的依赖也是必要的</p>
<div><pre><code>    <span>&lt;!--以下scope为provided的依赖也是必须要导入的--></span>
    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-scala_${scala.binary.version}<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${flink.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
        <span><span><span>&lt;</span>scope</span><span>></span></span>provided<span><span><span>&lt;/</span>scope</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-clients_${scala.binary.version}<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${flink.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
        <span><span><span>&lt;</span>scope</span><span>></span></span>provided<span><span><span>&lt;/</span>scope</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-streaming-scala_${scala.binary.version}<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${flink.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
        <span><span><span>&lt;</span>scope</span><span>></span></span>provided<span><span><span>&lt;/</span>scope</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div><h2 id="kafka-source-consumer"> Kafka Source (Consumer)</h2>
<p>先介绍基于官网的标准的kafka consumer的方式,以下代码摘自<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html#kafka-consumer" target="_blank" rel="noopener noreferrer">官网文档</a></p>
<div><pre><code><span>val</span> properties <span>=</span> <span>new</span> Properties<span>(</span><span>)</span>
properties<span>.</span>setProperty<span>(</span><span>"bootstrap.servers"</span><span>,</span> <span>"localhost:9092"</span><span>)</span>
properties<span>.</span>setProperty<span>(</span><span>"group.id"</span><span>,</span> <span>"test"</span><span>)</span>
<span>val</span> stream <span>=</span> env<span>.</span>addSource<span>(</span><span>new</span> FlinkKafkaConsumer<span>[</span><span>String</span><span>]</span><span>(</span><span>"topic"</span><span>,</span> <span>new</span> SimpleStringSchema<span>(</span><span>)</span><span>,</span> properties<span>)</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>可以看到一上来定义了一堆kafka的连接信息,这种方式各项参数都是硬编码的方式写死的,非常的不灵敏,下面我们来看看如何用<code>StreamX</code>接入 <code>kafka</code>的数据,只需要按照规定的格式定义好配置文件然后编写代码即可,配置和代码如下</p>
<h3 id="基础消费示例"> 基础消费示例</h3>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> test_user
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest
  <span>enable.auto.commit</span><span>:</span> <span>true</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><div><p>注意事项</p>
<p><code>kafka.source</code>这个前缀是固定的,kafka properties相关的参数必须遵守<a href="http://kafka.apache.org" target="_blank" rel="noopener noreferrer">kafka官网</a>对参数key的设置规范</p>
</div>
</CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>quickstart</span>

<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> kafkaSourceApp <span>extends</span> FlinkStreaming <span>{</span>

    <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
        print<span>(</span>source<span>)</span>
    <span>}</span>

<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>api<span>.</span>datastream<span>.</span></span><span>DataStream</span><span>;</span>

<span>public</span> <span>class</span> <span>KafkaSimpleJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>String</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span><span>;</span>

        source<span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="高级配置参数"> 高级配置参数</h3>
<p><code>KafkaSource</code>是基于Flink Kafka Connector封装一个更简单的kafka读取类,构造方法里需要传入<code>StreamingContext</code>,当程序启动时传入配置文件即可,框架会自动解析配置文件,在<code>new KafkaSource</code>的时候会自动的从配置文件中获取相关信息,初始化并返回一个Kafka Consumer,在这里topic下只配置了一个topic,因此在消费的时候不用指定topic直接默认获取这个topic来消费, 这只是一个最简单的例子,更多更复杂的规则和读取操作则要通过<code>.getDataStream()</code>在该方法里传入参数才能实现
我们看看<code>getDataStream</code>这个方法的签名</p>
<div><pre><code><span>def</span> getDataStream<span>[</span>T<span>:</span> TypeInformation<span>]</span><span>(</span>topic<span>:</span> java<span>.</span>io<span>.</span>Serializable <span>=</span> <span>null</span><span>,</span>
    alias<span>:</span> <span>String</span> <span>=</span> <span>""</span><span>,</span>
    deserializer<span>:</span> KafkaDeserializationSchema<span>[</span>T<span>]</span><span>,</span>
    strategy<span>:</span> WatermarkStrategy<span>[</span>KafkaRecord<span>[</span>T<span>]</span><span>]</span> <span>=</span> <span>null</span>
<span>)</span><span>:</span> DataStream<span>[</span>KafkaRecord<span>[</span>T<span>]</span><span>]</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>参数具体作用如下</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">参数类型</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>topic</code></td>
<td style="text-align:left">Serializable</td>
<td style="text-align:left">一个topic或者一组topic</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left"><code>alias</code></td>
<td style="text-align:left">String</td>
<td style="text-align:left">用于区别不同的kafka实例</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left"><code>deserializer</code></td>
<td style="text-align:left">DeserializationSchema</td>
<td style="text-align:left">topic里数据的具体解析类</td>
<td style="text-align:left">KafkaStringDeserializationSchema</td>
</tr>
<tr>
<td style="text-align:left"><code>strategy</code></td>
<td style="text-align:left">WatermarkStrategy</td>
<td style="text-align:left">watermark生成策略</td>
<td style="text-align:left">无</td>
</tr>
</tbody>
</table>
<p>下面我们来看看更多的使用和配置方式</p>
<div>
<ul>
<li>消费多个Kafka实例</li>
<li>消费多个Topic</li>
<li>Topic动态发现</li>
<li>从指定Offset消费</li>
<li>指定KafkaDeserializationSchema</li>
<li>指定WatermarkStrategy</li>
</ul>
</div>
<h3 id="消费多个kafka实例"> 消费多个Kafka实例</h3>
<p>在框架开发之初就考虑到了多个不同实例的kafka的配置情况.如何来统一配置,并且规范格式呢?在streamx中是这么解决的,假如我们要同时消费两个不同实例的kafka,配置文件定义如下,
可以看到在<code>kafka.source</code>下直接放kafka的实例名称(名字可以任意),在这里我们统一称为 <mark><code>alias</code></mark> , <mark><code>alias</code></mark> 必须是唯一的,来区别不同的实例,然后别的参数还是按照之前的规范,
统统放到当前这个实例的namespace下即可.如果只有一个kafka实例,则可以不用配置<code>alias</code>
在写代码消费时注意指定对应的 <mark><code>alias</code></mark> 即可,配置和代码如下</p>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>kafka1</span><span>:</span>
    <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
    <span>topic</span><span>:</span> test_user
    <span>group.id</span><span>:</span> user_01
    <span>auto.offset.reset</span><span>:</span> earliest
    <span>enable.auto.commit</span><span>:</span> <span>true</span>
  <span>kafka2</span><span>:</span>
    <span>bootstrap.servers</span><span>:</span> kfk4<span>:</span><span>9092</span><span>,</span>kfk5<span>:</span><span>9092</span><span>,</span>kfk6<span>:</span><span>9092</span>
    <span>topic</span><span>:</span> kafka2
    <span>group.id</span><span>:</span> kafka2
    <span>auto.offset.reset</span><span>:</span> earliest
    <span>enable.auto.commit</span><span>:</span> <span>true</span>    
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>//消费kafka1实例的数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>alias <span>=</span> <span>"kafka1"</span><span>)</span>
  <span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>print<span>(</span><span>)</span>
  
<span>//消费kafka2实例的数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>alias <span>=</span> <span>"kafka2"</span><span>)</span>
  <span>.</span>uid<span>(</span><span>"kfkSource2"</span><span>)</span>
  <span>.</span>name<span>(</span><span>"kfkSource2"</span><span>)</span>
  <span>.</span>print<span>(</span><span>)</span>  
  
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
<span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>

<span>//消费kafka1实例的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source1 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>alias</span><span>(</span><span>"kafka1"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>print</span><span>(</span><span>)</span><span>;</span>  

<span>//消费kafka1实例的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source2 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>alias</span><span>(</span><span>"kafka2"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span> 
            
context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>            
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></div></div><div><p>特别注意</p>
<p>java api在编写代码时,一定要将<code>alias</code>等这些参数的设置放到调用<code>.getDataStream()</code>之前</p>
</div>
</CodeGroupItem>
</CodeGroup>
<h3 id="消费多个topic"> 消费多个Topic</h3>
<p>配置消费多个topic也很简单,在配置文件<code>topic</code>下配置多个topic名称即可,用<code>,</code>或空格分隔,代码消费处理的时候指定topic参数即可,<code>scala</code> api下如果是消费一个topic,则直接传入topic名称即可,如果要消费多个,传入一个<code>List</code>即可
<code>java</code>api通过 <code>topic()</code>方法传入要消费topic的名称,是一个String类型的可变参数,可以传入一个或多个<code>topic</code>名称,配置和代码如下</p>
<CodeGroup>
<CodeGroupItem title="配置">
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> topic1<span>,</span>topic2<span>,</span>topic3<span>...</span>
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>...</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>//消费指定单个topic的数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>topic <span>=</span> <span>"topic1"</span><span>)</span>
  <span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>print<span>(</span><span>)</span>

<span>//消费一批topic数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>topic <span>=</span> List<span>(</span><span>"topic1"</span><span>,</span><span>"topic2"</span><span>,</span><span>"topic3"</span><span>)</span><span>)</span>
<span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>print<span>(</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>//消费指定单个topic的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source1 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>topic</span><span>(</span><span>"topic1"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>
        
<span>//消费一组topic的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source1 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>topic</span><span>(</span><span>"topic1"</span><span>,</span><span>"topic2"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>     
        
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>提示</p>
<p><code>topic</code>支持配置多个<code>topic</code>实例,每个<code>topic</code>直接用<code>,</code>分隔或者空格分隔,如果topic下配置多个实例,在消费的时必须指定具体的topic名称</p>
</div>
<h3 id="topic-发现"> Topic 发现</h3>
<p>关于kafka的分区动态,默认情况下，是禁用了分区发现的。若要启用它，请在提供的属性配置中为 <code>flink.partition-discovery.interval-millis</code> 设置大于 <code>0</code>,表示发现分区的间隔是以毫秒为单位的
更多详情请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#partition-discovery" target="_blank" rel="noopener noreferrer">官网文档</a></p>
<p>Flink Kafka Consumer 还能够使用正则表达式基于 Topic 名称的模式匹配来发现 Topic,详情请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#topic-discovery" target="_blank" rel="noopener noreferrer">官网文档</a>
在<code>StreamX</code>中提供更简单的方式,具体需要在 <code>pattern</code>下配置要匹配的<code>topic</code>实例名称的正则即可</p>
<CodeGroup>
<CodeGroupItem title="配置">
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>pattern</span><span>:</span> ^topic<span>[</span>1<span>-</span><span>9</span><span>]</span>
  <span>group.id</span><span>:</span> user_02
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>...</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>//消费正则topic数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>topic <span>=</span> <span>"topic-a"</span><span>)</span>
<span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>print<span>(</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
<span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>

<span>//消费通配符topic数据</span>
<span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>topic</span><span>(</span><span>"topic-a"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>              
    
context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>         
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>特别注意</p>
<p><code>topic</code>和<code>pattern</code>不能同时配置,当配置了<code>pattern</code>正则匹配时,在消费的时候依然可以指定一个确定的<code>topic</code>名称,此时会检查<code>pattern</code>是否匹配当前的<code>topic</code>,如不匹配则会报错</p>
</div>
<h3 id="配置开始消费的位置"> 配置开始消费的位置</h3>
<p>Flink Kafka Consumer 允许通过配置来确定 Kafka 分区的起始位置,<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#kafka-consumers-start-position-configuration" target="_blank" rel="noopener noreferrer">官网文档</a>Kafka 分区的起始位置具体操作方式如下</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>val</span> env <span>=</span> StreamExecutionEnvironment<span>.</span>getExecutionEnvironment<span>(</span><span>)</span>
<span>val</span> myConsumer <span>=</span> <span>new</span> FlinkKafkaConsumer<span>[</span><span>String</span><span>]</span><span>(</span><span>.</span><span>.</span><span>.</span><span>)</span>
myConsumer<span>.</span>setStartFromEarliest<span>(</span><span>)</span>      <span>// 尽可能从最早的记录开始</span>
myConsumer<span>.</span>setStartFromLatest<span>(</span><span>)</span>        <span>// 从最新的记录开始</span>
myConsumer<span>.</span>setStartFromTimestamp<span>(</span><span>.</span><span>.</span><span>.</span><span>)</span>  <span>// 从指定的时间开始（毫秒）</span>
myConsumer<span>.</span>setStartFromGroupOffsets<span>(</span><span>)</span>  <span>// 默认的方法</span>

<span>val</span> stream <span>=</span> env<span>.</span>addSource<span>(</span>myConsumer<span>)</span>
<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>final</span> <span>StreamExecutionEnvironment</span> env <span>=</span> <span>StreamExecutionEnvironment</span><span>.</span><span>getExecutionEnvironment</span><span>(</span><span>)</span><span>;</span>

<span>FlinkKafkaConsumer</span><span><span>&lt;</span><span>String</span><span>></span></span> myConsumer <span>=</span> <span>new</span> <span>FlinkKafkaConsumer</span><span><span>&lt;</span><span>></span></span><span>(</span><span>.</span><span>.</span><span>.</span><span>)</span><span>;</span>
myConsumer<span>.</span><span>setStartFromEarliest</span><span>(</span><span>)</span><span>;</span>     <span>// 尽可能从最早的记录开始</span>
myConsumer<span>.</span><span>setStartFromLatest</span><span>(</span><span>)</span><span>;</span>       <span>// 从最新的记录开始</span>
myConsumer<span>.</span><span>setStartFromTimestamp</span><span>(</span><span>.</span><span>.</span><span>.</span><span>)</span><span>;</span> <span>// 从指定的时间开始（毫秒）</span>
myConsumer<span>.</span><span>setStartFromGroupOffsets</span><span>(</span><span>)</span><span>;</span> <span>// 默认的方法</span>

<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> stream <span>=</span> env<span>.</span><span>addSource</span><span>(</span>myConsumer<span>)</span><span>;</span>
<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p>在<code>StreamX</code>中不推荐这种方式进行设定,提供了更方便的方式,只需要在配置里指定 <mark><code>auto.offset.reset</code></mark> 即可</p>
<ul>
<li><code>earliest</code> 从最早的记录开始</li>
<li><code>latest</code> 从最新的记录开始</li>
</ul>
<h3 id="指定分区offset"> 指定分区Offset</h3>
<p>你也可以为每个分区指定 consumer 应该开始消费的具体 offset,只需要按照如下的配置文件配置<code>start.from</code>相关的信息即可</p>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> topic1<span>,</span>topic2<span>,</span>topic3<span>...</span>
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>start.from</span><span>:</span>
    <span>timestamp</span><span>:</span> <span>1591286400000</span> <span>#指定timestamp,针对所有的topic生效</span>
    <span>offset</span><span>:</span> <span># 给topic的partition指定offset</span>
      <span>topic</span><span>:</span> topic_abc<span>,</span>topic_123
      <span>topic_abc</span><span>:</span> 0<span>:</span><span>182</span><span>,</span>1<span>:</span><span>183</span><span>,</span>2<span>:</span><span>182</span> <span>#分区0从182开始消费,分区1从183开始,分区2从182开始...</span>
      <span>topic_123</span><span>:</span> 0<span>:</span><span>182</span><span>,</span>1<span>:</span><span>183</span><span>,</span>2<span>:</span><span>182</span>
  <span>...</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><h3 id="指定deserializer"> 指定deserializer</h3>
<p>默认不指定<code>deserializer</code>则在内部采用String的方式反序列化topic中的数据,可以手动指定<code>deserializer</code>,这样可以一步直接返回目标<code>DataStream</code>,具体完整代码如下</p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span>TypeInformation
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span>TypeExtractor<span>.</span>getForClass
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span>KafkaDeserializationSchema

<span>object</span> KafkaSourceApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        KafkaSource<span>(</span><span>)</span>
          <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>deserializer <span>=</span> <span>new</span> UserSchema<span>)</span>
          <span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
          <span>.</span>print<span>(</span><span>)</span>
  <span>}</span>

<span>}</span>

<span>class</span> UserSchema <span>extends</span> KafkaDeserializationSchema<span>[</span>User<span>]</span> <span>{</span>
  <span>override</span> <span>def</span> isEndOfStream<span>(</span>nextElement<span>:</span> User<span>)</span><span>:</span> <span>Boolean</span> <span>=</span> <span>false</span>
  <span>override</span> <span>def</span> getProducedType<span>:</span> TypeInformation<span>[</span>User<span>]</span> <span>=</span> getForClass<span>(</span>classOf<span>[</span>User<span>]</span><span>)</span>
  <span>override</span> <span>def</span> deserialize<span>(</span>record<span>:</span> ConsumerRecord<span>[</span>Array<span>[</span><span>Byte</span><span>]</span><span>,</span> Array<span>[</span><span>Byte</span><span>]</span><span>]</span><span>)</span><span>:</span> User <span>=</span> <span>{</span>
    <span>val</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span>record<span>.</span>value<span>(</span><span>)</span><span>)</span>
    JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>value<span>)</span>
  <span>}</span>
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span><span>String</span><span>,</span>age<span>:</span><span>Int</span><span>,</span>gender<span>:</span><span>Int</span><span>,</span>address<span>:</span><span>String</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span><span>KafkaDeserializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span><span>ConsumerRecord</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>import</span> <span>static</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span><span>TypeExtractor</span><span>.</span>getForClass<span>;</span>

<span>public</span> <span>class</span> <span>KafkaSourceJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>deserializer</span><span>(</span><span>new</span> <span>JavaUserSchema</span><span>(</span><span>)</span><span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span>
                <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUserSchema</span> <span>implements</span> <span>KafkaDeserializationSchema</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>{</span>
    <span>private</span> <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>boolean</span> <span>isEndOfStream</span><span>(</span><span>JavaUser</span> nextElement<span>)</span> <span>return</span> <span>false</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>TypeInformation</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>getProducedType</span><span>(</span><span>)</span> <span>return</span> <span>getForClass</span><span>(</span><span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>JavaUser</span> <span>deserialize</span><span>(</span><span>ConsumerRecord</span><span>&lt;</span><span>byte</span><span>[</span><span>]</span><span>,</span> <span>byte</span><span>[</span><span>]</span><span>></span> <span>record</span><span>)</span> <span>throws</span> <span>Exception</span> <span>{</span>
        <span>String</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span><span>record</span><span>.</span><span>value</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>return</span> mapper<span>.</span><span>readValue</span><span>(</span>value<span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="返回记录kafkarecord"> 返回记录KafkaRecord</h3>
<p>返回的对象被包装在<code>KafkaRecord</code>中,<code>kafkaRecord</code>中有当前的<code>offset</code>,<code>partition</code>,<code>timestamp</code>等诸多有用的信息供开发者使用,其中<code>value</code>即返回的目标对象,如下图:</p>
<p><img src="/streamx-docs/assets/img/doc-img/streamx_kafkaapi.jpeg" alt=""></p>
<h3 id="指定strategy"> 指定strategy</h3>
<p>在许多场景中,记录的时间戳是(显式或隐式)嵌入到记录本身中。此外,用户可能希望定期或以不规则的方式<code>Watermark</code>,例如基于<code>Kafka</code>流中包含当前事件时间的<code>watermark</code>的特殊记录。对于这些情况，<code>Flink Kafka Consumer</code>是允许指定<code>AssignerWithPeriodicWatermarks</code>或<code>AssignerWithPunctuatedWatermarks</code></p>
<p>在<code>StreamX</code>中运行传入一个<code>WatermarkStrategy</code>作为参数来分配<code>Watermark</code>,如下面的示例,解析<code>topic</code>中的数据为<code>user</code>对象,<code>user</code>中有个 <mark><code>orderTime</code></mark> 是时间类型,我们以这个为基准,为其分配一个<code>Watermark</code></p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>{</span>KafkaRecord<span>,</span> KafkaSource<span>}</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>eventtime<span>.</span></span><span>{</span>SerializableTimestampAssigner<span>,</span> WatermarkStrategy<span>}</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span>TypeInformation
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span>TypeExtractor<span>.</span>getForClass
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span>KafkaDeserializationSchema
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span>ConsumerRecord

<span>import</span> <span>java<span>.</span>time<span>.</span></span>Duration
<span>import</span> <span>java<span>.</span>util<span>.</span></span>Date

<span>object</span> KafkaSourceStrategyApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    KafkaSource<span>(</span><span>)</span>
      <span>.</span>getDataStream<span>[</span>User<span>]</span><span>(</span>
        deserializer <span>=</span> <span>new</span> UserSchema<span>,</span>
        strategy <span>=</span> WatermarkStrategy
          <span>.</span>forBoundedOutOfOrderness<span>[</span>KafkaRecord<span>[</span>User<span>]</span><span>]</span><span>(</span>Duration<span>.</span>ofMinutes<span>(</span><span>1</span><span>)</span><span>)</span>
          <span>.</span>withTimestampAssigner<span>(</span><span>new</span> SerializableTimestampAssigner<span>[</span>KafkaRecord<span>[</span>User<span>]</span><span>]</span> <span>{</span>
            <span>override</span> <span>def</span> extractTimestamp<span>(</span>element<span>:</span> KafkaRecord<span>[</span>User<span>]</span><span>,</span> recordTimestamp<span>:</span> <span>Long</span><span>)</span><span>:</span> <span>Long</span> <span>=</span> <span>{</span>
              element<span>.</span>value<span>.</span>orderTime<span>.</span>getTime
            <span>}</span>
          <span>}</span><span>)</span>
      <span>)</span><span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
      <span>.</span>print<span>(</span><span>)</span>
  <span>}</span>

<span>}</span>

<span>class</span> UserSchema <span>extends</span> KafkaDeserializationSchema<span>[</span>User<span>]</span> <span>{</span>
  <span>override</span> <span>def</span> isEndOfStream<span>(</span>nextElement<span>:</span> User<span>)</span><span>:</span> <span>Boolean</span> <span>=</span> <span>false</span>
  <span>override</span> <span>def</span> getProducedType<span>:</span> TypeInformation<span>[</span>User<span>]</span> <span>=</span> getForClass<span>(</span>classOf<span>[</span>User<span>]</span><span>)</span>
  <span>override</span> <span>def</span> deserialize<span>(</span>record<span>:</span> ConsumerRecord<span>[</span>Array<span>[</span><span>Byte</span><span>]</span><span>,</span> Array<span>[</span><span>Byte</span><span>]</span><span>]</span><span>)</span><span>:</span> User <span>=</span> <span>{</span>
    <span>val</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span>record<span>.</span>value<span>(</span><span>)</span><span>)</span>
    JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>value<span>)</span>
  <span>}</span>
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span> <span>String</span><span>,</span> age<span>:</span> <span>Int</span><span>,</span> gender<span>:</span> <span>Int</span><span>,</span> address<span>:</span> <span>String</span><span>,</span> orderTime<span>:</span> Date<span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>eventtime<span>.</span></span><span>SerializableTimestampAssigner</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>eventtime<span>.</span></span><span>WatermarkStrategy</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span><span>KafkaDeserializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span><span>ConsumerRecord</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>
<span>import</span> <span>java<span>.</span>time<span>.</span></span><span>Duration</span><span>;</span>
<span>import</span> <span>java<span>.</span>util<span>.</span></span><span>Date</span><span>;</span>

<span>import</span> <span>static</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span><span>TypeExtractor</span><span>.</span>getForClass<span>;</span>

<span>/**
 * @author benjobs
 */</span>
<span>public</span> <span>class</span> <span>KafkaSourceStrategyJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>deserializer</span><span>(</span><span>new</span> <span>JavaUserSchema</span><span>(</span><span>)</span><span>)</span>
                <span>.</span><span>strategy</span><span>(</span>
                        <span>WatermarkStrategy</span><span>.</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>></span></span><span>forBoundedOutOfOrderness</span><span>(</span><span>Duration</span><span>.</span><span>ofMinutes</span><span>(</span><span>1</span><span>)</span><span>)</span>
                                <span>.</span><span>withTimestampAssigner</span><span>(</span>
                                        <span>(</span><span>SerializableTimestampAssigner</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>></span></span><span>)</span>
                                                <span>(</span>element<span>,</span> recordTimestamp<span>)</span> <span>-></span> element<span>.</span><span>value</span><span>(</span><span>)</span><span>.</span>orderTime<span>.</span><span>getTime</span><span>(</span><span>)</span>
                                <span>)</span>
                <span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span>
                <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>


        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUserSchema</span> <span>implements</span> <span>KafkaDeserializationSchema</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>{</span>
    <span>private</span> <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>boolean</span> <span>isEndOfStream</span><span>(</span><span>JavaUser</span> nextElement<span>)</span> <span>return</span> <span>false</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>TypeInformation</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>getProducedType</span><span>(</span><span>)</span> <span>return</span> <span>getForClass</span><span>(</span><span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>JavaUser</span> <span>deserialize</span><span>(</span><span>ConsumerRecord</span><span>&lt;</span><span>byte</span><span>[</span><span>]</span><span>,</span> <span>byte</span><span>[</span><span>]</span><span>></span> <span>record</span><span>)</span> <span>throws</span> <span>Exception</span> <span>{</span>
        <span>String</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span><span>record</span><span>.</span><span>value</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>return</span> mapper<span>.</span><span>readValue</span><span>(</span>value<span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
    <span>Date</span> orderTime<span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br><span>59</span><br><span>60</span><br><span>61</span><br><span>62</span><br><span>63</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>注意事项</p>
<p>如果<code>watermark assigner</code>依赖于从<code>Kafka</code>读取的消息来上涨其<code>watermark</code>(通常就是这种情况),那么所有主题和分区都需要有连续的消息流。否则, <mark>整个应用程序的<code>watermark</code>将无法上涨</mark> ，所有基于时间的算子(例如时间窗口或带有计时器的函数)也无法运行。单个的<code>Kafka</code>分区也会导致这种反应。考虑设置适当的 <mark><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html#dealing-with-idle-sources" target="_blank" rel="noopener noreferrer"><code>idelness timeouts</code></a></mark> 来缓解这个问题。</p>
</div>
<h2 id="kafka-sink-producer"> Kafka Sink (Producer)</h2>
<p>在<code>StreamX</code>中<code>Kafka Producer</code> 被称为<code>KafkaSink</code>,它允许将消息写入一个或多个<code>Kafka topic中</code></p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code> <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span><span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
 KafkaSink<span>(</span><span>)</span><span>.</span>sink<span>(</span>source<span>)</span>     
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code> <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
 <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
 <span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
         <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
         <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>String</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span><span>;</span>
 
 <span>new</span> <span>KafkaSink</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span><span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>
 
 context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p><code>sink</code>是具体的写入数据的方法,参数列表如下</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">参数类型</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">必须</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>stream</code></td>
<td style="text-align:left">DataStream[T]</td>
<td style="text-align:left">要写的数据流</td>
<td style="text-align:left">无</td>
<td style="text-align:left"><i style="color:green"></i></td>
</tr>
<tr>
<td style="text-align:left"><code>alias</code></td>
<td style="text-align:left">String</td>
<td style="text-align:left"><code>kafka</code>的实例别名</td>
<td style="text-align:left">无</td>
<td style="text-align:left"><i style="color:red"></i></td>
</tr>
<tr>
<td style="text-align:left"><code>serializationSchema</code></td>
<td style="text-align:left">SerializationSchema[T]</td>
<td style="text-align:left">写入的序列化器</td>
<td style="text-align:left">SimpleStringSchema</td>
<td style="text-align:left"><i style="color:red"></i></td>
</tr>
<tr>
<td style="text-align:left"><code>partitioner</code></td>
<td style="text-align:left">FlinkKafkaPartitioner[T]</td>
<td style="text-align:left">kafka分区器</td>
<td style="text-align:left">KafkaEqualityPartitioner[T]</td>
<td style="text-align:left"><i style="color:red"></i></td>
</tr>
</tbody>
</table>
<h3 id="容错和语义"> 容错和语义</h3>
<p>启用 Flink 的 <code>checkpointing</code> 后，<code>KafkaSink</code> 可以提供<code>精确一次</code>的语义保证,具体开启<code>checkpointing</code>的设置请参考第二章关于<a href="/doc/guide/quickstart/conf/#checkpoints">项目配置</a>部分</p>
<p>除了启用 Flink 的 checkpointing，你也可以通过将适当的 <code>semantic</code> 参数传递给 <code>KafkaSink</code> 来选择三种不同的操作模式</p>
<div>
<ul>
<li>EXACTLY_ONCE  使用 Kafka 事务提供精确一次语义</li>
<li>AT_LEAST_ONCE 至少一次,可以保证不会丢失任何记录(但是记录可能会重复)</li>
<li>NONE Flink 不会有任何语义的保证，产生的记录可能会丢失或重复</li>
</ul>
</div>
<p>具体操作如下,只需要在<code>kafka.sink</code>下配置<code>semantic</code>即可</p>
<div><pre><code><span>kafka.sink</span><span>:</span>
    <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
    <span>topic</span><span>:</span> kfk_sink
    <span>transaction.timeout.ms</span><span>:</span> <span>1000</span>
    <span>semantic</span><span>:</span> AT_LEAST_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
    <span>batch.size</span><span>:</span> <span>1</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><div><p>注意事项</p>
<p><code>Semantic.EXACTLY_ONCE</code>模式依赖于事务提交的能力。事务提交发生于触发 checkpoint 之前，以及从 checkpoint 恢复之后。如果从 Flink 应用程序崩溃到完全重启的时间超过了 Kafka 的事务超时时间，那么将会有数据丢失（Kafka 会自动丢弃超出超时时间的事务）。考虑到这一点，请根据预期的宕机时间来合理地配置事务超时时间。</p>
<p>默认情况下，Kafka broker 将 transaction.max.timeout.ms 设置为 15 分钟。此属性不允许为大于其值的 producer 设置事务超时时间。 默认情况下，FlinkKafkaProducer 将 producer config 中的 transaction.timeout.ms 属性设置为 1 小时，因此在使用 Semantic.EXACTLY_ONCE 模式之前应该增加 transaction.max.timeout.ms 的值。</p>
<p>在 KafkaConsumer 的 read_committed 模式中，任何未结束（既未中止也未完成）的事务将阻塞来自给定 Kafka topic 的未结束事务之后的所有读取数据。 换句话说，在遵循如下一系列事件之后：</p>
<div>
<ul>
<li>用户启动了 transaction1 并使用它写了一些记录</li>
<li>用户启动了 transaction2 并使用它编写了一些其他记录</li>
<li>用户提交了 transaction2</li>
</ul>
</div>
<p>即使 transaction2 中的记录已提交，在提交或中止 transaction1 之前，消费者也不会看到这些记录。这有 2 层含义：</p>
<ul>
<li>首先，在 Flink 应用程序的正常工作期间，用户可以预料 Kafka 主题中生成的记录的可见性会延迟，相当于已完成 checkpoint 之间的平均时间。</li>
<li>其次，在 Flink 应用程序失败的情况下，此应用程序正在写入的供消费者读取的主题将被阻塞，直到应用程序重新启动或配置的事务超时时间过去后，才恢复正常。此标注仅适用于有多个 agent 或者应用程序写入同一 Kafka 主题的情况。</li>
</ul>
<p>注意：<code>Semantic.EXACTLY_ONCE</code> 模式为每个 FlinkKafkaProducer 实例使用固定大小的 KafkaProducer 池。每个 checkpoint 使用其中一个 producer。如果并发 checkpoint 的数量超过池的大小，FlinkKafkaProducer 将抛出异常，并导致整个应用程序失败。请合理地配置最大池大小和最大并发 checkpoint 数量。</p>
<p>注意：<code>Semantic.EXACTLY_ONCE</code> 会尽一切可能不留下任何逗留的事务，否则会阻塞其他消费者从这个 Kafka topic 中读取数据。但是，如果 Flink 应用程序在第一次 checkpoint 之前就失败了，那么在重新启动此类应用程序后，系统中不会有先前池大小（pool size）相关的信息。因此，在第一次 checkpoint 完成前对 Flink 应用程序进行缩容，且并发数缩容倍数大于安全系数 FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR 的值的话，是不安全的。</p>
</div>
<h3 id="多实例kafka指定alias"> 多实例kafka指定alias</h3>
<p>如果写时有多个不同实例的kafka需要配置,同样采用<code>alias</code>来区别不用的kafka实例,配置如下:</p>
<div><pre><code><span>kafka.sink</span><span>:</span>
    <span>kafka_cluster1</span><span>:</span>
        <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
        <span>topic</span><span>:</span> kfk_sink
        <span>transaction.timeout.ms</span><span>:</span> <span>1000</span>
        <span>semantic</span><span>:</span> AT_LEAST_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
        <span>batch.size</span><span>:</span> <span>1</span>
    <span>kafka_cluster2</span><span>:</span>
        <span>bootstrap.servers</span><span>:</span> kfk6<span>:</span><span>9092</span><span>,</span>kfk7<span>:</span><span>9092</span><span>,</span>kfk8<span>:</span><span>9092</span>
        <span>topic</span><span>:</span> kfk_sink
        <span>transaction.timeout.ms</span><span>:</span> <span>1000</span>
        <span>semantic</span><span>:</span> AT_LEAST_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
        <span>batch.size</span><span>:</span> <span>1</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><p>在写入的时候,需要手动指定<code>alias</code>,注意下<code>scala</code> api和<code>java</code> api在代码上稍有不同,<code>scala</code>直接在<code>sink</code>方法里指定参数,<code>java</code> api则是通过<code>alias()</code>方法来设置,其底层实现是完全一致的</p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code> <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span><span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
 KafkaSink<span>(</span><span>)</span><span>.</span>sink<span>(</span>source<span>,</span>alias <span>=</span> <span>"kafka_cluster1"</span><span>)</span>     
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code> <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
 <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
 <span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
         <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
         <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>String</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span><span>;</span>
 
 <span>new</span> <span>KafkaSink</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span><span>.</span><span>alias</span><span>(</span><span>"kafka_cluster1"</span><span>)</span><span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>
 
 context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="指定serializationschema"> 指定SerializationSchema</h3>
<p><code>Flink Kafka Producer</code> 需要知道如何将 Java/Scala 对象转化为二进制数据。 KafkaSerializationSchema 允许用户指定这样的schema, 相关操作方式和文档请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#the-serializationschema" target="_blank" rel="noopener noreferrer">官网文档</a></p>
<p>在<code>KafkaSink</code>里默认不指定序列化方式,采用的是<code>SimpleStringSchema</code>来进行序列化,这里开发者可以显示的指定一个自定义的序列化器,通过<code>serializationSchema</code>参数指定即可,例如,将<code>user</code>对象安装自定义的格式写入<code>kafka</code></p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>serialization<span>.</span></span>SerializationSchema
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> KafkaSinkApp <span>extends</span> FlinkStreaming <span>{</span>
  
  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span>
      <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
      <span>.</span>map<span>(</span>x <span>=></span> JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>x<span>.</span>value<span>)</span><span>)</span>
      
    KafkaSink<span>(</span><span>)</span><span>.</span>sink<span>[</span>User<span>]</span><span>(</span>source<span>,</span> serialization <span>=</span> <span>new</span> SerializationSchema<span>[</span>User<span>]</span><span>(</span><span>)</span> <span>{</span>
      <span>override</span> <span>def</span> serialize<span>(</span>user<span>:</span> User<span>)</span><span>:</span> Array<span>[</span><span>Byte</span><span>]</span> <span>=</span> <span>{</span>
        s<span>"${user.name},${user.age},${user.gender},${user.address}"</span><span>.</span>getBytes
      <span>}</span>
    <span>}</span><span>)</span>
    
  <span>}</span>
  
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span> <span>String</span><span>,</span> age<span>:</span> <span>Int</span><span>,</span> gender<span>:</span> <span>Int</span><span>,</span> address<span>:</span> <span>String</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>sink<span>.</span></span><span>KafkaSink</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>FilterFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>serialization<span>.</span></span><span>SerializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>api<span>.</span>datastream<span>.</span></span><span>DataStream</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>public</span> <span>class</span> kafkaSinkJavaApp <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>

        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>

        <span>DataStream</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span>
                        mapper<span>.</span><span>readValue</span><span>(</span>value<span>.</span><span>value</span><span>(</span><span>)</span><span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>)</span><span>;</span>

        <span>new</span> <span>KafkaSink</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>serializer</span><span>(</span>
                        <span>(</span><span>SerializationSchema</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> element <span>-></span>
                                <span>String</span><span>.</span><span>format</span><span>(</span><span>"%s,%d,%d,%s"</span><span>,</span> element<span>.</span>name<span>,</span> element<span>.</span>age<span>,</span> element<span>.</span>gender<span>,</span> element<span>.</span>address<span>)</span><span>.</span><span>getBytes</span><span>(</span><span>)</span>
                <span>)</span><span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="指定partitioner"> 指定partitioner</h3>
<p><code>KafkaSink</code>允许显示的指定一个kafka分区器,不指定默认使用<code>StreamX</code>内置的 <mark>KafkaEqualityPartitioner</mark> 分区器,顾名思义,该分区器可以均匀的将数据写到各个分区中去,<code>scala</code> api是通过<code>partitioner</code>参数来设置分区器,
<code>java</code> api中是通过<code>partitioner()</code>方法来设置的</p>
<div><p>注意事项</p>
<p>Flink Kafka Connector中默认使用的是 <mark>FlinkFixedPartitioner</mark> 分区器,该分区器需要特别注意<code>sink</code>的并行度和<code>kafka</code>的分区数,不然会出现往一个分区写</p>
</div>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/streamx_kafkaapi.jpeg" type="image/jpeg"/>
    </item>
    <item>
      <title>MongoDb Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/mongo/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/mongo/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">MongoDb Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Redis Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/connector/redis/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/connector/redis/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Redis Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>安装部署</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/console/deploy/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/console/deploy/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">安装部署</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<blockquote>
<p>streamx-console 是一个综合实时数据平台，低代码 ( Low Code ) ,Flink Sql 平台，可以较好的管理 Flink 任务，集成了项目编译、发布、参数配置、启动、savepoint,火焰图 ( flame graph ) ,Flink SQL,监控等诸多功能于一体，大大简化了 Flink 任务的日常操作和维护，融合了诸多最佳实践。其最终目标是打造成一个实时数仓，流批一体的一站式大数据解决方案</p>
</blockquote>
<p>streamx-console 提供了开箱即用的安装包，安装之前对环境有些要求，具体要求如下：</p>
<h2 id="环境要求"> 环境要求</h2>
<ClientOnly>
  <table-data name="envs"></table-data>
</ClientOnly>
<p>目前 StreamX 对 Flink 的任务发布，同时支持 <code>Flink on YARN</code> 和 <code>Flink on Kubernetes</code> 两种模式。</p>
<h3 id="hadoop"> Hadoop</h3>
<p>使用 <code>Flink on YARN</code>，需要部署的集群安装并配置 Hadoop的相关环境变量，如你是基于 CDH 安装的 hadoop 环境，
相关环境变量可以参考如下配置:</p>
<div><pre><code><span>export</span> <span>HADOOP_HOME</span><span>=</span>/opt/cloudera/parcels/CDH/lib/hadoop <span>#hadoop 安装目录</span>
<span>export</span> <span>HADOOP_CONF_DIR</span><span>=</span>/etc/hadoop/conf
<span>export</span> <span>HIVE_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hive
<span>export</span> <span>HBASE_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hbase
<span>export</span> <span>HADOOP_HDFS_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hadoop-hdfs
<span>export</span> <span>HADOOP_MAPRED_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hadoop-mapreduce
<span>export</span> <span>HADOOP_YARN_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hadoop-yarn
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><h3 id="kubernetes"> Kubernetes</h3>
<p>使用 <code>Flink on Kubernetes</code>，需要额外部署/或使用已经存在的 Kubernetes 集群，请参考条目： <a href="./../flink-k8s/1-deployment.html"><strong>Flink Kubernetes Integration</strong></a>。</p>
<h2 id="编译-安装"> 编译 &amp; 安装</h2>
<p>你可以选择手动编译安装也可以直接下载编译好的安装包，手动编译安装步骤如下</p>
<h3 id="编译"> 编译</h3>
<ul>
<li>Maven 3.6+</li>
<li>npm 7.11.2 ( https://nodejs.org/en/ )</li>
<li>JDK 1.8+</li>
</ul>
<div><pre><code><span>git</span> clone https://github.com/streamxhub/streamx.git
<span>cd</span> Streamx
mvn clean <span>install</span> -DskipTests -Denv<span>=</span>prod
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>安装完成之后就看到最终的工程文件，位于 <code>streamx/streamx-console/streamx-console-service/target/streamx-console-service-1.0.0-bin.tar.gz</code>,解包后安装目录如下</p>
<div><pre><code>.
streamx-console-service-1.0.0
├── bin
│    ├── flame-graph
│    ├──   └── *.py                                             //火焰图相关功能脚本 ( 内部使用，用户无需关注 )
│    ├── startup.sh                                             //启动脚本
│    ├── setclasspath.sh                                        //java 环境变量相关的脚本 ( 内部使用，用户无需关注 )
│    ├── shutdown.sh                                            //停止脚本
│    ├── yaml.sh                                                //内部使用解析 yaml 参数的脚本 ( 内部使用，用户无需关注 )
├── conf
│    ├── application.yaml                                       //项目的配置文件 ( 注意不要改动名称 )
│    ├── application-prod.yml                                   //项目的配置文件 ( 开发者部署需要改动的文件，注意不要改动名称 )
│    ├── flink-application.template                             //flink 配置模板 ( 内部使用，用户无需关注 )
│    ├── logback-spring.xml                                     //logback
│    └── ...
├── lib
│    └── *.jar                                                  //项目的 jar 包
├── plugins
│    ├── streamx-jvm-profiler-1.0.0.jar                         //jvm-profiler,火焰图相关功能 ( 内部使用，用户无需关注 )
│    └── streamx-flink-sqlclient-1.0.0.jar                      //Flink SQl 提交相关功能 ( 内部使用，用户无需关注 )
├── logs                                                        //程序 log 目录
├── temp                                                        //内部使用到的零时路径，不要删除
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div><h3 id="修改配置"> 修改配置</h3>
<p>安装解包已完成，接下来准备数据相关的工作</p>
<ul>
<li>新建数据库 <code>streamx</code>
确保在部署机可以连接的 mysql 里新建数据库 <code>streamx</code></li>
<li>修改连接信息
进入到 <code>conf</code> 下，修改 <code>conf/application-prod.yml</code>,找到 datasource 这一项，找到 mysql 的配置，修改成对应的信息即可，如下</li>
</ul>
<div><pre><code>  <span>datasource</span><span>:</span>
    <span>dynamic</span><span>:</span>
      <span># 是否开启 SQL 日志输出，生产环境建议关闭，有性能损耗</span>
      <span>p6spy</span><span>:</span> <span>false</span>
      <span>hikari</span><span>:</span>
        <span>connection-timeout</span><span>:</span> <span>30000</span>
        <span>max-lifetime</span><span>:</span> <span>1800000</span>
        <span>max-pool-size</span><span>:</span> <span>15</span>
        <span>min-idle</span><span>:</span> <span>5</span>
        <span>connection-test-query</span><span>:</span> select 1
        <span>pool-name</span><span>:</span> HikariCP<span>-</span>DS<span>-</span>POOL
      <span># 配置默认数据源</span>
      <span>primary</span><span>:</span> primary
      <span>datasource</span><span>:</span>
        <span># 数据源-1，名称为 primary</span>
        <span>primary</span><span>:</span>
          <span>username</span><span>:</span> $user
          <span>password</span><span>:</span> $password
          <span>driver-class-name</span><span>:</span> com.mysql.cj.jdbc.Driver
          <span>url</span><span>:</span> <span>jdbc</span><span>:</span> mysql<span>:</span>//$host<span>:</span>$port/streamx<span>?</span>useUnicode=true<span>&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=GMT%2B8</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br></div></div><div><p>提示</p>
<p>特别提示: 安装过程中不需要手动做数据初始化，只需要设置好数据库信息即可，会自动完成建表和数据初始化等一些列操作</p>
</div>
<h3 id="启动"> 启动</h3>
<p>进入到 <code>bin</code> 下直接执行 startup.sh 即可启动项目，默认端口是 <mark>10000</mark>,如果没啥意外则会启动成功</p>
<div><pre><code><span>cd</span> streamx-console-service-1.0.0/bin
<span>bash</span> startup.sh
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><p>相关的日志会输出到 <mark>streamx-console-service-1.0.0/logs/streamx.out</mark> 里</p>
<p>打开浏览器 输入 <strong> <mark>http://$host:10000</mark> </strong> 即可登录，登录界面如下</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_login.jpeg"/>
<div><p>提示</p>
<p>默认密码: <strong> admin / streamx </strong></p>
</div>
<h2 id="系统配置"> 系统配置</h2>
<p>进入系统之后，第一件要做的事情就是修改系统配置，在菜单/StreamX/Setting 下，操作界面如下:</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_settings.png"/>
<p>主要配置项分为以下几类</p>
<div>
<ul>
<li>Flink Home</li>
<li>Maven Home</li>
<li>StreamX Env</li>
<li>Email</li>
</ul>
</div>
<h3 id="flink-home"> Flink Home</h3>
<p>这里配置全局的 Flink Home,此处是系统唯一指定 Flink 环境的地方，会作用于所有的作业</p>
<div><p>提示</p>
<p>特别提示: 最低支持的 Flink 版本为 1.11.1, 之后的版本都支持</p>
</div>
<h3 id="maven-home"> Maven Home</h3>
<p>指定 maven Home, 目前暂不支持，下个版本实现</p>
<h3 id="streamx-env"> StreamX Env</h3>
<ul>
<li>StreamX Webapp address <br>
这里配置 StreamX Console 的 web url 访问地址，主要火焰图功能会用到，具体任务会将收集到的信息通过此处暴露的 url 发送 http 请求到系统，进行收集展示<br></li>
<li>StreamX Console Workspace <br>
配置系统的工作空间，用于存放项目源码，编译后的项目等</li>
</ul>
<h3 id="email"> Email</h3>
<p>Alert Email 相关的配置是配置发送者邮件的信息，具体配置请查阅相关邮箱资料和文档进行配置</p>
]]></content:encoded>
    </item>
    <item>
      <title>开发环境</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/console/deployment/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/console/deployment/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">开发环境</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p><span style="background-color:#ffffff; color:#333333">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><a href="https://github.com/streamxhub/streamx" target="_blank">StreamX</a><span style="background-color:#ffffff; color:#333333">&nbsp;</span>遵循 Apache-2.0 开源协议，将会是个长期更新的活跃项目，欢迎大家提交<a href="https://github.com/streamxhub/streamx/pulls">PR</a> 或 <a href="https://github.com/streamxhub/streamx/issues/new/choose">Issue</a>。喜欢请给个 <a href="https://github.com/streamxhub/streamx/stargazers">Star</a>。您的支持是我们最大的动力， 该项目从开源以来受到不少朋友的关注和认可，表示感谢，已陆续有来自金融，数据分析，车联网，智能广告，地产等公司的朋友在使用或二开，也不乏来自一线大厂的朋友在研究使用，欢迎更多的开发者加入一块贡献，不只是代码的贡献，还寻求使用文档，体验报告，问答等方面的贡献</p>
<p>StreamX 总体组件栈架构如下， 由 streamx-core 和 streamx-console 两个大的部分组成，其中 streamx-core 是开发时框架，这里不做讲解，本章节具体讲讲如何在本地搭建 streamx-console 流批一体平台的开发环境，为了方便讲解，本文中所说的 <code>streamx-console</code> 均指 <code>streamx-console 平台</code></p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_archite.png"/><br>
</center>
<p>StreamX Console 从 1.2.0 开始实现了 Flink-Runtime 的解耦，即<strong>不强制依赖 Hadoop 或 Kubernetes 环境</strong>，可以根据实际开发/使用需求自行安装 Hadoop 或 Kubernetes。</p>
<br/>
<h2 id="安装-hadoop-可选-yarn-runtime"> 安装 Hadoop（可选，YARN Runtime）</h2>
<p>关于 hadoop 环境有两种方式解决，<code>本地安装 hadoop 环境</code> 和 <code>使用已有的 hadoop 环境</code>,不论是本地安装 hadoop 环境还是使用已有的 hadoop 环境，都需要确保以下条件</p>
<ul>
<li>安装并且配置好 <code>hadoop</code>,<code>yarn</code></li>
<li>已配置 <code>HADOOP_HOME</code> 和 <code>HADOOP_CONF_DIR</code></li>
<li>已成功启动 <code>hadoop</code> 和 <code>yarn</code></li>
</ul>
<h3 id="本地安装-hadoop-环境"> 本地安装 Hadoop 环境</h3>
<p>关于如何在本地安装 Hadoop 环境可自行查阅相关资料，这里不作过多讲解。</p>
<h3 id="使用已有-hadoop-集群"> 使用已有 Hadoop 集群</h3>
<p>推荐使用已有的 Hadoop 集群 ( 测试环境 ) ,如使用已有 hadoop 集群需要将以下配置 copy 到开发机器</p>
<ul>
<li><code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code> 这三个配置文件 copy 到开发机器</li>
<li>如果开启了 kerberos 认证，需要将 <code>keytab</code> 文件和 <code>krb5.conf</code> copy 到开发机器</li>
</ul>
<p>需要注意的是，<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code> 这些配置文件里的一系列主机地址 ( host ) 需要在当前的开发机器上配置出来，需要确保本机可以连接集群里的机器。</p>
<h2 id="安装-kubernetes-可选-k8s-runtime"> 安装 Kubernetes （可选，K8s Runtime）</h2>
<p>本地开发可以通过 MiniKube 或 KubeSphere 等项目快速安装 Kubernetes 环境，当然选择现有的 K8s Cluster 设施更加推荐。此外按时计费的腾讯云 TKE / 阿里云 ACK 也是快速开发很好的选择。</p>
<p>额外配置需求请参考： <a href="./../flink-k8s/1-deployment.html"><strong>StreamX Flink-K8s 集成支持</strong></a></p>
<h2 id="安装-flink-可选-standalone-runtime"> 安装 Flink（可选，Standalone Runtime）</h2>
<p>从官网下载 Flink,并且启动测试，配置 FLINK_HOME</p>
<div><pre><code><span>wget</span> https://mirrors.bfsu.edu.cn/apache/flink/flink-1.13.1/flink-1.13.1-bin-scala_2.11.tgz
<span>tar</span> xzf flink-1.13.1-bin-scala_2.11.tgz /opt/
<span>cd</span> /opt/flink-1.13.1
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>启动本地 Flink 群集，可以检查下 flink 是否正常启动</p>
<div><pre><code>./bin/start-cluster.sh
</code></pre>
<div><span>1</span><br></div></div><h2 id="安装-maven"> 安装 Maven</h2>
<p>最新的 Maven 下载地址：<code>http://maven.apache.org/download.cgi</code>，我们创建一个连接，以便 mvn 可以在任何地方运行。</p>
<div><pre><code><span>cd</span> ~
<span>wget</span> https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.8.1/binaries/apache-maven-3.8.1-bin.tar.gz
<span>tar</span> -xzvf apache-maven-3.8.1-bin.tar.gz
<span>ln</span> -s /root/apache-maven-3.8.1/bin/mvn /usr/bin/mvn
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></div></div><h2 id="安装-mysql"> 安装 MySQL</h2>
<p><code>console</code> 用到了 MySQL,因此需要准备 MySQL 环境，你可以本地安装 MySQL,也可以直接使用已有的 MySQL,关于 MySQL 的安装配置，请自行查阅资料，这里不作过多讲解</p>
<h2 id="安装-nodejs"> 安装 Nodejs</h2>
<p><code>console</code> 前端部分采用 nodejs 开发，需要 nodejs 环境，下载安装最新的 nodejs 即可</p>
<h2 id="安装配置-streamx"> 安装配置 StreamX</h2>
<p>如果以上准备工作都已经就绪，此时就可以安装配置 <code>streamx-console</code> 了，<code>streamx-console</code> 是前后端分离的项目，在项目最终打包部署时为了方便快捷，减少用户的使用和学习成本，使用了前后端混合打包部署模式，但在开发阶段建议使用前后端分离模式进行开发调试，具体步骤如下</p>
<h3 id="后端"> 后端</h3>
<p><code>streamx-console</code> 后端采用 springBoot + Mybatis 开发， JWT 权限验证，非常常见的后端技术栈。下面来看看后端按照部署具体流程</p>
<h4 id="编译"> 编译</h4>
<p>首先将 <code>StreamX</code> 工程下载到本地并且编译</p>
<div><pre><code><span>git</span> clone https://github.com/streamxhub/streamx.git
<span>cd</span> streamx
mvn clean <span>install</span> -DskipTests -Denv<span>=</span>prod
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><h4 id="解包"> 解包</h4>
<p>安装完成之后就看到最终的工程文件解包，位于 <code>streamx/streamx-console/streamx-console-service/target/streamx-console-service-${version}-bin.tar.gz</code>,解包之后的目录如下:</p>
<div><pre><code>.
streamx-console-service-${version}
├── bin
│    ├── flame-graph
│    ├──   └── *.py
│    ├── startup.sh
│    ├── setclasspath.sh
│    ├── shutdown.sh
│    ├── yaml.sh
├── conf
│    ├── application.yaml
│    ├── application-prod.yml
│    ├── flink-application.template
│    ├── logback-spring.xml
│    └── ...
├── lib
│    └── *.jar
├── plugins
│    ├── streamx-jvm-profiler-1.0.0.jar
│    └── streamx-flink-sqlclient-1.0.0.jar
├── logs
├── temp
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div><p>将解包后的整个工程文件 copy 到 target 之外的其他任意位置即可完成此步骤，该步主要是防止下次执行 mvn clean 被清理，如放到 <code>/opt/streamx/</code>,则此时该文件的完整路径是 <code>/opt/streamx/streamx-console-service-${version}</code>,记住这个路径，后面会用到</p>
<h4 id="配置"> 配置</h4>
<p>用 IDE 导入刚从 git 上 clone 下来的 StreamX 源码 ( 推荐使用 <code>IntelliJ IDEA</code> ) ,进入到 <code>resources</code> 下，编辑 application-prod.xml,找到 <code>datasource</code>,修改下 jdbc 的连接信息，具体可参考安装部署章节 <a href="http://www.streamxhub.com/zh/doc/console/deploy/#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE" target="_blank" rel="noopener noreferrer">修改配置</a> 部分</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_conf.jpg" />
<p>如果你要连接的目标集群开启了 kerberos 认证，则需要配置 kerberos 信息，在 <code>resources</code> 下找到 <code>kerberos.xml</code> 配置上相关信息即可，默认 kerberos 是关闭状态，要启用需将 <code>enable</code> 设置为 true, 如下:</p>
<div><pre><code><span>security</span><span>:</span>
  <span>kerberos</span><span>:</span>
    <span>login</span><span>:</span>
      <span>enable</span><span>:</span> <span>false</span>
      <span>principal</span><span>:</span>
      <span>krb5</span><span>:</span>
      <span>keytab</span><span>:</span>
<span>java</span><span>:</span>
  <span>security</span><span>:</span>
    <span>krb5</span><span>:</span>
      <span>conf</span><span>:</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></div></div><h4 id="启动"> 启动</h4>
<p><code>streamx-console</code> 是基于 springBoot 开发的 web 应用，<code>com.streamxhub.streamx.console.StreamXConsole</code> 为主类， 在启动主类之前，需要设置下 <code>VM options</code> 和 <code>Environment variables</code></p>
<h5 id="vm-options"> VM options</h5>
<p>在 <code>VM options</code> 需要设置 <code>app.home</code>:值为上面解包后的 streamx-console 的完整路径:</p>
<div><pre><code>-Dapp.home<span>=</span>/opt/streamx/streamx-console-service-<span>${version}</span>
</code></pre>
<div><span>1</span><br></div></div><p><br><br>
如果开发机使用的 jdk 版本是 jdk1.8 以上版本， 则需要加上如下参数: <br></p>
<div><pre><code><span>-</span><span>-</span>add<span>-</span>opens java.base/jdk.internal.loader=ALL<span>-</span>UNNAMED <span>-</span><span>-</span>add<span>-</span>opens jdk.zipfs/jdk.nio.zipfs=ALL<span>-</span>UNNAMED
</code></pre>
<div><span>1</span><br></div></div><h5 id="environment-variables"> Environment variables</h5>
<p>如使用非本地安装的 hadoop 集群 ( 测试 hadoop ) <code>Environment variables</code> 中需要配置 <code>HADOOP_USER_NAME</code> 和 <code>HADOOP_CONF_DIR</code>,
<code>HADOOP_USER_NAME</code> 为 hdfs 或者有读写权限的 hadoop 用户名，<code>HADOOP_CONF_DIR</code> 为上面第一步安装 hadoop 步骤中从测试集群 copy 相关配置文件在开发机器上的存放位置，如果是本地安装的 hadoop 则不需要配置该项，</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_ideaopt.jpg" />
<p>如果一切准假就绪，就可以直接启动 <code>StreamXConsole</code> 主类启动项目，后端就启动成功了。会看到有相关的启动信息打印输出</p>
<h3 id="前端"> 前端</h3>
<p>streamx web 前端部分采用 nodejs + vue 开发，因此需要在机器上按照 node 环境，完整流程如下:</p>
<h4 id="修改请求-url"> 修改请求 URL</h4>
<p>由于是前后端分离项目，前端需要知道后端 ( streamx-console ) 的访问地址，才能前后配合工作，因此需要后端的 URL,具体位置在:
<code>streamx-console/streamx-console-webapp/src/api/baseUrl.js</code></p>
<p>配置默认如下:</p>
<div><pre><code><span>export</span> <span>function</span> <span>baseUrl</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> url <span>=</span> <span>''</span>
    <span>switch</span> <span>(</span>process<span>.</span>env<span>.</span><span>NODE_ENV</span><span>)</span> <span>{</span>
        <span>//混合打包 ( production,不用配置，maven 编译项目阶段-Denv=prod 自动将环境参数透传到这里 )</span>
        <span>case</span> <span>'production'</span><span>:</span>
            url <span>=</span> <span>(</span>arguments<span>[</span><span>0</span><span>]</span> <span>||</span> <span>null</span><span>)</span> <span>?</span> <span>(</span>location<span>.</span>protocol <span>+</span> <span>'//'</span> <span>+</span> location<span>.</span>host<span>)</span> <span>:</span> <span>'/'</span>
            <span>break</span>
        <span>//开发测试阶段采用前后端分离，这里配置后端的请求 URI</span>
        <span>case</span> <span>'development'</span><span>:</span>
            url <span>=</span> <span>'http://localhost:10000'</span>
            <span>break</span>
    <span>}</span>
    <span>return</span> url
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></div></div><p>将 <code>development</code> 下的 URL 连接改为后端的 URI 即可</p>
<h4 id="编译项目"> 编译项目</h4>
<p>接下来需要编译项目，具体步骤如下:</p>
<div><pre><code><span>cd</span> streamx-console/streamx-console-webapp
<span>npm</span> <span>install</span>
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><h4 id="启动项目"> 启动项目</h4>
<p>以上步骤执行完毕即可启动项目即可</p>
<div><pre><code><span>cd</span> streamx-console/streamx-console-webapp
<span>npm</span> run serve
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div>]]></content:encoded>
    </item>
    <item>
      <title>快速开始</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/console/quickstart/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/console/quickstart/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">快速开始</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="如何使用"> 如何使用</h2>
<p>streamx-console 定位是流批一体的大数据平台，一站式解决方案，使用起来非常简单，没有复杂的概念和繁琐的操作，标准的 Flink 程序 ( 安装 Flink 官方要去的结构和规范 ) 和用 <code>streamx</code> 开发的项目都做了很好的支持，下面我们使用 <code>streamx-quickstart</code> 来快速开启 streamx-console 之旅</p>
<p><code>streamx-quickstart</code> 是 StreamX 开发 Flink 的上手示例程序，具体请查阅</p>
<ul>
<li>Github: <a href="https://github.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">https://github.com/streamxhub/streamx-quickstart.git</a></li>
<li>Gitee: <a href="https://gitee.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">https://gitee.com/streamxhub/streamx-quickstart.git</a></li>
</ul>
<h3 id="部署-datastream-任务"> 部署 DataStream 任务</h3>
<p>下面的示例演示了如何部署一个 DataStream 应用</p>
<p><video src="/streamx-docs/assets/video/datastream.mp4" controls="controls" width="100%" height="100%"></video></p>
<h3 id="部署-flinksql-任务"> 部署 FlinkSql 任务</h3>
<p>下面的示例演示了如何部署一个 FlinkSql 应用</p>
<p><video src="/streamx-docs/assets/video/flinksql.mp4" controls="controls" width="100%" height="100%"></video></p>
<ul>
<li>项目演示使用到的 flink sql 如下</li>
</ul>
<div><pre><code><span>CREATE</span> <span>TABLE</span> user_log <span>(</span>
    user_id <span>VARCHAR</span><span>,</span>
    item_id <span>VARCHAR</span><span>,</span>
    category_id <span>VARCHAR</span><span>,</span>
    behavior <span>VARCHAR</span><span>,</span>
    ts <span>TIMESTAMP</span><span>(</span><span>3</span><span>)</span>
 <span>)</span> <span>WITH</span> <span>(</span>
<span>'connector.type'</span> <span>=</span> <span>'kafka'</span><span>,</span> <span>-- 使用 kafka connector</span>
<span>'connector.version'</span> <span>=</span> <span>'universal'</span><span>,</span>  <span>-- kafka 版本，universal 支持 0.11 以上的版本</span>
<span>'connector.topic'</span> <span>=</span> <span>'user_behavior'</span><span>,</span>  <span>-- kafka topic</span>
<span>'connector.properties.bootstrap.servers'</span><span>=</span><span>'kafka-1:9092,kafka-2:9092,kafka-3:9092'</span><span>,</span>
<span>'connector.startup-mode'</span> <span>=</span> <span>'earliest-offset'</span><span>,</span> <span>-- 从起始 offset 开始读取</span>
<span>'update-mode'</span> <span>=</span> <span>'append'</span><span>,</span>
<span>'format.type'</span> <span>=</span> <span>'json'</span><span>,</span>  <span>-- 数据源格式为 json</span>
<span>'format.derive-schema'</span> <span>=</span> <span>'true'</span> <span>-- 从 DDL schema 确定 json 解析规则</span>
 <span>)</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> pvuv_sink <span>(</span>
    dt <span>VARCHAR</span><span>,</span>
    pv <span>BIGINT</span><span>,</span>
    uv <span>BIGINT</span>
 <span>)</span> <span>WITH</span> <span>(</span>
<span>'connector.type'</span> <span>=</span> <span>'jdbc'</span><span>,</span> <span>-- 使用 jdbc connector</span>
<span>'connector.url'</span> <span>=</span> <span>'jdbc:mysql://test-mysql:3306/test'</span><span>,</span> <span>-- jdbc url</span>
<span>'connector.table'</span> <span>=</span> <span>'pvuv_sink'</span><span>,</span> <span>-- 表名</span>
<span>'connector.username'</span> <span>=</span> <span>'root'</span><span>,</span> <span>-- 用户名</span>
<span>'connector.password'</span> <span>=</span> <span>'123456'</span><span>,</span> <span>-- 密码</span>
<span>'connector.write.flush.max-rows'</span> <span>=</span> <span>'1'</span> <span>-- 默认 5000 条，为了演示改为 1 条</span>
 <span>)</span><span>;</span>

<span>INSERT</span> <span>INTO</span> pvuv_sink
<span>SELECT</span>
  DATE_FORMAT<span>(</span>ts<span>,</span> <span>'yyyy-MM-dd HH:00'</span><span>)</span> dt<span>,</span>
  <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>AS</span> pv<span>,</span>
  <span>COUNT</span><span>(</span><span>DISTINCT</span> user_id<span>)</span> <span>AS</span> uv
<span>FROM</span> user_log
<span>GROUP</span> <span>BY</span> DATE_FORMAT<span>(</span>ts<span>,</span> <span>'yyyy-MM-dd HH:00'</span><span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br></div></div><ul>
<li>使用到 maven 依赖如下</li>
</ul>
<div><pre><code><span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>mysql<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>mysql-connector-java<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>5.1.48<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>

<span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-sql-connector-kafka_2.12<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>

<span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-connector-jdbc_2.11<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>

<span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-json<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br></div></div><ul>
<li>Kafka 模拟发送的数据如下</li>
</ul>
<div><pre><code><span>{</span><span>"user_id"</span><span>:</span> <span>"543462"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"1715"</span><span>,</span> <span>"category_id"</span><span>:</span> <span>"1464116"</span><span>,</span> <span>"behavior"</span><span>:</span> <span>"pv"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
<span>{</span><span>"user_id"</span><span>:</span> <span>"662867"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"2244074"</span><span>,</span><span>"category_id"</span><span>:</span><span>"1575622"</span><span>,</span><span>"behavior"</span><span>:</span> <span>"pv"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
<span>{</span><span>"user_id"</span><span>:</span> <span>"662867"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"2244074"</span><span>,</span><span>"category_id"</span><span>:</span><span>"1575622"</span><span>,</span><span>"behavior"</span><span>:</span> <span>"pv"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
<span>{</span><span>"user_id"</span><span>:</span> <span>"662867"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"2244074"</span><span>,</span><span>"category_id"</span><span>:</span><span>"1575622"</span><span>,</span><span>"behavior"</span><span>:</span> <span>"learning flink"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></div></div><h3 id="任务启动流程"> 任务启动流程</h3>
<p>任务启动流程图如下</p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_start.png"/><br>
<strong>streamx-console 提交任务流程</strong>
</center>
<p>关于项目的概念，<code>Development Mode</code>,<code>savepoint</code>,<code>NoteBook</code>,自定义 jar 管理，任务发布，任务恢复，参数配置，参数对比，多版本管理等等更多使用教程和文档后续持续更新。..</p>
]]></content:encoded>
    </item>
    <item>
      <title>Flink Kubernetes Integration</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/flink-k8s/1-deployment/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/flink-k8s/1-deployment/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Flink Kubernetes Integration</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>StreamX Flink Kubernetes Integration is based on <a href="https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/resource-providers/native_kubernetes/" target="_blank" rel="noopener noreferrer">Flink Native Kubernetes</a>, and supports the following Flink-Native-K8s runtime mode:</p>
<ul>
<li>Native-Kubernetes Application</li>
<li>Native-Kubernetes Session</li>
</ul>
<p>Currently, a single StreamX application instance supports only a single Kubernetes cluster.</p>
<br/>
<h2 id="additional-software-environment"> Additional Software Environment</h2>
<p>StreamX Flink- K8s Integration requires the following additional software environment:</p>
<ul>
<li>Kubernetes</li>
<li>Maven（StreamX running machines require）</li>
<li>Docker（StreamX running machines require）</li>
</ul>
<p>StreamX instance do not need to be mandatorily deployed on the Kubernetes nodes. It can be deployed on the machine external to the Kubernetes cluster, just needs to be open to networks communication with the Kubernetes cluster.</p>
<br/>
<h2 id="deployment-preparation"> Deployment Preparation</h2>
<h3 id="kubernetes-access"> Kubernetes Access</h3>
<p>StreamX uses system file <code>～/.kube/config</code> directly as the connection credential for Kubernetes cluster. An easy way to do this is to copy the <code>～/.kube/config</code> from K8s node to user directory of StreamX node directly. Of course, you can generate conf file for K8s custom accounts to further constrain permission.</p>
<p>After that, you can quickly check the connectivity of the target K8s cluster via <code>kubectl</code> on machine where StreamX was deployed:</p>
<div><pre><code>kubectl cluster-info
</code></pre>
<div><span>1</span><br></div></div><h3 id="kubernetes-rbac"> Kubernetes RBAC</h3>
<p>The K8s RBAC resource used by Flink need to be created in advance, as described in the Flink Document. Refer to: https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/resource-providers/native_kubernetes/#rbac</p>
<p>Assuming that Flink uses a k8s namespace of <code>flink-dev</code> and does not explicitly specify K8s account, you can create a simple 		<code>clusterrolebinding</code> resource as follows:</p>
<div><pre><code>kubectl create clusterrolebinding flink-role-binding-default --clusterrole=edit --serviceaccount=flink-dev:default
</code></pre>
<div><span>1</span><br></div></div><h3 id="remote-docker-register-service"> Remote Docker Register Service</h3>
<p>You need to configure the connection information for the remote Docker register service on the Settings page of StreamX.</p>
<p><img src="/streamx-docs/assets/img/doc-img/docker_register_setting.png" alt="image-20210927182540478"></p>
<p>You need to create a namespace named <code>streamx</code> in remote docker register repository. The docker image automatically built by StreamX will be pushed to this namespace, so make sure that the <em>Docker Register User</em> has <code>pull</code> and <code>push</code> permissions for this namespace.</p>
<p>Quick test using Docker command-line tool:</p>
<div><pre><code><span># verify access</span>
docker login --username<span>=</span><span>&lt;</span>your_username<span>></span> <span>&lt;</span>your_register_addr<span>></span>
<span># verify push permission</span>
docker pull busybox
docker tag busybox <span>&lt;</span>your_register_addr<span>></span>/streamx/busybox
docker push <span>&lt;</span>your_register_addr<span>></span>/streamx/busybox
<span># verify pull permission</span>
docker pull <span>&lt;</span>your_register_addr<span>></span>/streamx/busybox
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br></div></div><br/>
<h2 id="flink-job-submission"> Flink Job Submission</h2>
<h3 id="flink-application-job"> Flink-Application Job</h3>
<p><img src="/streamx-docs/assets/img/doc-img/k8s_application_submit.png" alt="image-20210927203759713"></p>
<p>The configuration to be specified are the following:</p>
<ul>
<li><strong>Flink Base Docker Image</strong>： Tag of the base Flink Docker image can be obtained from <a href="https://hub.docker.com/_/flink" target="_blank" rel="noopener noreferrer">DockerHub - offical/flink</a>, and also supports the user's private image.</li>
<li><strong>Rest-Service Exposed Type</strong>：Corresponds to the Flink native configuration of  <a href="https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/config/#kubernetes" target="_blank" rel="noopener noreferrer">kubernetes.rest-service.exposed.type</a>:
<ul>
<li><code>ClusterIP</code>：Requires StreamX for direct access to the K8s internal network;</li>
<li><code>LoadBalancer</code>：Requires K8s to create the LoadBalancer resource in advance while StreamX can access to that LoadBalancer gateway.</li>
<li><code>NodePort</code>：Requires StreamX to be allowed to connect to all K8s nodes directly.</li>
</ul>
</li>
<li><strong>Kubernetes Pod Template</strong>： Flink custom k8s pod-template configuration.</li>
</ul>
<p>When a Flink job is started, the Flink Web UI page can be accessed directly from the Detail page of StreamX:</p>
<p><img src="/streamx-docs/assets/img/doc-img/k8s_app_detail.png" alt="image-20210927210034861"></p>
<h3 id="flink-session-job"> Flink-Session Job</h3>
<p>The additional configuration of Flink-Native-Kubernetes Session job (e.g pod-template)  is entirely determined by the Flink-Session cluster deployed in advance, please refer to Flink Document: https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/resource-providers/native_kubernetes</p>
<br/>
<h2 id="related-configuration"> Related Configuration</h2>
<p>The Flink-K8s-Integration related configuration of StreamX in application.yaml are as follows:</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Description</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>streamx.docker.register.image-namespace</td>
<td>Default namespace of remote docker register service repository.</td>
<td>steramx</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-task-timeout-sec.job-status</td>
<td>Execution timeou for each group of flink state tracking tasks.</td>
<td>120</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-task-timeout-sec.cluster-metric</td>
<td>Execution timeout for each group of flink metrics tracking tasks.</td>
<td>120</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-interval-sec.job-status</td>
<td>Execution interval for each group of flink state tracking tasks.</td>
<td>5</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-interval-sec.cluster-metric</td>
<td>Execution interval for each group of flink metrics tracking tasks</td>
<td>10</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.silent-state-keep-sec</td>
<td>Trace fault tolerance time for Slient state.</td>
<td>60</td>
</tr>
</tbody>
</table>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/docker_register_setting.png" type="image/png"/>
    </item>
    <item>
      <title>K8s PVC Usage</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/flink-k8s/2-k8s-pvc-integration/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/flink-k8s/2-k8s-pvc-integration/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">K8s PVC Usage</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="k8s-pvc-resource-usage"> K8s PVC Resource Usage</h2>
<p>The current version of StreamX Flink-K8s task has support for PVC resources based on Flink k8s-pod-template. Streamx supports editing <code>pod-template</code>, <code>jm-pod-template</code>, <code>tm-pod-template</code> configurations directly on the UI page.</p>
<br/>
<p>The following is a simple example, assuming that two PVCs, <code>flink-checkpoint</code> and <code>flink-savepoint</code>, have been created in advance.</p>
<p><img src="/streamx-docs/assets/img/doc-img/k8s_pvc.png" alt="image-20210927215912190"></p>
<p>pod-template configuration:</p>
<div><pre><code><span>apiVersion</span><span>:</span> v1
<span>kind</span><span>:</span> Pod
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> pod<span>-</span>template
<span>spec</span><span>:</span>
  <span>containers</span><span>:</span>
    <span>-</span> <span>name</span><span>:</span> flink<span>-</span>main<span>-</span>container
      <span>volumeMounts</span><span>:</span>
        <span>-</span> <span>name</span><span>:</span> checkpoint<span>-</span>pvc
          <span>mountPath</span><span>:</span> /opt/flink/checkpoints
        <span>-</span> <span>name</span><span>:</span> savepoint<span>-</span>pvc
          <span>mountPath</span><span>:</span> /opt/flink/savepoints
  <span>volumes</span><span>:</span>
    <span>-</span> <span>name</span><span>:</span> checkpoint<span>-</span>pvc
      <span>persistentVolumeClaim</span><span>:</span>
        <span>claimName</span><span>:</span> flink<span>-</span>checkpoint
    <span>-</span> <span>name</span><span>:</span> savepoint<span>-</span>pvc
      <span>persistentVolumeClaim</span><span>:</span>
        <span>claimName</span><span>:</span> flink<span>-</span>savepoint
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br></div></div><p>Since <code>rockdb-backend</code> is used, there are 3 ways to provide this dependency:</p>
<ol>
<li>
<p>The Flink Base Docker Image provided already contains this dependency.</p>
</li>
<li>
<p>Place the <code>flink-statebackend-rocksdb_xx.jar</code> in the StreamX local directory <code>Workspace/jars</code>.</p>
</li>
<li>
<p>Add the <code>rockdb-backend</code> coordinate to the Dependency configuration of the StreamX page (at this point StreamX will automatically resolve the dependency conflict):</p>
<p><img src="/streamx-docs/assets/img/doc-img/rocksdb_dependency.png" alt="image-20210927220203314"></p>
</li>
</ol>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/k8s_pvc.png" type="image/png"/>
    </item>
    <item>
      <title>Hadoop Resource Integration</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/flink-k8s/3-hadoop-resource-integration/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/flink-k8s/3-hadoop-resource-integration/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Hadoop Resource Integration</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="hadoop-resource-integration"> Hadoop Resource Integration</h2>
<p>To use Hadoop resources under StreamX Flink-K8s runtime, such as checkpoint mounting HDFS, accessing Hive, etc.  Users need to build the relevant Flink Base Docker Image by themselves. The Image needs to contain the following content：</p>
<ul>
<li>Include Hadoop Lib resource, and set <code>HADOOP_CLASSPATH</code>；</li>
<li>Include Hadoop Config resource，and set <code>HADOOP_CONF_DIR</code>；</li>
<li>Include  Hive Config Resource when using Hive;</li>
</ul>
<br/>
<p>This is actually quite inconvenient 🥲, we will support automatic integration of Hadoop features in subsequent releases, Plz look forward to !</p>
]]></content:encoded>
    </item>
    <item>
      <title>Flink SQL</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/flinksql/flinksql/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/flinksql/flinksql/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Flink SQL</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
    </item>
    <item>
      <title>框架介绍</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/guide/intro/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/guide/intro/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">框架介绍</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="streamx"> StreamX</h1>
<p>let flink|spark easy</p>
<blockquote>
<p>一个神奇的框架,让Flink开发更简单</p>
</blockquote>
<h2 id="🚀-什么是streamx"> 🚀 什么是StreamX</h2>
<p>    大数据技术如今发展的如火如荼,已经呈现百花齐放欣欣向荣的景象,实时处理流域 <code>Apache Spark</code> 和 <code>Apache Flink</code> 更是一个伟大的进步,尤其是<code>Apache Flink</code>被普遍认为是下一代大数据流计算引擎,
我们在使用 <code>Flink</code> 时发现从编程模型, 启动配置到运维管理都有很多可以抽象共用的地方, 我们将一些好的经验固化下来并结合业内的最佳实践, 通过不断努力终于诞生了今天的框架 —— <code>StreamX</code>, 项目的初衷是 —— 让 <code>Flink</code> 开发更简单,
使用<code>StreamX</code>开发,可以极大降低学习成本和开发门槛, 让开发者只用关心最核心的业务,<code>StreamX</code> 规范了项目的配置,鼓励函数式编程,定义了最佳的编程方式,提供了一系列开箱即用的<code>Connectors</code>,标准化了配置、开发、测试、部署、监控、运维的整个过程, 提供<code>scala</code>和<code>java</code>两套api,
其最终目的是打造一个一站式大数据平台,流批一体,湖仓一体的解决方案</p>
<p><video src="/streamx-docs/assets/video/streamx-video.mp4" controls="controls" autoplay="autoplay" width="100%" height="100%"></video></p>
<h2 id="🎉-features"> 🎉 Features</h2>
<ul>
<li>开发脚手架</li>
<li>多版本Flink支持(1.11,x, 1.12.x, 1.13 )</li>
<li>一系列开箱即用的connectors</li>
<li>支持项目编译功能(maven 编译)</li>
<li>在线参数配置</li>
<li>支持<code>Applicaion</code> 模式, <code>Yarn-Per-Job</code>模式启动</li>
<li>快捷的日常操作(任务<code>启动</code>、<code>停止</code>、<code>savepoint</code>,从<code>savepoint</code>恢复)</li>
<li>支持火焰图</li>
<li>支持<code>notebook</code>(在线任务开发)</li>
<li>项目配置和依赖版本化管理</li>
<li>支持任务备份、回滚(配置回滚)</li>
<li>在线管理依赖(maven pom)和自定义jar</li>
<li>自定义udf、连接器等支持</li>
<li>Flink SQL WebIDE</li>
<li>支持catalog、hive</li>
<li>任务运行失败发送告警邮件</li>
<li>支持失败重启重试</li>
<li>从任务<code>开发</code>阶段到<code>部署管理</code>全链路支持</li>
<li>...</li>
</ul>
<h2 id="🏳‍🌈-组成部分"> 🏳‍🌈 组成部分</h2>
<p><code>Streamx</code>有三部分组成,分别是<code>streamx-core</code>,<code>streamx-pump</code> 和 <code>streamx-console</code></p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_archite.png"/><br>
</center>
<h3 id="_1️⃣-streamx-core"> 1️⃣ streamx-core</h3>
<p><code>streamx-core</code> 定位是一个开发时框架,关注编码开发,规范了配置文件,按照约定优于配置的方式进行开发,提供了一个开发时 <code>RunTime Content</code>和一系列开箱即用的<code>Connector</code>,扩展了<code>DataStream</code>相关的方法,融合了<code>DataStream</code>和<code>Flink sql</code> api,简化繁琐的操作,聚焦业务本身,提高开发效率和开发体验</p>
<h3 id="_2️⃣-streamx-pump"> 2️⃣ streamx-pump</h3>
<p><code>pump</code> 是抽水机,水泵的意思,<code>streamx-pump</code>的定位是一个数据抽取的组件,类似于<code>flinkx</code>,基于<code>streamx-core</code>中提供的各种<code>connector</code>开发,目的是打造一个方便快捷,开箱即用的大数据实时数据抽取和迁移组件,并且集成到<code>streamx-console</code>中,解决实时数据源获取问题,目前在规划中</p>
<h3 id="_3️⃣-streamx-console"> 3️⃣ streamx-console</h3>
<p><code>streamx-console</code> 是一个综合实时数据平台,低代码(<code>Low Code</code>)平台,可以较好的管理<code>Flink</code>任务,集成了项目编译、发布、参数配置、启动、<code>savepoint</code>,火焰图(<code>flame graph</code>),<code>Flink SQL</code>,
监控等诸多功能于一体,大大简化了<code>Flink</code>任务的日常操作和维护,融合了诸多最佳实践。旧时王谢堂前燕,飞入寻常百姓家,让大公司有能力研发使用的项目,现在人人可以使用,
其最终目标是打造成一个实时数仓,流批一体的一站式大数据解决方案,该平台使用但不仅限以下技术:</p>
<ul>
<li><a href="http://flink.apache.org" target="_blank" rel="noopener noreferrer">Apache Flink</a></li>
<li><a href="http://hadoop.apache.org" target="_blank" rel="noopener noreferrer">Apache YARN</a></li>
<li><a href="https://spring.io/projects/spring-boot/" target="_blank" rel="noopener noreferrer">Spring Boot</a></li>
<li><a href="http://www.mybatis.org" target="_blank" rel="noopener noreferrer">Mybatis</a></li>
<li><a href="http://mp.baomidou.com" target="_blank" rel="noopener noreferrer">Mybatis-Plus</a></li>
<li><a href="http://www.brendangregg.com/FlameGraphs" target="_blank" rel="noopener noreferrer">Flame Graph</a></li>
<li><a href="https://github.com/uber-common/jvm-profiler" target="_blank" rel="noopener noreferrer">JVM-Profiler</a></li>
<li><a href="https://cn.vuejs.org/" target="_blank" rel="noopener noreferrer">Vue</a></li>
<li><a href="https://vuepress.vuejs.org/" target="_blank" rel="noopener noreferrer">VuePress</a></li>
<li><a href="https://antdv.com/" target="_blank" rel="noopener noreferrer">Ant Design of Vue</a></li>
<li><a href="https://pro.antdv" target="_blank" rel="noopener noreferrer">ANTD PRO VUE</a></li>
<li><a href="https://xtermjs.org/" target="_blank" rel="noopener noreferrer">xterm.js</a></li>
<li><a href="https://microsoft.github.io/monaco-editor/" target="_blank" rel="noopener noreferrer">Monaco Editor</a></li>
<li>...</li>
</ul>
<p>感谢以上优秀的开源项目和很多未提到的优秀开源项目,给予最大的respect,特别感谢<a href="http://zeppelin.apache.org" target="_blank" rel="noopener noreferrer">Apache Zeppelin</a>,<a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener noreferrer">IntelliJ IDEA</a>,
感谢<a href="https://github.com/GuoNingNing/fire-spark" target="_blank" rel="noopener noreferrer">fire-spark</a>项目,早期给予的灵感和帮助, 感谢我老婆在项目开发时给予的支持,悉心照顾我的生活和日常,给予我足够的时间开发这个项目</p>
<h2 id="👻-为什么不是-❓"> 👻 为什么不是...❓</h2>
<h3 id="apache-zeppelin"> Apache Zeppelin</h3>
<p><a href="http://zeppelin.apache.org" target="_blank" rel="noopener noreferrer">Apache Zeppelin</a>是一个非常优秀的开源项目👏 对<code>Flink</code>做了很好的支持,<code>Zeppelin</code>创新型的<code>notebook</code>功能,让开发者非常方便的<code>On-line</code>编程,快捷的提交任务,语言层面同时支持<code>java</code>,<code>scala</code>,<code>python</code>,国内阿里的章剑峰大佬也在积极推动该项目,向剑峰大佬致以崇高的敬意🙏🙏🙏,
但该项目目前貌似没有解决项目的管理和运维方面的痛点,针对比较复杂的项目和大量的作业管理就有些力不从心了,一般来讲不论是<code>DataStream</code>作业还是<code>Flink SQL</code>作业,大概都会经历作业的<code>开发阶段</code>,<code>测试阶段</code>,<code>打包阶段</code>,<code>上传服务器阶段</code>,<code>启动任务阶段</code>等这些步骤,这是一个链路很长的步骤,且整个过程耗时比较长,体验不好,
即使修改了一个符号,项目改完上线都得走上面的流程,我们期望这些步骤能够动动鼠标一键式解决,还希望至少能有一个任务列表的功能,能够方便的管理任务,可以清楚的看到哪些任务正在运行,哪些停止了,任务的资源消耗情况,可以在任务列表页面一键<code>启动</code>或<code>停止</code>任务,并且自动管理<code>savePoint</code>,这些问题也是开发者实际开发中会遇到了问题,
<code>streamx-console</code>很好的解决了这些痛点,定位是一个一站式实时数据平台,并且开发了更多令人激动的功能(诸如<code>Flink SQL WebIDE</code>,<code>依赖隔离</code>,<code>任务回滚</code>,<code>火焰图</code>等)</p>
<h3 id="flinkx"> FlinkX</h3>
<p><a href="http://github.com/DTStack/flinkx" target="_blank" rel="noopener noreferrer">FlinkX</a> 是基于flink的分布式数据同步工具,实现了多种异构数据源之间高效的数据迁移,定位比较明确,专门用来做数据抽取和迁移,可以作为一个服务组件来使用,<code>StreamX</code>关注开发阶段和任务后期的管理,定位有所不同,<code>streamx-pump</code>模块也在规划中,
致力于解决数据源抽取和迁移,最终会集成到<code>streamx-console</code>中</p>
]]></content:encoded>
    </item>
    <item>
      <title>快速开始</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/guide/quickstart/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/guide/quickstart/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">快速开始</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>千里之行始于足下,从这一刻开始即将进入<code>StreamX</code> 开发的奇妙旅程,准备好了吗? 让我们开始吧</p>
<h2 id="起步"> 起步</h2>
<p>我们要做的第一件事就是将项目clone到本地,执行编译,在编译前请确保以下事项</p>
<div>
<ul>
<li>确保本机安装的JDK<code>1.8</code>及以上的版本</li>
<li>确保本机已经安装了maven</li>
</ul>
</div>
<p>如果准备就绪,就可以clone项目并且执行编译了</p>
<div><pre><code><span>git</span> clone https://github.com/streamxhub/streamx.git
<span>cd</span> Streamx
mvn clean <span>install</span> -DskipTests
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>顺利的话就会看到编译成功.</p>
<div><pre><code>[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Streamx 1.0.0:
[INFO]
[INFO] Streamx ............................................ SUCCESS [  1.882 s]
[INFO] Streamx : Common ................................... SUCCESS [ 15.700 s]
[INFO] Streamx : Flink Parent ............................. SUCCESS [  0.032 s]
[INFO] Streamx : Flink Common ............................. SUCCESS [  8.243 s]
[INFO] Streamx : Flink Core ............................... SUCCESS [ 17.332 s]
[INFO] Streamx : Flink Test ............................... SUCCESS [ 42.742 s]
[INFO] Streamx : Spark Parent ............................. SUCCESS [  0.018 s]
[INFO] Streamx : Spark Core ............................... SUCCESS [ 12.028 s]
[INFO] Streamx : Spark Test ............................... SUCCESS [  5.828 s]
[INFO] Streamx : Spark Cli ................................ SUCCESS [  0.016 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  01:43 min
[INFO] Finished at: 2021-03-15T17:02:22+08:00
[INFO] ------------------------------------------------------------------------
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br></div></div><h2 id="datastream"> DataStream</h2>
<p>从这里开始用StreamX开发第一个<code>DataStream</code> 程序,从最简单的需求入手,这里的需求是从<code>kafka</code>读取用户数据写入<code>mysql</code>,需求如下:</p>
<div>
<ul>
<li>从kafka里读取用户的数据写入到mysql中</li>
<li>只要年龄小于30岁的用户数据</li>
<li><code>kafka</code> 的 <code>topic</code>为<code>test_user</code></li>
<li>要写入的<code>mysql</code>的表为<code>t_user</code></li>
</ul>
</div>
<p><code>kafka</code>的<code>topic</code> 的数据格式如下</p>
<div><pre><code><span>{</span>
  <span>"name"</span> <span>:</span> <span>"$name"</span><span>,</span>
  <span>"age"</span> <span>:</span> $age<span>,</span>
  <span>"gender"</span> <span>:</span> $gender<span>,</span>
  <span>"address"</span> <span>:</span> <span>"$address"</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><p><code>mysql</code>表<code>t_user</code>结构如下:</p>
<div><pre><code><span>create</span> <span>table</span> <span>user</span><span>(</span>
    <span>`</span>name<span>`</span> <span>varchar</span><span>(</span><span>32</span><span>)</span><span>,</span>
    <span>`</span>age<span>`</span> <span>int</span><span>(</span><span>3</span><span>)</span><span>,</span>
    <span>`</span>gender<span>`</span> <span>int</span><span>(</span><span>1</span><span>)</span><span>,</span>
    <span>`</span>address<span>`</span> <span>varchar</span><span>(</span><span>255</span><span>)</span>
<span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><h3 id="编码开发"> 编码开发</h3>
<p>用Streamx开发一个这样的需求非常简单,只需要几行代码就搞定了</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>quickstart</span>

<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>/**
 * @author benjobs
 */</span>
<span>object</span> QuickStartApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span>
      <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
      <span>.</span>map<span>(</span>x <span>=></span> JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>x<span>.</span>value<span>)</span><span>)</span>
      <span>.</span>filter<span>(</span>_<span>.</span>age <span>&lt;</span> <span>30</span><span>)</span>

    JdbcSink<span>(</span><span>)</span><span>.</span>sink<span>[</span>User<span>]</span><span>(</span>source<span>)</span><span>(</span>user <span>=></span>
    s<span>"""
        |insert into t_user(`name`,`age`,`gender`,`address`)
        |value('${user.name}',${user.age},${user.gender},'${user.address}')
        |"""</span><span>.</span>stripMargin
    <span>)</span>
  <span>}</span>
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span> <span>String</span><span>,</span> age<span>:</span> <span>Int</span><span>,</span> gender<span>:</span> <span>Int</span><span>,</span> address<span>:</span> <span>String</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>SQLFromFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>sink<span>.</span></span><span>JdbcSink</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>FilterFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>api<span>.</span>datastream<span>.</span></span><span>DataStream</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>/**
 * @author benjobs
 */</span>
<span>public</span> <span>class</span> <span>QuickStartJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>

        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>

        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>

        <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>

        <span>DataStream</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span>
                        mapper<span>.</span><span>readValue</span><span>(</span>value<span>.</span><span>value</span><span>(</span><span>)</span><span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>)</span>
                <span>.</span><span>filter</span><span>(</span><span>(</span><span>FilterFunction</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span> value<span>.</span>age <span>&lt;</span> <span>30</span><span>)</span><span>;</span>

        <span>new</span> <span>JdbcSink</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>sql</span><span>(</span><span>(</span><span>SQLFromFunction</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> <span>JavaUser</span><span>::</span><span>toSql</span><span>)</span>
                <span>.</span><span>towPCSink</span><span>(</span>source<span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>

    <span>public</span> <span>String</span> <span>toSql</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>format</span><span>(</span>
                <span>"insert into t_user(`name`,`age`,`gender`,`address`) value('%s',%d,%d,'%s')"</span><span>,</span>
                name<span>,</span>
                age<span>,</span>
                gender<span>,</span>
                address<span>)</span><span>;</span>
    <span>}</span>

<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>提示</p>
<p>项目工程和代码已经提供好了,位于 <a href="https://github.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">streamx-quickstart</a> 开箱即用,不需要写一行代码</p>
</div>
<h3 id="配置文件"> 配置文件</h3>
<p><code>kafka</code> 和 <code>mysql</code>相关准备工作就绪了,接下来要改项目的配置,找到项目路径下的<code>assembly/conf/application.yml</code>
这里有很多配置项,先不用管,也不用改,现在只关注 <code>kafka</code> 和 <code>mysql</code> 两项配置</p>
<div><pre><code><span># kafka source</span>
<span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> test_user
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest

<span># jdbc</span>
<span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE
  <span>driverClassName</span><span>:</span> com.mysql.cj.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></div></div><p>找到上面两项配置修改相关的 <code>kafka</code> 和 <code>mysql</code> 信息即可.</p>
<div><p>提示</p>
<p><code>配置</code>是非常重要的概念,会在下一章节进行介绍说明</p>
</div>
<h3 id="本地测试"> 本地测试</h3>
<p>准备工作完毕现在启动项目进行本地测试,找到项目主类<code>com.streamxhub.streamx.quickstart.QuickStartApp</code> 运行启动,并且加上参数
<code>--conf $path</code>,path即为当前项目的配置文件的路径(上面修改的配置文件),如果没啥意外的话,项目就会在本地启动成功,会看到<code>kafka</code>里的数据按照需求成功的写入到<code>mysql</code>中</p>
<div><p>注意事项</p>
<p>项目启动时配置文件必须要指定,如不指定该参数会报错如下:</p>
<div><pre><code>Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError: 
[Streamx] Usage:can&#39;t fond config,please set &quot;--conf $path &quot; in main arguments
	at com.streamxhub.streamx.flink.core.scala.util.FlinkStreamingInitializer.initParameter(FlinkStreamingInitializer.scala:126)
	at com.streamxhub.streamx.flink.core.scala.util.FlinkStreamingInitializer.&lt;init&gt;(FlinkStreamingInitializer.scala:85)
	at com.streamxhub.streamx.flink.core.scala.util.FlinkStreamingInitializer$.initStream(FlinkStreamingInitializer.scala:55)
	at com.streamxhub.streamx.flink.core.scala.FlinkStreaming$class.main(FlinkStreaming.scala:94)
	at com.streamxhub.streamx.quickstart.QuickStartApp$.main(QuickStartApp.scala:9)
	at com.streamxhub.streamx.quickstart.QuickStartApp.main(QuickStartApp.scala)

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></div>
<h3 id="部署上线"> 部署上线</h3>
<p>当程序开发完成并且通过本地测试后,需要往测试或生产环境发布,一般的方式是将jar包上传到集群的某一台部署机,进行<code>flink run</code>命令启动项目,然后跟上各种资源参数,
虽然flink启动的时候官方提供了很多短参数的方式进行设置(-c -m -jar等),但是这种方式,不仅可读性差,还极容易出错,稍不注意一个参数不合法就会导致任务启动失败,
在StreamX里这些都简化了,StreamX启动项目如下</p>
<div>
<ul>
<li>解项目压缩包</li>
<li>执行启动脚本</li>
</ul>
</div>
<h3 id="项目结构"> 项目结构</h3>
<p>解包<code>Streamx-flink-quickstart-1.0.0.tar.gz</code>后项目的目录结构如下:</p>
<div><pre><code>.
Streamx-flink-quickstart-1.0.0
├── bin
│   ├── startup.sh                             //启动脚本  
│   ├── setclasspath.sh                        //java环境变量相关的脚本(内部使用的,用户无需关注)
│   ├── shutdown.sh                            //任务停止脚本(不建议使用)
│   ├── flink.sh                               //启动时内部使用到的脚本(内部使用的,用户无需关注)
├── conf                           
|   ├── application.yaml                       //项目的配置文件
├── lib
│   └── Streamx-flink-quickstart-1.0.0.jar     //项目的jar包
└── temp
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><h3 id="启动命令"> 启动命令</h3>
<p>启动之前要确定<code>conf/application.yaml</code>下的配置文件,在当前<code>Streamx-flink-quickstart</code>项目里都已经配置好了,不需改动,直接启动项目即可:</p>
<div><pre><code>bin/startup.sh --conf conf/application.yaml 
</code></pre>
<div><span>1</span><br></div></div><h2 id="flink-sql"> Flink Sql</h2>
<p>从这里开始用StreamX开发第一个Flink Sql 程序,利用Flink内置的 <code>DataGen</code>和<code>print</code> Connectors 来完成</p>
<div><p>提示</p>
<p>项目工程和代码已经提供好了,位于 <a href="https://github.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">streamx-quickstart</a> 开箱即用,不需要写一行代码</p>
</div>
<h3 id="编码开发-2"> 编码开发</h3>
<p>下面的代码演示了如何用<code>scala</code>和<code>java</code>两种api开发这样一个需求,全部代码如下</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>test<span>.</span>tablesql</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreamTable

<span>object</span> HelloFlinkSQL <span>extends</span> FlinkStreamTable <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>/**
     * 一行胜千言
     */</span>
    context<span>.</span>sql<span>(</span><span>"myflinksql"</span><span>)</span>
  <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java" active>
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>TableContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>TableEnvConfig</span><span>;</span>

<span>public</span> <span>class</span> <span>HelloFlinkSQL</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>TableEnvConfig</span> tableEnvConfig <span>=</span> <span>new</span> <span>TableEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>tableConfig<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
            <span>System</span><span>.</span>out<span>.</span><span>println</span><span>(</span><span>"set tableConfig..."</span><span>)</span><span>;</span>
        <span>}</span><span>)</span><span>;</span>

        <span>TableContext</span> context <span>=</span> <span>new</span> <span>TableContext</span><span>(</span>tableEnvConfig<span>)</span><span>;</span>
        context<span>.</span><span>sql</span><span>(</span><span>"myflinksql"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="配置文件-2"> 配置文件</h3>
<p>找到项目路径下的assembly/conf/sql.yml,内容如下</p>
<div><pre><code>myflinksql: <span>|</span>
  <span>CREATE</span> <span>TABLE</span> datagen <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING<span>,</span>
    ts <span>AS</span> localtimestamp<span>,</span>
    WATERMARK <span>FOR</span> ts <span>AS</span> ts
  <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'datagen'</span><span>,</span>
    <span>-- optional options --</span>
    <span>'rows-per-second'</span><span>=</span><span>'5'</span><span>,</span>
    <span>'fields.f_sequence.kind'</span><span>=</span><span>'sequence'</span><span>,</span>
    <span>'fields.f_sequence.start'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_sequence.end'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random.min'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_random.max'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random_str.length'</span><span>=</span><span>'10'</span>
  <span>)</span><span>;</span>

  <span>CREATE</span> <span>TABLE</span> print_table <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING
    <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'print'</span>
  <span>)</span><span>;</span>

  <span>INSERT</span> <span>INTO</span> print_table <span>select</span> f_sequence<span>,</span>f_random<span>,</span>f_random_str <span>from</span> datagen<span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div><h3 id="本地测试-2"> 本地测试</h3>
<p>找到项目主类<code>com.streamxhub.streamx.quickstart.HelloFlinkSQL</code> 运行启动,并且加上参数
<code>--sql $path</code>,path即为当前项目的sql文件的路径,如果没啥意外的话,项目就会在本地启动成功,生成的数据会打印到控制台</p>
]]></content:encoded>
    </item>
    <item>
      <title>项目配置</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/model/conf/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/model/conf/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">项目配置</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>配置在<code>StreamX</code>中是非常重要的概念,先说说为什么需要配置</p>
<h2 id="为什么需要配置"> 为什么需要配置</h2>
<p>开发<code>DataStream</code>程序,大体流程都可以抽象为以下4步</p>
<div>
<ul>
<li>StreamExecutionEnvironment初始并配置</li>
<li>Source接入数据</li>
<li>Transformation逻辑处理</li>
<li>Sink结果数据落地</li>
</ul>
</div>
<p><img src="/streamx-docs/assets/img/doc-img/process_steps.png" alt=""></p>
<p>开发<code>DataStream</code>程序都需要定义<code>Environment</code>初始化并且配置环境相关的参数,一般我们都会在第一步初始化<code>Environment</code>并配置各种参数,配置的参数大概有以下几类</p>
<div>
<ul>
<li>Parallelism 默认并行度配置</li>
<li>TimeCharacteristic 时间特征配置</li>
<li>checkpoint 检查点的相关配置</li>
<li>Watermark 相关配置</li>
<li>State Backend 状态后端配置</li>
<li>Restart Strategy 重启策略配置</li>
<li>其他配置...</li>
</ul>
</div>
<p>以上的配置基本都是比较普遍且通用的,是每个程序上来第一步就要定义的,是一项重复的工作</p>
<p>当程序写好后,要上线运行,任务启动提交都差不多用下面的命令行的方式,设置各种启动参数,
这时就得开发者清楚的知道每个参数的含义,如果再设置几个运行时资源参数,那启动命名会很长,可读性很差,参数解析用到了强校验,一旦设置错误,会直接报错,导致任务启动失败,最直接的异常是 <mark>找不到程序的jar</mark></p>
<div><pre><code>flink run -m yarn-cluster -p <span>1</span> -c com.xx.Main job.jar
</code></pre>
<div><span>1</span><br></div></div><p>开发<code>Flink Sql</code>程序,也需要设置一系列环境参数,除此之外,如果要使用纯sql的方式开发,举一个最简单的例子,代码如下</p>
<div><pre><code><span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>table<span>.</span>api<span>.</span></span><span>EnvironmentSettings</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>table<span>.</span>api<span>.</span></span><span>Table</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>table<span>.</span>api<span>.</span></span><span>TableEnvironment</span><span>;</span>

<span>public</span> <span>class</span> <span>JavaTableApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>EnvironmentSettings</span> bbSettings <span>=</span> <span>EnvironmentSettings</span>
        <span>.</span><span>newInstance</span><span>(</span><span>)</span>
        <span>.</span><span>useBlinkPlanner</span><span>(</span><span>)</span>
        <span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>
        
        <span>TableEnvironment</span> bsTableEnv <span>=</span> <span>TableEnvironment</span><span>.</span><span>create</span><span>(</span>bbSettings<span>)</span><span>;</span>

        <span>String</span> sourceDDL <span>=</span> <span>"CREATE TABLE datagen (  "</span> <span>+</span>
                <span>" f_random INT,  "</span> <span>+</span>
                <span>" f_random_str STRING,  "</span> <span>+</span>
                <span>" ts AS localtimestamp,  "</span> <span>+</span>
                <span>" WATERMARK FOR ts AS ts  "</span> <span>+</span>
                <span>") WITH (  "</span> <span>+</span>
                <span>" 'connector' = 'datagen',  "</span> <span>+</span>
                <span>" 'rows-per-second'='10',  "</span> <span>+</span>
                <span>" 'fields.f_random.min'='1',  "</span> <span>+</span>
                <span>" 'fields.f_random.max'='5',  "</span> <span>+</span>
                <span>" 'fields.f_random_str.length'='10'  "</span> <span>+</span>
                <span>")"</span><span>;</span>

        bsTableEnv<span>.</span><span>executeSql</span><span>(</span>sourceDDL<span>)</span><span>;</span>

        <span>String</span> sinkDDL <span>=</span> <span>"CREATE TABLE print_table ("</span> <span>+</span>
                <span>" f_random int,"</span> <span>+</span>
                <span>" c_val bigint, "</span> <span>+</span>
                <span>" wStart TIMESTAMP(3) "</span> <span>+</span>
                <span>") WITH ('connector' = 'print') "</span><span>;</span>
        
        bsTableEnv<span>.</span><span>executeSql</span><span>(</span>sinkDDL<span>)</span><span>;</span>
    <span>}</span>

<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br></div></div><p>我们会看到除了设置EnvironmentSettings参数之外,剩下的几乎大段大段的代码都是在写<code>sql</code>,用java代码拼接各种sql,这种编码的方式,极不优雅,如果业务复杂,更是难以维护,而且会发现,整个编码的模式是统一的,
都是声明一段sql,然后调用<code>executeSql</code>方法</p>
<p><strong>我们的设想是</strong>:能不能以一种更好的方式将这种重复的工作简单化,将<code>DataStream</code>和<code>Flink Sql</code>任务中的一些环境初始化相关的参数和启动相关参数简化,最好一行代码都不写,针对<code>Flink Sql</code>作业,也不想在代码里写大段的sql,能不能以一种更优雅的方式解决?</p>
<p><strong>答案是肯定的</strong></p>
<p>针对参数设置的问题,在<code>StreamX</code>中提出统一程序配置的概念,把程序的一系列参数从开发到部署阶段按照特定的格式配置到<code>application.yml</code>里,抽象出
一个通用的配置模板,按照这种规定的格式将上述配置的各项参数在配置文件里定义出来,在程序启动的时候将这个项目配置传入到程序中即可完成环境的初始化工作,在任务启动的时候也会自动识别启动时的参数,于是就有了<code>配置文件</code>这一概念</p>
<p>针对Flink Sql作业在代码里写sql的问题,<code>StreamX</code>针对<code>Flink Sql</code>作业做了更高层级封装和抽象,开发者只需要将sql按照一定的规范要求定义到<code>sql.yaml</code>文件中,在程序启动时将该sql文件传入到主程序中,
就会自动按照要求加载执行sql,于是就有了<code>sql文件</code>的概念</p>
<h2 id="相关术语"> 相关术语</h2>
<p>为了方便开发者理解和相互交流,我们把上面引出的,把程序的一系列参数从开发到部署阶段按照特定的格式配置到文件里,这个有特定作用的文件就是项目的 <strong> <mark><code>配置文件</code></mark> </strong></p>
<p>Flink Sql任务中将提取出来的sql放到<code>sql.yaml</code>中,这个有特定作用的文件就是项目的 <strong> <mark><code>sql文件</code></mark> </strong></p>
<h2 id="配置文件"> 配置文件</h2>
<p>在StreamX中,<code>DataStream</code>作业和<code>Flink Sql</code>作业配置文件是通用的,换言之,这个配置文件既能定义<code>DataStream</code>的各项配置,也能定义<code>Flink Sql</code>的各项配置(Flink Sql作业中配置文件是可选的), 配置文件的格式必须是<code>yaml</code>格式, 必须得符合yaml的格式规范</p>
<p>下面我们来详细看看这个配置文件的各项配置都是如何进行配置的,有哪些注意事项</p>
<div><pre><code><span>flink</span><span>:</span>
  <span>deployment</span><span>:</span>
    <span>option</span><span>:</span>
      <span>target</span><span>:</span> application
      <span>detached</span><span>:</span>
      <span>shutdownOnAttachedExit</span><span>:</span>
      <span>jobmanager</span><span>:</span>
    <span>property</span><span>:</span> <span>#@see: https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html</span>
      <span>$internal.application.main</span><span>:</span> com.streamxhub.streamx.flink.quickstart.QuickStartApp
      <span>yarn.application.name</span><span>:</span> Streamx QuickStart App
      <span>yarn.application.queue</span><span>:</span> 
      <span>taskmanager.numberOfTaskSlots</span><span>:</span> <span>1</span>
      <span>parallelism.default</span><span>:</span> <span>2</span>
      <span>jobmanager.memory</span><span>:</span>
        <span>flink.size</span><span>:</span>
        <span>heap.size</span><span>:</span>
        <span>jvm-metaspace.size</span><span>:</span>
        <span>jvm-overhead.max</span><span>:</span>
        <span>off-heap.size</span><span>:</span>
        <span>process.size</span><span>:</span>
      <span>taskmanager.memory</span><span>:</span>
        <span>flink.size</span><span>:</span>
        <span>framework.heap.size</span><span>:</span>
        <span>framework.off-heap.size</span><span>:</span>
        <span>managed.size</span><span>:</span>
        <span>process.size</span><span>:</span>
        <span>task.heap.size</span><span>:</span>
        <span>task.off-heap.size</span><span>:</span>
        <span>jvm-metaspace.size</span><span>:</span>
        <span>jvm-overhead.max</span><span>:</span>
        <span>jvm-overhead.min</span><span>:</span>
        <span>managed.fraction</span><span>:</span> <span>0.4</span>
  <span>checkpoints</span><span>:</span>
    <span>enable</span><span>:</span> <span>true</span>
    <span>interval</span><span>:</span> <span>30000</span>
    <span>mode</span><span>:</span> EXACTLY_ONCE
    <span>timeout</span><span>:</span> <span>300000</span>
    <span>unaligned</span><span>:</span> <span>true</span>
  <span>watermark</span><span>:</span>
    <span>interval</span><span>:</span> <span>10000</span>
  <span># 状态后端</span>
  <span>state</span><span>:</span>
    <span>backend</span><span>:</span> <span># see https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html</span>
      <span>value</span><span>:</span> filesystem <span># 保存类型('jobmanager', 'filesystem', 'rocksdb')</span>
      <span>memory</span><span>:</span> <span>5242880</span> <span># 针对jobmanager有效,最大内存</span>
      <span>async</span><span>:</span> <span>false</span>    <span># 针对(jobmanager,filesystem)有效,是否开启异步</span>
      <span>incremental</span><span>:</span> <span>true</span> <span>#针对rocksdb有效,是否开启增量</span>
      <span>#rocksdb 的配置参考 https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#rocksdb-state-backend</span>
      <span>#rocksdb配置key的前缀去掉:state.backend</span>
      <span>#rocksdb.block.blocksize:</span>
    <span>checkpoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
    <span>savepoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
  <span># 重启策略</span>
  <span>restart-strategy</span><span>:</span>
    <span>value</span><span>:</span> fixed<span>-</span>delay  <span>#重启策略[(fixed-delay|failure-rate|none)共3个可配置的策略]</span>
    <span>fixed-delay</span><span>:</span>
      <span>attempts</span><span>:</span> <span>3</span>
      <span>delay</span><span>:</span> <span>5000</span>
    <span>failure-rate</span><span>:</span>
      <span>max-failures-per-interval</span><span>:</span>
      <span>failure-rate-interval</span><span>:</span>
      <span>delay</span><span>:</span>
  <span># table</span>
  <span>table</span><span>:</span>
    <span>planner</span><span>:</span> blink <span># (blink|old|any)</span>
    <span>mode</span><span>:</span> streaming <span>#(batch|streaming)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br><span>59</span><br><span>60</span><br><span>61</span><br><span>62</span><br><span>63</span><br><span>64</span><br><span>65</span><br><span>66</span><br></div></div><p>上面是关于<code>开发时</code>和<code>项目部署</code>时需要关注的环境相关的完整的配置,这些配置是在<code>flink</code>的namespace下进行配置的,主要分为2大类</p>
<ul>
<li><code>deployment</code>下的配置是项目部署相关的配置(<code>即项目启动时的一系列资源相关的配置参数</code>)</li>
<li>其他是开发时需要关注的环境相关的配置</li>
</ul>
<p>开发时需要关注的环境相关的配置有5项</p>
<div>
<ul>
<li><code>checkpoints</code></li>
<li><code>watermark</code></li>
<li><code>state</code></li>
<li><code>restart-strategy</code></li>
<li><code>table</code></li>
</ul>
</div>
<h3 id="deployment"> Deployment</h3>
<p>deployment下放的是部署相关的参数和配置项,具体又分为两类</p>
<ul>
<li><code>option</code></li>
<li><code>property</code></li>
</ul>
<h4 id="option"> option</h4>
<p><code>option</code>下放的参数是flink run 下支持的参数,目前支持的参数如下
<ClientOnly>
<table-data name="option"></table-data>
</ClientOnly>
<code>parallelism</code> (-p) 并行度不支持在option里配置,会在后面的property里配置
<code>class</code> (-c) 程序main不支持在option里配置,会在后面的property里配置</p>
<div><p>注意事项</p>
<p>option下的参数必须是 <code>完整参数名</code></p>
</div>
<h4 id="property"> property</h4>
<p><code>property</code>下放的参数是标准参数-D下的参数,可以分为两类</p>
<ul>
<li>基础参数</li>
<li>Memory参数</li>
</ul>
<h5 id="基础参数"> 基础参数</h5>
<p>基础参数可以配置的选项非常之多,这里举例5个最基础的设置
<ClientOnly>
<table-data name="property"></table-data>
</ClientOnly></p>
<div><p>注意事项</p>
<p><code>$internal.application.main</code> 和 <code>yarn.application.name</code> 这两个参数是必须的</p>
</div>
<p>如您需要设置更多的参数,可参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html" target="_blank" rel="noopener noreferrer"><code>这里</code></a>
一定要将这些参数放到<code>property</code>下,并且参数名称要正确,<code>StreamX</code>会自动解析这些参数并生效</p>
<h5 id="memory参数"> Memory参数</h5>
<p>Memory相关的参数设置也非常之多,一般常见的配置如下</p>
<ClientOnly>
  <table-data name="memory"></table-data>
</ClientOnly>
<p>同样,如你想配置更多的内存相关的参数,请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup.html" target="_blank" rel="noopener noreferrer"><code>这里</code></a> 查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup.html" target="_blank" rel="noopener noreferrer"><code>Flink Process Memory</code></a> , <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_tm.html" target="_blank" rel="noopener noreferrer"><code>jobmanager</code></a> 及 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_jobmanager.html" target="_blank" rel="noopener noreferrer"><code>taskmanager</code></a>
相关的内存配置将这些参数放到<code>property</code>下,保证参数正确即可生效</p>
<h5 id="配置总内存"> 配置总内存</h5>
<p>Flink JVM 进程的进程总内存（Total Process Memory）包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。 Flink 总内存（Total Flink Memory）包括 JVM 堆内存（Heap Memory）和堆外内存（Off-Heap Memory）。 其中堆外内存包括直接内存（Direct Memory）和本地内存（Native Memory）</p>
<center>
<img src="/streamx-docs/assets/img/doc-img/process_mem_model.svg" width="340px"/>
</center>
<p>配置 Flink 进程内存最简单的方法是指定以下两个配置项中的任意一个：</p>
<ClientOnly>
<table-data name="totalMem"></table-data>
</ClientOnly>
<div><p>注意事项</p>
<p>不建议同时设置进程总内存和 Flink 总内存。 这可能会造成内存配置冲突，从而导致部署失败。 额外配置其他内存部分时，同样需要注意可能产生的配置冲突。</p>
</div>
<h3 id="checkpoints"> Checkpoints</h3>
<p>Checkpoints 的配置比较简单,按照下面的方式进行配置即可</p>
<ClientOnly>
  <table-data name="checkpoints"></table-data>
</ClientOnly>
<h3 id="watermark"> Watermark</h3>
<p><code>watermark</code> 配置只需要设置下Watermark的生成周期<code>interval</code>即可</p>
<h3 id="state"> State</h3>
<p><code>state</code>是设置状态相关的配置</p>
<div><pre><code><span>state</span><span>:</span>
  <span>backend</span><span>:</span> <span># see https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html</span>
    <span>value</span><span>:</span> filesystem <span># 保存类型('jobmanager', 'filesystem', 'rocksdb')</span>
    <span>memory</span><span>:</span> <span>5242880</span> <span># 针对jobmanager有效,最大内存</span>
    <span>async</span><span>:</span> <span>false</span>    <span># 针对(jobmanager,filesystem)有效,是否开启异步</span>
    <span>incremental</span><span>:</span> <span>true</span> <span>#针对rocksdb有效,是否开启增量</span>
    <span>#rocksdb 的配置参考 https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#rocksdb-state-backend</span>
    <span>#rocksdb配置key的前缀去掉:state.backend</span>
    <span>#rocksdb.block.blocksize:</span>
  <span>checkpoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
  <span>savepoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
  <span>checkpoints.num-retained</span><span>:</span> <span>1</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><p>我们可以看到大体可以分为两类</p>
<ul>
<li>backend 相关的配置</li>
<li>checkpoints 相关的配置</li>
</ul>
<h4 id="backend"> backend</h4>
<p>很直观的,<code>backend</code>下是设置状态后端相关的配置,状态后台的配置遵照<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html" target="_blank" rel="noopener noreferrer"><code>官网文档</code></a>的配置规则,在这里支持以下配置</p>
<ClientOnly>
  <table-data name="backend"></table-data>
</ClientOnly>
<p>如果<code>backend</code>的保存类型为<code>rocksdb</code>,则可能要进一步设置<code>rocksdb</code>相关的配置,可以参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#rocksdb-state-backend" target="_blank" rel="noopener noreferrer"><code>官网</code></a>来进行相关配置,
需要注意的是官网关于<code>rocksdb</code>的配置都是以<code>state.backend</code>为前缀,而当前的命名空间就是在<code>state.backend</code>下,注意要保证参数名正确</p>
<div><p>注意事项</p>
<p><code>value</code>项非标准配置,该项用来设置状态的保存类型(<code>jobmanager</code> | <code>filesystem</code> | <code>rocksdb</code>),其他项均为标准配置,遵守官网的规范</p>
</div>
<h3 id="restart-strategy"> Restart Strategy</h3>
<p>重启策略的配置非常直观,在flink中有三种重启策略,对应了这里的三种配置,如下:</p>
<div><pre><code>  <span>restart-strategy</span><span>:</span>
    <span>value</span><span>:</span> fixed<span>-</span>delay  <span>#重启策略[(fixed-delay|failure-rate|none)共3个可配置的策略]</span>
    <span>fixed-delay</span><span>:</span>
      <span>attempts</span><span>:</span> <span>3</span>
      <span>delay</span><span>:</span> <span>5000</span>
    <span>failure-rate</span><span>:</span>
      <span>max-failures-per-interval</span><span>:</span>
      <span>failure-rate-interval</span><span>:</span>
      <span>delay</span><span>:</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div><p><code>value</code>下配置具体的选择哪种重启策略</p>
<div>
<ul>
<li>fixed-delay</li>
<li>failure-rate</li>
<li>none</li>
</ul>
</div>
<h4 id="fixed-delay-固定间隔"> fixed-delay(固定间隔)</h4>
<ClientOnly>
<table-data name="fixed-delay"></table-data>
</ClientOnly>
<div><p>示例</p>
<div><pre><code><span>attempts</span><span>:</span> <span>5</span>
<span>delay</span><span>:</span> 3 s
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><p>即:任务最大的失败重试次数是<code>5次</code>,每次任务重启的时间间隔是<code>3秒</code>,如果失败次数到达5次,则任务失败退出</p>
</div>
<h4 id="failure-rate-失败率"> failure-rate(失败率)</h4>
<ClientOnly>
<table-data name="failure-rate"></table-data>
</ClientOnly>
<div><p>示例</p>
<div><pre><code> <span>max-failures-per-interval</span><span>:</span> <span>10</span>
 <span>failure-rate-interval</span><span>:</span> 5 min
 <span>delay</span><span>:</span> 2 s
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>即:每次异常重启的时间间隔是<code>2秒</code>,如果在<code>5分钟内</code>,失败总次数到达<code>10次</code> 则任务失败.</p>
</div>
<h4 id="none-无重启"> None (无重启)</h4>
<p>无重启无需配置任务参数</p>
<h4 id="单位后缀"> 单位后缀</h4>
<p>时间间隔和频率设置需注意,可以不带单位后缀,如果不带单位后缀则默认会当成<code>毫秒</code>来处理,可选的单位有</p>
<ul>
<li>s    秒</li>
<li>m    分钟</li>
<li>min  分钟</li>
<li>h    小时</li>
<li>d    日</li>
</ul>
<h3 id="table"> Table</h3>
<p>在<code>table</code>下是Flink Sql相关的配置,目前支持的配置项和作用如下</p>
<ul>
<li>planner</li>
<li>mode</li>
<li>catalog</li>
<li>database</li>
</ul>
<ClientOnly>
  <table-data name="tables"></table-data>
</ClientOnly>
<h2 id="sql-文件"> Sql 文件</h2>
<p>Sql 文件必须是yaml格式的文件,得遵循yaml文件的定义规则,具体内部sql格式的定义非常简单,如下:</p>
<div><pre><code><span>sql</span>: <span>|</span>
  <span>CREATE</span> <span>TABLE</span> datagen <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING<span>,</span>
    ts <span>AS</span> localtimestamp<span>,</span>
    WATERMARK <span>FOR</span> ts <span>AS</span> ts
  <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'datagen'</span><span>,</span>
    <span>-- optional options --</span>
    <span>'rows-per-second'</span><span>=</span><span>'5'</span><span>,</span>
    <span>'fields.f_sequence.kind'</span><span>=</span><span>'sequence'</span><span>,</span>
    <span>'fields.f_sequence.start'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_sequence.end'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random.min'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_random.max'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random_str.length'</span><span>=</span><span>'10'</span>
  <span>)</span><span>;</span>

  <span>CREATE</span> <span>TABLE</span> print_table <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING
    <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'print'</span>
  <span>)</span><span>;</span>

  <span>INSERT</span> <span>INTO</span> print_table <span>select</span> f_sequence<span>,</span>f_random<span>,</span>f_random_str <span>from</span> datagen<span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div><p><code>sql</code>为当前sql的id,必须是唯一的,后面的内容则是具体的sql</p>
<div><p>特别注意</p>
<p>上面内容中 <strong>sql:</strong> 后面的 <strong>|</strong> 是必带的, 加上 <strong>|</strong> 会保留整段内容的格式,重点是保留了换行符, StreamX封装了Flink Sql的提交,可以直接将多个Sql一次性定义出来,每个Sql必须用 <strong>;</strong> 分割,每段 Sql也必须遵循Flink Sql规定的格式和规范</p>
</div>
<h2 id="总结"> 总结</h2>
<p>本章节详细介绍了<code>配置文件</code>和<code>sql文件</code>的由来和具体配置,相信你已经有了一个初步的印象和概念,具体使用请查后续章节</p>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/process_steps.png" type="image/png"/>
    </item>
    <item>
      <title>编程模型</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/model/model/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/model/model/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">编程模型</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>任何框架都有一些要遵循的规则和约定,我们只有遵循并掌握了这些规则,才能更加游刃有余的使用,使其发挥事半功倍的效果, 我们开发<code>flink</code>作业,其实就是利用<code>flink</code>提供的api,按照<code>flink</code>要求的开发方式,写一个可以执行的(必须有<code>main()</code>函数)的程序,在程序里接入各种<code>Connector</code>经过一系列的<code>算子</code>操作,最终将数据通过<code>Connector</code>sink到目标存储,
我们把这种按照某种约定的规则去逐步编程的方式称之为<code>编程模型</code>, 这一章节我们就来聊聊<code>StreamX</code>的<code>编程模型</code>以及开发注意事项</p>
<p>我们从这几个方面开始入手</p>
<div>
<ul>
<li>架构</li>
<li>编程模型</li>
<li>RunTime Context</li>
<li>生命周期</li>
<li>目录结构</li>
<li>打包部署</li>
</ul>
</div>
<h2 id="架构"> 架构</h2>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_archite.png"/><br>
</center>
<h2 id="编程模型"> 编程模型</h2>
<p><code>streamx-core</code>定位是编程时框架,快速开发脚手架,专门为简化Flink开发而生,开发者在开发阶段会使用到该模块,下面我们来看看<code>DataStrema</code>和<code>Flink sql</code>用<code>StreamX</code>来开发编程模型是什么样的,有什么规范和要求</p>
<h3 id="datastream"> DataStream</h3>
<p><code>StreamX</code>提供了<code>scala</code>和<code>java</code>两种api来开发<code>DataStream</code>程序,具体代码开发如下</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> MyFlinkApp <span>extends</span> FlinkStreaming <span>{</span>

    <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        <span>.</span><span>.</span><span>.</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>public</span> <span>class</span> <span>MyFlinkJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>environment<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
            <span>//用户可以给environment设置参数...</span>
            <span>System</span><span>.</span>out<span>.</span><span>println</span><span>(</span><span>"environment argument set..."</span><span>)</span><span>;</span>
        <span>}</span><span>)</span><span>;</span>
        
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>javaConfig<span>)</span><span>;</span>
            
        <span>.</span><span>.</span><span>.</span><span>.</span>    

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p>用<code>scala</code> api开发,程序必须要继承<code>FlinkStreaming</code>,继承之后,会强制让开发者实现<code>handle()</code>方法,该方法就是用户写代码的入口, 同时<code>streamingContext</code>供开发者使用</p>
<p>用<code>java</code> api开发由于语言本身的限制没法省掉<code>main()</code>方法,所以会是一个标准的<code>main()</code>函数, 需要用户手动创建<code>StreamingContext</code>,<code>StreamingContext</code>是非常重要的一个类,稍后会介绍</p>
<div><p>特别注意</p>
<p>以上几行scala和java代码就是用StreamX开发DataStream必不可少的最基本的骨架代码,用StreamX开发DataStream程序,从这几行代码开始,
JAVA API开发需要开发者手动启动任务<code>start</code></p>
</div>
<h3 id="flink-sql"> Flink SQL</h3>
<p>TableEnvironment 是用来创建 Table &amp; SQL 程序的上下文执行环境,也是 Table &amp; SQL 程序的入口,Table &amp; SQL 程序的所有功能都是围绕 TableEnvironment 这个核心类展开的。TableEnvironment 的主要职能包括：对接外部系统，表及元数据的注册和检索，执行SQL语句，提供更详细的配置选项。</p>
<p>Flink社区一直在推进 DataStream 的批处理能力,统一流批一体,在Flink 1.12中流批一体真正统一运行,诸多历史API如DataSet API,BatchTableEnvironment API等被废弃,退出历史舞台,官方推荐使用 <mark>TableEnvironment</mark> 和 <mark>StreamTableEnvironment</mark></p>
<p><code>StreamX</code>针对 <mark>TableEnvironment</mark> 和 <mark>StreamTableEnvironment</mark> 这两种环境的开发,提供了对应的更方便快捷的API</p>
<h4 id="tableenvironment"> TableEnvironment</h4>
<p>开发Table &amp; SQL 作业,TableEnvironment 会是 Flink 推荐使用的入口类, 同时能支持 JAVA API 和 SCALA API,下面的代码演示了在<code>StreamX</code>如何开发一个TableEnvironment类型的作业</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkTable

<span>object</span> TableApp <span>extends</span> FlinkTable <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>.</span><span>.</span><span>.</span>
  <span>}</span>
  
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>TableContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>TableEnvConfig</span><span>;</span>

<span>public</span> <span>class</span> <span>JavaTableApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>TableEnvConfig</span> tableEnvConfig <span>=</span> <span>new</span> <span>TableEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>TableContext</span> context <span>=</span> <span>new</span> <span>TableContext</span><span>(</span>tableEnvConfig<span>)</span><span>;</span>
        <span>.</span><span>.</span><span>.</span>
        context<span>.</span><span>start</span><span>(</span><span>"Flink SQl Job"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>特别注意</p>
<p>以上几行scala和java代码就是用StreamX开发TableEnvironment必不可少的最基本的骨架代码,用StreamX开发TableEnvironment程序,从这几行代码开始,
SCALA API必须继承FlinkTable,JAVA API开发需要手动构造TableContext,需要开发者手动启动任务<code>start</code></p>
</div>
<h4 id="streamtableenvironment"> StreamTableEnvironment</h4>
<p>StreamTableEnvironment用于流计算场景,流计算的对象是DataStream。相比 TableEnvironment,StreamTableEnvironment 提供了 DataStream 和 Table 之间相互转换的接口,如果用户的程序除了使用 Table API &amp; SQL 编写外,还需要使用到 DataStream API,则需要使用 StreamTableEnvironment。
下面的代码演示了在<code>StreamX</code>如何开发一个StreamTableEnvironment类型的作业</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>test<span>.</span>tablesql</span>

<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreamTable

<span>object</span> StreamTableApp <span>extends</span> FlinkStreamTable <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>.</span><span>.</span><span>.</span>
  <span>}</span>
  
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamTableContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamTableEnvConfig</span><span>;</span>

<span>public</span> <span>class</span> <span>JavaStreamTableApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamTableEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamTableEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamTableContext</span> context <span>=</span> <span>new</span> <span>StreamTableContext</span><span>(</span>javaConfig<span>)</span><span>;</span>

        <span>.</span><span>.</span><span>.</span>

        context<span>.</span><span>start</span><span>(</span><span>"Flink SQl Job"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>特别注意</p>
<p>以上几行scala和java代码就是用StreamX开发StreamTableEnvironment必不可少的最基本的骨架代码,用StreamX开发StreamTableEnvironment程序,从这几行代码开始,java代码需要手动构造StreamTableContext,<code>JAVA API</code>开发需要开发者手动启动任务<code>&quot;start&quot;</code></p>
</div>
<h2 id="runtime-context"> RunTime Context</h2>
<p><code>RunTime Context</code> —— <mark><code>StreamingContext</code></mark> , <mark><code>TableContext</code></mark> , <mark><code>StreamTableContext</code></mark> 是<code>StreamX</code>中几个非常重要三个对象,接下来我们具体看看这三个<code>Context</code>的定义和作用</p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_coreapi.png" width="60%"/>
</center>
<h3 id="streamingcontext"> StreamingContext</h3>
<p><code>StreamingContext</code>继承自<code>StreamExecutionEnvironment</code>,在<code>StreamExecutionEnvironment</code>的基础之上增加了<code>ParameterTool</code>,简单可以理解为</p>
<p><strong> <mark><code>StreamingContext</code></mark> </strong> = <strong> <mark><code>ParameterTool</code></mark> </strong> + <strong> <mark><code>StreamExecutionEnvironment</code></mark> </strong></p>
<p>具体定义如下</p>
<div><pre><code><span>class</span> StreamingContext<span>(</span><span>val</span> parameter<span>:</span> ParameterTool<span>,</span> <span>private</span> <span>val</span> environment<span>:</span> StreamExecutionEnvironment<span>)</span> 
    <span>extends</span> StreamExecutionEnvironment<span>(</span>environment<span>.</span>getJavaEnv<span>)</span> <span>{</span>

  <span>/**
   * for scala
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> <span>(</span>ParameterTool<span>,</span> StreamExecutionEnvironment<span>)</span><span>)</span> <span>=</span> <span>this</span><span>(</span>args<span>.</span>_1<span>,</span> args<span>.</span>_2<span>)</span>

  <span>/**
   * for Java
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkStreamingInitializer<span>.</span>initJavaStream<span>(</span>args<span>)</span><span>)</span>
  
  <span>.</span><span>.</span><span>.</span>  
<span>}</span>  
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br></div></div><div><p>特别注意</p>
<p>这个对象非常重要,在DataStream作业中会贯穿整个任务的生命周期,StreamingContext本身继承自StreamExecutionEnvironment,配置文件会完全融合到StreamingContext中,这样就可以非常方便的从StreamingContext中获取各种参数</p>
</div>
<p>在StreamX中,StreamingContext 也是 JAVA API编写DataStream作业的入口类,StreamingContext的构造方法中有一个是专门为JAVA API打造的,该构造函数定义如下:</p>
<div><pre><code><span>/**
 * for Java
 * @param args
*/</span>
<span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkStreamingInitializer<span>.</span>initJavaStream<span>(</span>args<span>)</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>由上面的构造方法可以看到创建<code>StreamingContext</code>,需要传入一个<code>StreamEnvConfig</code>对象,<code>StreamEnvConfig</code>定义如下:</p>
<div><pre><code><span>class</span> StreamEnvConfig<span>(</span><span>val</span> args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>,</span> <span>val</span> conf<span>:</span> StreamEnvConfigFunction<span>)</span>
</code></pre>
<div><span>1</span><br></div></div><p>StreamEnvConfig的构造方法中,其中</p>
<ul>
<li><code>args</code> 为启动参数,必须为<code>main</code>方法里的<code>args</code></li>
<li><code>conf</code> 为<code>StreamEnvConfigFunction</code>类型的<code>Function</code></li>
</ul>
<p><code>StreamEnvConfigFunction</code> 定义如下</p>
<div><pre><code><span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>StreamEnvConfigFunction</span> <span>{</span>
    <span>/**
     * 用于初始化StreamExecutionEnvironment的时候,用于可以实现该函数,自定义要设置的参数...
     *
     * @param environment
     * @param parameterTool
     */</span>
    <span>void</span> <span>configuration</span><span>(</span><span>StreamExecutionEnvironment</span> environment<span>,</span> <span>ParameterTool</span> parameterTool<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><p>该<code>Function</code>的作用是让开发者可以通过钩子的方式设置更多的参数,会将 <code>parameter</code>(解析配置文件里所有的参数)和初始化好的<code>StreamExecutionEnvironment</code>对象传给开发者去完成更多的参数设置,如:</p>
<div><pre><code><span>StreamEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>environment<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
    <span>System</span><span>.</span>out<span>.</span><span>println</span><span>(</span><span>"environment argument set..."</span><span>)</span><span>;</span>
    environment<span>.</span><span>getConfig</span><span>(</span><span>)</span><span>.</span><span>enableForceAvro</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>javaConfig<span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><h3 id="tablecontext"> TableContext</h3>
<p><code>TableContext</code>继承自<code>TableEnvironment</code>,在<code>TableEnvironment</code>的基础之上增加了<code>ParameterTool</code>,用来创建 <code>Table</code> &amp; <code>SQL</code> 程序的上下文执行环境,简单可以理解为</p>
<p><strong> <mark><code>TableContext</code></mark> </strong> = <strong> <mark><code>ParameterTool</code></mark> </strong> + <strong> <mark><code>TableEnvironment</code></mark> </strong></p>
<p>具体定义如下</p>
<div><pre><code><span>class</span> TableContext<span>(</span><span>val</span> parameter<span>:</span> ParameterTool<span>,</span>
                   <span>private</span> <span>val</span> tableEnv<span>:</span> TableEnvironment<span>)</span> 
                   <span>extends</span> TableEnvironment 
                   <span>with</span> FlinkTableTrait <span>{</span>

  <span>/**
   * for scala
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> <span>(</span>ParameterTool<span>,</span> TableEnvironment<span>)</span><span>)</span> <span>=</span> <span>this</span><span>(</span>args<span>.</span>_1<span>,</span> args<span>.</span>_2<span>)</span>

  <span>/**
   * for java
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> TableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaTable<span>(</span>args<span>)</span><span>)</span>
  
  <span>.</span><span>.</span><span>.</span>
<span>}</span>  
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br></div></div><p>在StreamX中,<code>TableContext</code> 也是 JAVA API编写TableEnvironment类型的Table Sql作业的入口类,TableContext的构造方法中有一个是专门为JAVA API打造的,该构造函数定义如下:</p>
<div><pre><code>
<span>/**
* for java
* @param args
*/</span>
<span>def</span> <span>this</span><span>(</span>args<span>:</span> TableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaTable<span>(</span>args<span>)</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><p>由上面的构造方法可以看到创建<code>TableContext</code>,需要传入一个<code>TableEnvConfig</code>对象,<code>TableEnvConfig</code>定义如下:</p>
<div><pre><code><span>class</span> TableEnvConfig<span>(</span><span>val</span> args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>,</span> <span>val</span> conf<span>:</span> TableEnvConfigFunction<span>)</span>
</code></pre>
<div><span>1</span><br></div></div><p>TableEnvConfig的构造方法中,其中</p>
<ul>
<li><code>args</code> 为启动参数,必须为<code>main</code>方法里的<code>args</code></li>
<li><code>conf</code> 为<code>TableEnvConfigFunction</code>类型的<code>Function</code></li>
</ul>
<p><code>TableEnvConfigFunction</code> 定义如下</p>
<div><pre><code><span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>TableEnvConfigFunction</span> <span>{</span>
    <span>/**
     * 用于初始化TableEnvironment的时候,用于可以实现该函数,自定义要设置的参数...
     *
     * @param tableConfig
     * @param parameterTool
     */</span>
    <span>void</span> <span>configuration</span><span>(</span><span>TableConfig</span> tableConfig<span>,</span> <span>ParameterTool</span> parameterTool<span>)</span><span>;</span>

<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></div></div><p>该<code>Function</code>的作用是让开发者可以通过钩子的方式设置更多的参数,会将 <code>parameter</code>(解析配置文件里所有的参数)和初始化好的<code>TableEnvironment</code>中的<code>TableConfig</code>对象传给开发者去完成更多的参数设置,如:</p>
<div><pre><code><span>TableEnvConfig</span> config <span>=</span> <span>new</span> <span>TableEnvConfig</span><span>(</span>args<span>,</span><span>(</span>tableConfig<span>,</span>parameterTool<span>)</span><span>-></span><span>{</span>
    tableConfig<span>.</span><span>setLocalTimeZone</span><span>(</span><span>ZoneId</span><span>.</span><span>of</span><span>(</span><span>"Asia/Shanghai"</span><span>)</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>
<span>TableContext</span> context <span>=</span> <span>new</span> <span>TableContext</span><span>(</span>config<span>)</span><span>;</span>
<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><h3 id="streamtablecontext"> StreamTableContext</h3>
<p><code>StreamTableContext</code>继承自<code>StreamTableEnvironment</code>,用于流计算场景,流计算的对象是<code>DataStream</code>,相比<code>TableEnvironment</code>,<code>StreamTableEnvironment</code> 提供了 <code>DataStream</code> 和 <code>Table</code> 之间相互转换的接口,
<code>StreamTableContext</code>在<code>StreamTableEnvironment</code>的基础之上增加了<code>ParameterTool</code>,又直接接入了<code>StreamTableEnvironment</code>的API,简单可以理解为</p>
<p><strong> <mark><code>StreamTableContext</code></mark> </strong> = <strong> <mark><code>ParameterTool</code></mark> </strong> + <strong> <mark><code>StreamTableEnvironment</code></mark> </strong> + <strong> <mark><code>StreamExecutionEnvironment</code></mark> </strong></p>
<p>具体定义如下</p>
<div><pre><code>
<span>class</span> StreamTableContext<span>(</span><span>val</span> parameter<span>:</span> ParameterTool<span>,</span>
                         <span>private</span> <span>val</span> streamEnv<span>:</span> StreamExecutionEnvironment<span>,</span>
                         <span>private</span> <span>val</span> tableEnv<span>:</span> StreamTableEnvironment<span>)</span> 
                         <span>extends</span> StreamTableEnvironment 
                         <span>with</span> FlinkTableTrait <span>{</span>

  <span>/**
   * 一旦 Table 被转化为 DataStream,
   * 必须使用 StreamExecutionEnvironment 的 execute 方法执行该 DataStream 作业。
   */</span>
  <span>private</span><span>[</span>scala<span>]</span> <span>var</span> isConvertedToDataStream<span>:</span> <span>Boolean</span> <span>=</span> <span>false</span>

  <span>/**
   * for scala
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> <span>(</span>ParameterTool<span>,</span> StreamExecutionEnvironment<span>,</span> StreamTableEnvironment<span>)</span><span>)</span> <span>=</span> 
  <span>this</span><span>(</span>args<span>.</span>_1<span>,</span> args<span>.</span>_2<span>,</span> args<span>.</span>_3<span>)</span>

  <span>/**
   * for Java
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamTableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaStreamTable<span>(</span>args<span>)</span><span>)</span>
  <span>.</span><span>.</span><span>.</span>  
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br></div></div><p>在StreamX中,<code>StreamTableContext</code> 是 JAVA API编写StreamTableEnvironment类型的Table Sql作业的入口类,StreamTableContext的构造方法中有一个是专门为JAVA API打造的,该构造函数定义如下:</p>
<div><pre><code>
  <span>/**
   * for Java
   *
   * @param args
   */</span>
<span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamTableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaStreamTable<span>(</span>args<span>)</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><p>由上面的构造方法可以看到创建<code>StreamTableContext</code>,需要传入一个<code>StreamTableEnvConfig</code>对象,<code>StreamTableEnvConfig</code>定义如下:</p>
<div><pre><code><span>class</span> StreamTableEnvConfig<span>(</span>
    <span>val</span> args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>,</span>
    <span>val</span> streamConfig<span>:</span> StreamEnvConfigFunction<span>,</span> 
    <span>val</span> tableConfig<span>:</span> TableEnvConfigFunction
<span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>StreamTableEnvConfig的构造方法中有三个参数,其中</p>
<ul>
<li><code>args</code> 为启动参数,必须为<code>main</code>方法里的<code>args</code></li>
<li><code>streamConfig</code> 为<code>StreamEnvConfigFunction</code>类型的<code>Function</code></li>
<li><code>tableConfig</code> 为<code>TableEnvConfigFunction</code>类型的<code>Function</code></li>
</ul>
<p><code>StreamEnvConfigFunction</code>和<code>TableEnvConfigFunction</code> 定义上面已经讲过,这里不再赘述</p>
<p>该<code>Function</code>的作用是让开发者可以通过钩子的方式设置更多的参数,和上面其他参数设置不同的是,该<code>Function</code>提供了同时设置<code>StreamExecutionEnvironment</code>和<code>TableEnvironment</code>的机会 ,会将 <code>parameter</code>和初始化好的<code>StreamExecutionEnvironment</code>和<code>TableEnvironment</code>中的<code>TableConfig</code>对象传给开发者去完成更多的参数设置,如:</p>
<div><pre><code>
<span>StreamTableEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamTableEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>environment<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
    environment<span>.</span><span>getConfig</span><span>(</span><span>)</span><span>.</span><span>enableForceAvro</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>,</span> <span>(</span>tableConfig<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
    tableConfig<span>.</span><span>setLocalTimeZone</span><span>(</span><span>ZoneId</span><span>.</span><span>of</span><span>(</span><span>"Asia/Shanghai"</span><span>)</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>StreamTableContext</span> context <span>=</span> <span>new</span> <span>StreamTableContext</span><span>(</span>javaConfig<span>)</span><span>;</span>

<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><div><p>特别提示</p>
<p>在<code>StreamTableContext</code>中可以直接使用<code>StreamExecutionEnvironment</code>的<code>API</code>, <mark>以$打头的方法</mark> 都是<code>StreamExecutionEnvironment</code>的API</p>
<p><img src="/streamx-docs/assets/img/doc-img/streamx_apis.jpeg" alt=""></p>
</div>
<h2 id="生命周期"> 生命周期</h2>
<p>生命周期的概念目前只针对<code>scala</code> API,该生命周期明确的定义了整个任务运行的全过程 ,但凡继承自<code>FlinkStreaming</code>或<code>FlinkTable</code>或<code>StreamingTable</code> 就会按这个生命周期执行,生命周期的核心方法如下</p>
<div><pre><code> <span>final</span> <span>def</span> main<span>(</span>args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    init<span>(</span>args<span>)</span>
    ready<span>(</span><span>)</span>
    handle<span>(</span><span>)</span>
    jobExecutionResult <span>=</span> context<span>.</span>start<span>(</span><span>)</span>
    destroy<span>(</span><span>)</span>
  <span>}</span>

  <span>private</span><span>[</span><span>this</span><span>]</span> <span>def</span> init<span>(</span>args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    SystemPropertyUtils<span>.</span>setAppHome<span>(</span>KEY_APP_HOME<span>,</span> classOf<span>[</span>FlinkStreaming<span>]</span><span>)</span>
    context <span>=</span> <span>new</span> StreamingContext<span>(</span>FlinkStreamingInitializer<span>.</span>initStream<span>(</span>args<span>,</span> config<span>)</span><span>)</span>
  <span>}</span>

  <span>/**
   * 用户可覆盖次方法...
   *
   */</span>
  <span>def</span> ready<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span><span>}</span>

  <span>def</span> config<span>(</span>env<span>:</span> StreamExecutionEnvironment<span>,</span> parameter<span>:</span> ParameterTool<span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span><span>}</span>

  <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span>

  <span>def</span> destroy<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span><span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br></div></div><p>生命周期如下</p>
<ul>
<li><strong>init</strong>          配置文件初始化阶段</li>
<li><strong>config</strong>        开发者手动设置参数阶段</li>
<li><strong>ready</strong>         启动之前执行自定义动作阶段</li>
<li><strong>handle</strong>        开发者代码接入阶段</li>
<li><strong>start</strong>         程序启动阶段</li>
<li><strong>destroy</strong>       销毁阶段</li>
</ul>
<p><img src="/streamx-docs/assets/img/doc-img/streamx_scala_life_cycle.png" alt="Life Cycle"></p>
<h3 id="生命周期之-init"> 生命周期之—— init</h3>
<p><strong>init</strong> 阶段,框架会自动解析传入的配置文件,按照里面的定义的各种参数初始化<code>StreamExecutionEnvironment</code>,这一步是框架自动执行,不需要开发者参与</p>
<h3 id="生命周期之-config"> 生命周期之—— config</h3>
<p><strong>config</strong> 阶段的目的是让开发者可以通过钩子的方式设置更多的参数(约定的配置文件以外的其他参数),在 <strong>config</strong> 阶段会将 <code>parameter</code>(<em>init</em> 阶段解析的配置文件里所有的参数)和<em>init</em> 阶段初始化好的<code>StreamExecutionEnvironment</code>对象传给开发者,
这样开发者就可以配置更多的参数</p>
<div><p>提示</p>
<p><strong>config</strong> 阶段是需要开发者参与的阶段,是可选的阶段</p>
</div>
<h3 id="生命周期之-ready"> 生命周期之—— ready</h3>
<p><strong>ready</strong> 阶段是在参数都设置完毕了,给开发者提供的一个用于做其他动作的入口, 该阶段是在<strong>初始化完成之后</strong>在<strong>程序启动之前</strong>进行</p>
<div><p>提示</p>
<p><strong>ready</strong> 阶段是需要开发者参与的阶段,是可选的阶段</p>
</div>
<h3 id="生命周期之-handle"> 生命周期之—— handle</h3>
<p><strong>handle</strong> 阶段是接入开发者编写的代码的阶段,是开发者编写代码的入口,也是最重要的一个阶段, 这个<code>handle</code> 方法会强制让开发者去实现</p>
<div><p>提示</p>
<p><strong>handle</strong> 阶段是需要开发者参与的阶段,是必须的阶段</p>
</div>
<h3 id="生命周期之-start"> 生命周期之—— start</h3>
<p><strong>start</strong> 阶段,顾名思义,这个阶段会启动任务,由框架自动执行</p>
<h3 id="生命周期之-destroy"> 生命周期之—— destroy</h3>
<p><strong>destroy</strong> 阶段,是程序运行完毕了,在jvm退出之前的最后一个阶段,一般用于收尾的工作</p>
<div><p>提示</p>
<p><strong>destroy</strong> 阶段是需要开发者参与的阶段,是可选的阶段</p>
</div>
<h2 id="目录结构"> 目录结构</h2>
<p>推荐的项目目录结构如下,具体可以参考<a href="https://github.com/streamxhub/streamx/streamx-flink/streamx-flink-quickstart" target="_blank" rel="noopener noreferrer">Streamx-flink-quickstart</a> 里的目录结构和配置</p>
<div><pre><code>.
|── assembly
│    ├── bin
│    │    ├── startup.sh                             //启动脚本  
│    │    ├── setclasspath.sh                        //java环境变量相关的脚本(框架内部使用,开发者无需关注)
│    │    ├── shutdown.sh                            //任务停止脚本(不建议使用)
│    │    └── flink.sh                               //启动时内部使用到的脚本(框架内部使用,开发者无需关注)
│    │── conf                           
│    │    ├── test
│    │    │    ├── application.yaml                  //测试(test)阶段的配置文件
│    │    │    └── sql.yaml                          //flink sql
│    │    │
│    │    ├── prod                      
│    │    │    ├── application.yaml                  //生产(prod)阶段的配置文件
│    │    │    └── sql.yaml                          //flink sql
│    │── logs                                        //logs目录
│    └── temp
│
│── src
│    └── main
│         ├── java 
│         ├── resources
│         └── scala 
│
│── assembly.xml
│
└── pom.xml
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br></div></div><p>assembly.xml 是assembly打包插件需要用到的配置文件,定义如下:</p>
<div><pre><code><span><span><span>&lt;</span>assembly</span><span>></span></span>
    <span><span><span>&lt;</span>id</span><span>></span></span>bin<span><span><span>&lt;/</span>id</span><span>></span></span>
    <span><span><span>&lt;</span>formats</span><span>></span></span>
        <span><span><span>&lt;</span>format</span><span>></span></span>tar.gz<span><span><span>&lt;/</span>format</span><span>></span></span>
    <span><span><span>&lt;/</span>formats</span><span>></span></span>
    <span><span><span>&lt;</span>fileSets</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/bin<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>bin<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>${project.build.directory}<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>lib<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
            <span><span><span>&lt;</span>includes</span><span>></span></span>
                <span><span><span>&lt;</span>include</span><span>></span></span>*.jar<span><span><span>&lt;/</span>include</span><span>></span></span>
            <span><span><span>&lt;/</span>includes</span><span>></span></span>
            <span><span><span>&lt;</span>excludes</span><span>></span></span>
                <span><span><span>&lt;</span>exclude</span><span>></span></span>original-*.jar<span><span><span>&lt;/</span>exclude</span><span>></span></span>
            <span><span><span>&lt;/</span>excludes</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/conf<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>conf<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/logs<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>logs<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/temp<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>temp<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
    <span><span><span>&lt;/</span>fileSets</span><span>></span></span>
<span><span><span>&lt;/</span>assembly</span><span>></span></span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br></div></div><h2 id="打包部署"> 打包部署</h2>
<p>推荐<a href="https://github.com/streamxhub/streamx/streamx-flink/streamx-flink-quickstart" target="_blank" rel="noopener noreferrer">Streamx-flink-quickstart</a>里的打包模式,直接运行<code>maven package</code>即可生成一个标准的StreamX推荐的项目包,解包后目录结构如下</p>
<div><pre><code>.
Streamx-flink-quickstart-1.0.0
├── bin
│   ├── startup.sh                             //启动脚本  
│   ├── setclasspath.sh                        //java环境变量相关的脚本(内部使用的,用户无需关注)
│   ├── shutdown.sh                            //任务停止脚本(不建议使用)
│   ├── flink.sh                               //启动时内部使用到的脚本(内部使用的,用户无需关注)
├── conf                           
│   ├── application.yaml                       //项目的配置文件
│   ├── sql.yaml                               // flink sql文件
├── lib
│   └── Streamx-flink-quickstart-1.0.0.jar     //项目的jar包
└── temp
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><h2 id="启动命令"> 启动命令</h2>
<p>启动之前确定application.yaml和 sql.yaml 配置文件,如果要启动的任务是<code>DataStream</code>任务,直接在startup.sh后跟上配置文件即可</p>
<div><pre><code>bin/startup.sh --conf conf/application.yaml
</code></pre>
<div><span>1</span><br></div></div><p>如果要启动的任务是<code>Flink Sql</code>任务,则需要跟上配置文件和sql.yaml</p>
<div><pre><code>bin/startup.sh --conf conf/application.yaml --sql conf/sql.yaml
</code></pre>
<div><span>1</span><br></div></div>]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/streamx_apis.jpeg" type="image/jpeg"/>
    </item>
    <item>
      <title>开发实例</title>
      <link>http://www.streamxhub.com/streamx-docs/doc/usecase/usecase/</link>
      <guid>http://www.streamxhub.com/streamx-docs/doc/usecase/usecase/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">开发实例</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>文档中心</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">文档中心</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
    </item>
    <item>
      <title>高级扩展</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/advanced/advanced/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/advanced/advanced/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">高级扩展</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Clickhouse Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/clickhouse/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/clickhouse/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Clickhouse Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Elasticsearch Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/es/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/es/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Elasticsearch Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Http Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/http/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/http/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Http Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Jdbc Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/jdbc/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/jdbc/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Jdbc Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>Flink 官方 提供了<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/jdbc.html" target="_blank" rel="noopener noreferrer">JDBC</a>的连接器,用于从 JDBC 中读取或者向其中写入数据,可提供 <mark>AT_LEAST_ONCE</mark> (至少一次)的处理语义</p>
<p><code>StreamX</code>中基于两阶段提交实现了 <mark>EXACTLY_ONCE</mark> (精确一次)语义的<code>JdbcSink</code>,并且采用<a href="https://github.com/brettwooldridge/HikariCP" target="_blank" rel="noopener noreferrer"><code>光 HikariCP</code></a>为连接池,让数据的读取和写入更简单更准确</p>
<h2 id="jdbc-信息配置"> Jdbc 信息配置</h2>
<p>在<code>StreamX</code>中<code>Jdbc Connector</code>的实现用到了<a href="https://github.com/brettwooldridge/HikariCP" target="_blank" rel="noopener noreferrer"><code>光 HikariCP</code></a>连接池,相关的配置在<code>jdbc</code>的namespace下,约定的配置如下:</p>
<div><pre><code><span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>connectionTimeout</span><span>:</span> <span>30000</span>
  <span>idleTimeout</span><span>:</span> <span>30000</span>
  <span>maxLifetime</span><span>:</span> <span>30000</span>
  <span>maximumPoolSize</span><span>:</span> <span>6</span>
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><h3 id="semantic-语义配置"> semantic 语义配置</h3>
<p><code>semantic</code>这个参数是在<code>JdbcSink</code>写时候的语义,仅对 <mark><code>JdbcSink</code></mark> 有效,<code>JdbcSource</code>会自动屏蔽该参数,有三个可选项</p>
<div>
<ul>
<li>EXACTLY_ONCE</li>
<li>AT_LEAST_ONCE</li>
<li>NONE</li>
</ul>
</div>
<h4 id="exactly-once"> EXACTLY_ONCE</h4>
<p>如果<code>JdbcSink</code>配置了 <code>EXACTLY_ONCE</code>语义,则底层采用了两阶段提交的实现方式来完成写入,此时要flink配合开启<code>Checkpointing</code>才会生效,如何开启checkpoint请参考第二章关于<a href="">checkpoint</a>配置部分</p>
<h4 id="at-least-once-none"> AT_LEAST_ONCE &amp;&amp; NONE</h4>
<p>默认不指定会采用<code>NONE</code>语义,这两种配置效果一样,都是保证 <mark>至少一次</mark> 语义</p>
<div><p>提示</p>
<p>开启<code>EXACTLY_ONCE</code>精确一次的好处是显而易见的,保证了数据的准确性,但成本也是高昂的,需要<code>checkpoint</code>的支持,底层模拟了事务的提交读,对实时性有一定的损耗,如果你的业务对数据的准确性要求不是那么高,则建议采用<code>AT_LEAST_ONCE</code>语义</p>
</div>
<h3 id="其他配置"> 其他配置</h3>
<p>除了特殊的<code>semantic</code> 配置项之外,其他的所有的配置都必须遵守 <mark><code>光 HikariCP</code></mark> 连接池的配置,具体可配置项和各个参数的作用请参考<code>光 HikariCP</code><a href="https://github.com/brettwooldridge/HikariCP#gear-configuration-knobs-baby" target="_blank" rel="noopener noreferrer">官网文档</a>.</p>
<h2 id="jdbc-读取数据"> Jdbc 读取数据</h2>
<p>在<code>StreamX</code>中<code>JdbcSource</code>用来读取数据,并且根据数据的<code>offset</code>做到数据读时可回放,我们看看具体如何用<code>JdbcSource</code>读取数据,假如需求如下</p>
<div>
<ul>
<li>从<code>t_order</code>表中读取数据,以<code>timestamp</code>字段为参照,起始值为<code>2020-12-16 12:00:00</code>往后抽取数据</li>
<li>将读取到的数据构造成<code>Order</code>对象返回</li>
</ul>
</div>
<p>jdbc配置和读取代码如下</p>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>jdbc</span><span>:</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>JdbcSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> MySQLSourceApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>

    JdbcSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span>Order<span>]</span><span>(</span>lastOne <span>=></span> <span>{</span>
      <span>//防止抽取过于密集,间隔5秒抽取一次数据                          </span>
      Thread<span>.</span>sleep<span>(</span><span>5000</span><span>)</span><span>;</span>
      <span>val</span> laseOffset <span>=</span> <span>if</span> <span>(</span>lastOne <span>==</span> <span>null</span><span>)</span> <span>"2020-12-16 12:00:00"</span> <span>else</span> lastOne<span>.</span>timestamp
      s<span>"select * from t_order where timestamp > '$laseOffset' order by timestamp asc "</span>
    <span>}</span><span>,</span>
      _<span>.</span>map<span>(</span>x <span>=></span> <span>new</span> Order<span>(</span>x<span>(</span><span>"market_id"</span><span>)</span><span>.</span>toString<span>,</span> x<span>(</span><span>"timestamp"</span><span>)</span><span>.</span>toString<span>)</span><span>)</span>
    <span>)</span><span>.</span>print<span>(</span><span>)</span>

  <span>}</span>

<span>}</span>

<span>class</span> Order<span>(</span><span>val</span> marketId<span>:</span> <span>String</span><span>,</span> <span>val</span> timestamp<span>:</span> <span>String</span><span>)</span> <span>extends</span> Serializable
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>SQLQueryFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>SQLResultFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>JdbcSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>
<span>import</span> <span>java<span>.</span>util<span>.</span></span><span>ArrayList</span><span>;</span>
<span>import</span> <span>java<span>.</span>util<span>.</span></span><span>List</span><span>;</span>

<span>public</span> <span>class</span> <span>MySQLJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>new</span> <span>JdbcSource</span><span><span>&lt;</span><span>Order</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span>
                        <span>(</span><span>SQLQueryFunction</span><span><span>&lt;</span><span>Order</span><span>></span></span><span>)</span> lastOne <span>-></span> <span>{</span>
                            <span>//防止抽取过于密集,间隔5秒抽取一次数据                          </span>
                            <span>Thread</span><span>.</span><span>sleep</span><span>(</span><span>5000</span><span>)</span><span>;</span>
                            
                            <span>Serializable</span> lastOffset <span>=</span> lastOne <span>==</span> <span>null</span> 
                            <span>?</span> <span>"2020-12-16 12:00:00"</span> 
                            <span>:</span> lastOne<span>.</span>timestamp<span>;</span>
                            
                            <span>return</span> <span>String</span><span>.</span><span>format</span><span>(</span>
                                <span>"select * from t_order "</span> <span>+</span>
                                <span>"where timestamp > '%s' "</span> <span>+</span>
                                <span>"order by timestamp asc "</span><span>,</span>
                                lastOffset
                            <span>)</span><span>;</span>
                        <span>}</span><span>,</span>
                        <span>(</span><span>SQLResultFunction</span><span><span>&lt;</span><span>Order</span><span>></span></span><span>)</span> iterable <span>-></span> <span>{</span>
                            <span>List</span><span><span>&lt;</span><span>Order</span><span>></span></span> result <span>=</span> <span>new</span> <span>ArrayList</span><span><span>&lt;</span><span>></span></span><span>(</span><span>)</span><span>;</span>
                            iterable<span>.</span><span>forEach</span><span>(</span>item <span>-></span> <span>{</span>
                                <span>Order</span> <span>Order</span> <span>=</span> <span>new</span> <span>Order</span><span>(</span><span>)</span><span>;</span>
                                <span>Order</span><span>.</span>marketId <span>=</span> item<span>.</span><span>get</span><span>(</span><span>"market_id"</span><span>)</span><span>.</span><span>toString</span><span>(</span><span>)</span><span>;</span>
                                <span>Order</span><span>.</span>timestamp <span>=</span> <span>Long</span><span>.</span><span>parseLong</span><span>(</span>item<span>.</span><span>get</span><span>(</span><span>"timestamp"</span><span>)</span><span>.</span><span>toString</span><span>(</span><span>)</span><span>)</span><span>;</span>
                                result<span>.</span><span>add</span><span>(</span><span>Order</span><span>)</span><span>;</span>
                            <span>}</span><span>)</span><span>;</span>
                            <span>return</span> result<span>;</span>
                        <span>}</span><span>)</span>
                <span>.</span><span>returns</span><span>(</span><span>TypeInformation</span><span>.</span><span>of</span><span>(</span><span>Order</span><span>.</span><span>class</span><span>)</span><span>)</span>
                <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p>以<code>java</code> api为例,这里要传入两个参数</p>
<div>
<ul>
<li><code>SQLQueryFunction&lt;T&gt; queryFunc</code></li>
<li><code>SQLResultFunction&lt;T&gt; resultFunc</code></li>
</ul>
</div>
<h3 id="queryfunc获取一条sql"> queryFunc获取一条sql</h3>
<p><code>queryFunc</code>是要传入一个<code>SQLQueryFunction</code>类型的<code>function</code>,该<code>function</code>用于获取查询sql的,会将最后一条记录返回给开发者,然后需要开发者根据最后一条记录返回一条新的查询<code>sql</code>,<code>queryFunc</code>定义如下:</p>
<div><pre><code><span>/**
 * @author benjobs
 */</span>
<span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>SQLQueryFunction</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>extends</span> <span>Serializable</span> <span>{</span>
    <span>/**
     * 获取要查询的SQL
     *
     * @return
     * @throws Exception
     */</span>
    <span>String</span> <span>query</span><span>(</span><span>T</span> last<span>)</span> <span>throws</span> <span>Exception</span><span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><p>所以上面的代码中,第一次上来<code>lastOne</code>(最后一条记录)为null,会判断一下,为null则取需求里默认的<code>offset</code>,查询的sql里根据<code>timestamp</code>字段正序排,这样在第一次查询之后,会返回最后的那条记录,下次直接可以使用这条记录作为下一次查询的根据</p>
<div><p>注意事项</p>
<p><code>JdbcSource</code>实现了<code>CheckpointedFunction</code>,即当程序开启 <mark><code>checkpoint</code></mark> 后,会将这些诸如<code>laseOffset</code>的状态数据保存到<code>state backend</code>,这样程序挂了,再次启动会自动从<code>checkpoint</code>中恢复<code>offset</code>,会接着上次的位置继续读取数据,
一般在生产环境,更灵活的方式是将<code>lastOffset</code>写入如<code>redis</code>等存储中,每次查询完之后再将最后的记录更新到<code>redis</code>,这样即便程序意外挂了,再次启动,也可以从<code>redis</code>中获取到最后的<code>offset</code>进行数据的抽取,也可以很方便的人为的任意调整这个<code>offset</code>进行数据的回放</p>
</div>
<h3 id="resultfunc-处理查询到的数据"> resultFunc 处理查询到的数据</h3>
<p><code>resultFunc</code>的参数类型是<code>SQLResultFunction&lt;T&gt;</code>,是将一个查询到的结果集放到<code>Iterable&lt;Map&lt;String, ?&gt;&gt;</code>中返回给开发者,可以看到返回了一个迭代器<code>Iterable</code>,迭代器每次迭代返回一个<code>Map</code>,该<code>Map</code>里记录了一行完整的记录,<code>Map</code>的<code>key</code>为查询字段,<code>value</code>为值,<code>SQLResultFunction&lt;T&gt;</code>定义如下</p>
<div><pre><code><span>/**
 * @author benjobs
 */</span>
<span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>SQLResultFunction</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>extends</span> <span>Serializable</span> <span>{</span>
    <span>/**
     * 将查下结果以Iterable&lt;Map>的方式返回,开发者去实现转成对象.
     *
     * @param map
     * @return
     */</span>
    <span>Iterable</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>result</span><span>(</span><span>Iterable</span><span><span>&lt;</span><span>Map</span><span>&lt;</span><span>String</span><span>,</span> <span>?</span><span>></span><span>></span></span> iterable<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><h2 id="jdbc-读取写入"> Jdbc 读取写入</h2>
<p><code>StreamX</code>中<code>JdbcSink</code>是用来写入数据,我们看看具体如何用<code>JdbcSink</code>写入数据,假如需求是需要从<code>kakfa</code>中读取数据,写入到<code>mysql</code></p>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>pattern</span><span>:</span> user
  <span>group.id</span><span>:</span> user_02
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>...</span>
  
<span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><div><p>注意事项</p>
<p>配置里<code>jdbc</code>下的 <mark><code>semantic</code></mark> 是写入的语义,在上面有介绍,该配置只会在<code>JdbcSink</code>下生效,<code>StreamX</code>中基于两阶段提交实现了 <mark>EXACTLY_ONCE</mark> 语义,
这本身需要被操作的数据库(<code>mysql</code>,<code>oracle</code>,<code>MariaDB</code>,<code>MS SQL Server</code>)等支持事务,理论上所有支持标准Jdbc事务的数据库都可以做到EXACTLY_ONCE(精确一次)的写入</p>
</div>
</CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span>TypeInformation
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span>TypeExtractor<span>.</span>getForClass
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span>KafkaDeserializationSchema

<span>object</span> JdbcSinkApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span>
          <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
          <span>.</span>map<span>(</span>x <span>=></span> JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>x<span>.</span>value<span>)</span><span>)</span>
          
        JdbcSink<span>(</span><span>)</span><span>.</span>sink<span>[</span>User<span>]</span><span>(</span>source<span>)</span><span>(</span>user <span>=></span>
          s<span>"""
          |insert into t_user(`name`,`age`,`gender`,`address`)
          |value('${user.name}',${user.age},${user.gender},'${user.address}')
          |"""</span><span>.</span>stripMargin
        <span>)</span>  
  <span>}</span>

<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span><span>String</span><span>,</span>age<span>:</span><span>Int</span><span>,</span>gender<span>:</span><span>Int</span><span>,</span>address<span>:</span><span>String</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span><span>KafkaDeserializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span><span>ConsumerRecord</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>import</span> <span>static</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span><span>TypeExtractor</span><span>.</span>getForClass<span>;</span>

<span>public</span> <span>class</span> <span>JdbcSinkJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>

        <span>DataStream</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span>
                    mapper<span>.</span><span>readValue</span><span>(</span>value<span>.</span><span>value</span><span>(</span><span>)</span><span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>)</span><span>;</span>

        <span>new</span> <span>JdbcSink</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>sql</span><span>(</span><span>(</span><span>SQLFromFunction</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> <span>JavaUser</span><span>::</span><span>toSql</span><span>)</span>
                <span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>
                
        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
    <span>public</span> <span>String</span> <span>toSql</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>format</span><span>(</span>
                <span>"insert into t_user(`name`,`age`,`gender`,`address`) value('%s',%d,%d,'%s')"</span><span>,</span>
                name<span>,</span>
                age<span>,</span>
                gender<span>,</span>
                address<span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="根据数据流生成目标sql"> 根据数据流生成目标SQL</h3>
<p>在写入的时候,需要知道具体写入的<code>sql</code>语句,该<code>sql</code>语句需要开发者通过<code>function</code>的方式提供,在<code>scala</code> api中,直接在<code>sink</code>方法后跟上<code>function</code>即可,<code>java</code> api 则是通过<code>sql()</code>方法传入一个<code>SQLFromFunction</code>类型的<code>function</code></p>
<p>下面以<code>java</code> api为例说明,我们来看看<code>java</code>api 中提供sql的<code>function</code>方法的定义</p>
<div><pre><code><span>/**
 * @author benjobs
 */</span>
<span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>SQLFromFunction</span><span><span>&lt;</span><span>T</span><span>></span></span> <span>extends</span> <span>Serializable</span> <span>{</span>
    <span>/**
     * @param bean
     * @return
     */</span>
    <span>String</span> <span>from</span><span>(</span><span>T</span> bean<span>)</span><span>;</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><p><code>SQLFromFunction</code>上的泛型<code>&lt;T&gt;</code>即为<code>DataStream</code>里实际的数据类型,该<code>function</code>里有一个方法<code>form(T bean)</code>,这个<code>bean</code>即为当前<code>DataStream</code>中的一条具体数据,会将该数据返给开发者,开发者来决定基于这条数据,生成一条具体可以往数据库中插入的<code>sql</code></p>
<h3 id="设置写入批次大小"> 设置写入批次大小</h3>
<p>在 非 <code>EXACTLY_ONCE</code>(精确一次的语义下)可以适当的设置<code>batch.size</code>来提高Jdbc写入的性能(前提是业务允许的情况下),具体配置如下</p>
<div><pre><code><span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
  <span>driverClassName</span><span>:</span> com.mysql.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
  <span>batch.size</span><span>:</span> <span>1000</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><p>这样一来就不是来一条数据就立即写入,而是积攒一个匹配然后执行批量插入</p>
<div><p>注意事项</p>
<p>这个设置仅在非<code>EXACTLY_ONCE</code>语义下生效,带来的好处是可以提高Jdbc写入的性能,一次大批量的插入数据,缺点是数据写入势必会有延迟,请根据实际使用情况谨慎使用</p>
</div>
<h2 id="多实例jdbc支持"> 多实例Jdbc支持</h2>
<h2 id="手动指定jdbc连接信息"> 手动指定Jdbc连接信息</h2>
]]></content:encoded>
    </item>
    <item>
      <title>Apache Kafka Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/kafka/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/kafka/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Apache Kafka Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html" target="_blank" rel="noopener noreferrer">Flink 官方</a>提供了<a href="http://kafka.apache.org" target="_blank" rel="noopener noreferrer">Apache Kafka</a>的连接器,用于从 Kafka topic 中读取或者向其中写入数据,可提供 <mark>精确一次</mark> 的处理语义</p>
<p><code>StreamX</code>中<code>KafkaSource</code>和<code>KafkaSink</code>基于官网的<code>kafka connector</code>进一步封装,屏蔽很多细节,简化开发步骤,让数据的读取和写入更简单</p>
<h2 id="依赖"> 依赖</h2>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html" target="_blank" rel="noopener noreferrer">Apache Flink</a> 集成了通用的 Kafka 连接器，它会尽力与 Kafka client 的最新版本保持同步。该连接器使用的 Kafka client 版本可能会在 Flink 版本之间发生变化。 当前 Kafka client 向后兼容 0.10.0 或更高版本的 Kafka broker。 有关 Kafka 兼容性的更多细节，请参考 <a href="https://kafka.apache.org/protocol.html#protocol_compatibility" target="_blank" rel="noopener noreferrer">Kafka</a> 官方文档。</p>
<div><pre><code>    <span>&lt;!--必须要导入的依赖--></span>
    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>com.streamxhub.streamx<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>Streamx-flink-core<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${project.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

    <span>&lt;!--flink-connector--></span>
    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-connector-kafka_2.11<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></div></div><p>同时在开发阶段,以下的依赖也是必要的</p>
<div><pre><code>    <span>&lt;!--以下scope为provided的依赖也是必须要导入的--></span>
    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-scala_${scala.binary.version}<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${flink.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
        <span><span><span>&lt;</span>scope</span><span>></span></span>provided<span><span><span>&lt;/</span>scope</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-clients_${scala.binary.version}<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${flink.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
        <span><span><span>&lt;</span>scope</span><span>></span></span>provided<span><span><span>&lt;/</span>scope</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

    <span><span><span>&lt;</span>dependency</span><span>></span></span>
        <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
        <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-streaming-scala_${scala.binary.version}<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
        <span><span><span>&lt;</span>version</span><span>></span></span>${flink.version}<span><span><span>&lt;/</span>version</span><span>></span></span>
        <span><span><span>&lt;</span>scope</span><span>></span></span>provided<span><span><span>&lt;/</span>scope</span><span>></span></span>
    <span><span><span>&lt;/</span>dependency</span><span>></span></span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div><h2 id="kafka-source-consumer"> Kafka Source (Consumer)</h2>
<p>先介绍基于官网的标准的kafka consumer的方式,以下代码摘自<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/dev/connectors/kafka.html#kafka-consumer" target="_blank" rel="noopener noreferrer">官网文档</a></p>
<div><pre><code><span>val</span> properties <span>=</span> <span>new</span> Properties<span>(</span><span>)</span>
properties<span>.</span>setProperty<span>(</span><span>"bootstrap.servers"</span><span>,</span> <span>"localhost:9092"</span><span>)</span>
properties<span>.</span>setProperty<span>(</span><span>"group.id"</span><span>,</span> <span>"test"</span><span>)</span>
<span>val</span> stream <span>=</span> env<span>.</span>addSource<span>(</span><span>new</span> FlinkKafkaConsumer<span>[</span><span>String</span><span>]</span><span>(</span><span>"topic"</span><span>,</span> <span>new</span> SimpleStringSchema<span>(</span><span>)</span><span>,</span> properties<span>)</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>可以看到一上来定义了一堆kafka的连接信息,这种方式各项参数都是硬编码的方式写死的,非常的不灵敏,下面我们来看看如何用<code>StreamX</code>接入 <code>kafka</code>的数据,只需要按照规定的格式定义好配置文件然后编写代码即可,配置和代码如下</p>
<h3 id="基础消费示例"> 基础消费示例</h3>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> test_user
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest
  <span>enable.auto.commit</span><span>:</span> <span>true</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><div><p>注意事项</p>
<p><code>kafka.source</code>这个前缀是固定的,kafka properties相关的参数必须遵守<a href="http://kafka.apache.org" target="_blank" rel="noopener noreferrer">kafka官网</a>对参数key的设置规范</p>
</div>
</CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>quickstart</span>

<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> kafkaSourceApp <span>extends</span> FlinkStreaming <span>{</span>

    <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
        print<span>(</span>source<span>)</span>
    <span>}</span>

<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>api<span>.</span>datastream<span>.</span></span><span>DataStream</span><span>;</span>

<span>public</span> <span>class</span> <span>KafkaSimpleJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>String</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span><span>;</span>

        source<span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="高级配置参数"> 高级配置参数</h3>
<p><code>KafkaSource</code>是基于Flink Kafka Connector封装一个更简单的kafka读取类,构造方法里需要传入<code>StreamingContext</code>,当程序启动时传入配置文件即可,框架会自动解析配置文件,在<code>new KafkaSource</code>的时候会自动的从配置文件中获取相关信息,初始化并返回一个Kafka Consumer,在这里topic下只配置了一个topic,因此在消费的时候不用指定topic直接默认获取这个topic来消费, 这只是一个最简单的例子,更多更复杂的规则和读取操作则要通过<code>.getDataStream()</code>在该方法里传入参数才能实现
我们看看<code>getDataStream</code>这个方法的签名</p>
<div><pre><code><span>def</span> getDataStream<span>[</span>T<span>:</span> TypeInformation<span>]</span><span>(</span>topic<span>:</span> java<span>.</span>io<span>.</span>Serializable <span>=</span> <span>null</span><span>,</span>
    alias<span>:</span> <span>String</span> <span>=</span> <span>""</span><span>,</span>
    deserializer<span>:</span> KafkaDeserializationSchema<span>[</span>T<span>]</span><span>,</span>
    strategy<span>:</span> WatermarkStrategy<span>[</span>KafkaRecord<span>[</span>T<span>]</span><span>]</span> <span>=</span> <span>null</span>
<span>)</span><span>:</span> DataStream<span>[</span>KafkaRecord<span>[</span>T<span>]</span><span>]</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>参数具体作用如下</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">参数类型</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>topic</code></td>
<td style="text-align:left">Serializable</td>
<td style="text-align:left">一个topic或者一组topic</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left"><code>alias</code></td>
<td style="text-align:left">String</td>
<td style="text-align:left">用于区别不同的kafka实例</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:left"><code>deserializer</code></td>
<td style="text-align:left">DeserializationSchema</td>
<td style="text-align:left">topic里数据的具体解析类</td>
<td style="text-align:left">KafkaStringDeserializationSchema</td>
</tr>
<tr>
<td style="text-align:left"><code>strategy</code></td>
<td style="text-align:left">WatermarkStrategy</td>
<td style="text-align:left">watermark生成策略</td>
<td style="text-align:left">无</td>
</tr>
</tbody>
</table>
<p>下面我们来看看更多的使用和配置方式</p>
<div>
<ul>
<li>消费多个Kafka实例</li>
<li>消费多个Topic</li>
<li>Topic动态发现</li>
<li>从指定Offset消费</li>
<li>指定KafkaDeserializationSchema</li>
<li>指定WatermarkStrategy</li>
</ul>
</div>
<h3 id="消费多个kafka实例"> 消费多个Kafka实例</h3>
<p>在框架开发之初就考虑到了多个不同实例的kafka的配置情况.如何来统一配置,并且规范格式呢?在streamx中是这么解决的,假如我们要同时消费两个不同实例的kafka,配置文件定义如下,
可以看到在<code>kafka.source</code>下直接放kafka的实例名称(名字可以任意),在这里我们统一称为 <mark><code>alias</code></mark> , <mark><code>alias</code></mark> 必须是唯一的,来区别不同的实例,然后别的参数还是按照之前的规范,
统统放到当前这个实例的namespace下即可.如果只有一个kafka实例,则可以不用配置<code>alias</code>
在写代码消费时注意指定对应的 <mark><code>alias</code></mark> 即可,配置和代码如下</p>
<CodeGroup>
<CodeGroupItem title="配置" active>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>kafka1</span><span>:</span>
    <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
    <span>topic</span><span>:</span> test_user
    <span>group.id</span><span>:</span> user_01
    <span>auto.offset.reset</span><span>:</span> earliest
    <span>enable.auto.commit</span><span>:</span> <span>true</span>
  <span>kafka2</span><span>:</span>
    <span>bootstrap.servers</span><span>:</span> kfk4<span>:</span><span>9092</span><span>,</span>kfk5<span>:</span><span>9092</span><span>,</span>kfk6<span>:</span><span>9092</span>
    <span>topic</span><span>:</span> kafka2
    <span>group.id</span><span>:</span> kafka2
    <span>auto.offset.reset</span><span>:</span> earliest
    <span>enable.auto.commit</span><span>:</span> <span>true</span>    
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>//消费kafka1实例的数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>alias <span>=</span> <span>"kafka1"</span><span>)</span>
  <span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>print<span>(</span><span>)</span>
  
<span>//消费kafka2实例的数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>alias <span>=</span> <span>"kafka2"</span><span>)</span>
  <span>.</span>uid<span>(</span><span>"kfkSource2"</span><span>)</span>
  <span>.</span>name<span>(</span><span>"kfkSource2"</span><span>)</span>
  <span>.</span>print<span>(</span><span>)</span>  
  
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
<span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>

<span>//消费kafka1实例的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source1 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>alias</span><span>(</span><span>"kafka1"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>print</span><span>(</span><span>)</span><span>;</span>  

<span>//消费kafka1实例的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source2 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>alias</span><span>(</span><span>"kafka2"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span> 
            
context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>            
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></div></div><div><p>特别注意</p>
<p>java api在编写代码时,一定要将<code>alias</code>等这些参数的设置放到调用<code>.getDataStream()</code>之前</p>
</div>
</CodeGroupItem>
</CodeGroup>
<h3 id="消费多个topic"> 消费多个Topic</h3>
<p>配置消费多个topic也很简单,在配置文件<code>topic</code>下配置多个topic名称即可,用<code>,</code>或空格分隔,代码消费处理的时候指定topic参数即可,<code>scala</code> api下如果是消费一个topic,则直接传入topic名称即可,如果要消费多个,传入一个<code>List</code>即可
<code>java</code>api通过 <code>topic()</code>方法传入要消费topic的名称,是一个String类型的可变参数,可以传入一个或多个<code>topic</code>名称,配置和代码如下</p>
<CodeGroup>
<CodeGroupItem title="配置">
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> topic1<span>,</span>topic2<span>,</span>topic3<span>...</span>
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>...</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>//消费指定单个topic的数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>topic <span>=</span> <span>"topic1"</span><span>)</span>
  <span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
  <span>.</span>print<span>(</span><span>)</span>

<span>//消费一批topic数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>topic <span>=</span> List<span>(</span><span>"topic1"</span><span>,</span><span>"topic2"</span><span>,</span><span>"topic3"</span><span>)</span><span>)</span>
<span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>print<span>(</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>//消费指定单个topic的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source1 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>topic</span><span>(</span><span>"topic1"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>
        
<span>//消费一组topic的数据</span>
<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source1 <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>topic</span><span>(</span><span>"topic1"</span><span>,</span><span>"topic2"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>     
        
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>提示</p>
<p><code>topic</code>支持配置多个<code>topic</code>实例,每个<code>topic</code>直接用<code>,</code>分隔或者空格分隔,如果topic下配置多个实例,在消费的时必须指定具体的topic名称</p>
</div>
<h3 id="topic-发现"> Topic 发现</h3>
<p>关于kafka的分区动态,默认情况下，是禁用了分区发现的。若要启用它，请在提供的属性配置中为 <code>flink.partition-discovery.interval-millis</code> 设置大于 <code>0</code>,表示发现分区的间隔是以毫秒为单位的
更多详情请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#partition-discovery" target="_blank" rel="noopener noreferrer">官网文档</a></p>
<p>Flink Kafka Consumer 还能够使用正则表达式基于 Topic 名称的模式匹配来发现 Topic,详情请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#topic-discovery" target="_blank" rel="noopener noreferrer">官网文档</a>
在<code>StreamX</code>中提供更简单的方式,具体需要在 <code>pattern</code>下配置要匹配的<code>topic</code>实例名称的正则即可</p>
<CodeGroup>
<CodeGroupItem title="配置">
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>pattern</span><span>:</span> ^topic<span>[</span>1<span>-</span><span>9</span><span>]</span>
  <span>group.id</span><span>:</span> user_02
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>...</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="scala">
<div><pre><code><span>//消费正则topic数据</span>
KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>topic <span>=</span> <span>"topic-a"</span><span>)</span>
<span>.</span>uid<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>name<span>(</span><span>"kfkSource1"</span><span>)</span>
<span>.</span>print<span>(</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
<span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>

<span>//消费通配符topic数据</span>
<span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
        <span>.</span><span>topic</span><span>(</span><span>"topic-a"</span><span>)</span>
        <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
        <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>              
    
context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>         
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>特别注意</p>
<p><code>topic</code>和<code>pattern</code>不能同时配置,当配置了<code>pattern</code>正则匹配时,在消费的时候依然可以指定一个确定的<code>topic</code>名称,此时会检查<code>pattern</code>是否匹配当前的<code>topic</code>,如不匹配则会报错</p>
</div>
<h3 id="配置开始消费的位置"> 配置开始消费的位置</h3>
<p>Flink Kafka Consumer 允许通过配置来确定 Kafka 分区的起始位置,<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#kafka-consumers-start-position-configuration" target="_blank" rel="noopener noreferrer">官网文档</a>Kafka 分区的起始位置具体操作方式如下</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>val</span> env <span>=</span> StreamExecutionEnvironment<span>.</span>getExecutionEnvironment<span>(</span><span>)</span>
<span>val</span> myConsumer <span>=</span> <span>new</span> FlinkKafkaConsumer<span>[</span><span>String</span><span>]</span><span>(</span><span>.</span><span>.</span><span>.</span><span>)</span>
myConsumer<span>.</span>setStartFromEarliest<span>(</span><span>)</span>      <span>// 尽可能从最早的记录开始</span>
myConsumer<span>.</span>setStartFromLatest<span>(</span><span>)</span>        <span>// 从最新的记录开始</span>
myConsumer<span>.</span>setStartFromTimestamp<span>(</span><span>.</span><span>.</span><span>.</span><span>)</span>  <span>// 从指定的时间开始（毫秒）</span>
myConsumer<span>.</span>setStartFromGroupOffsets<span>(</span><span>)</span>  <span>// 默认的方法</span>

<span>val</span> stream <span>=</span> env<span>.</span>addSource<span>(</span>myConsumer<span>)</span>
<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>final</span> <span>StreamExecutionEnvironment</span> env <span>=</span> <span>StreamExecutionEnvironment</span><span>.</span><span>getExecutionEnvironment</span><span>(</span><span>)</span><span>;</span>

<span>FlinkKafkaConsumer</span><span><span>&lt;</span><span>String</span><span>></span></span> myConsumer <span>=</span> <span>new</span> <span>FlinkKafkaConsumer</span><span><span>&lt;</span><span>></span></span><span>(</span><span>.</span><span>.</span><span>.</span><span>)</span><span>;</span>
myConsumer<span>.</span><span>setStartFromEarliest</span><span>(</span><span>)</span><span>;</span>     <span>// 尽可能从最早的记录开始</span>
myConsumer<span>.</span><span>setStartFromLatest</span><span>(</span><span>)</span><span>;</span>       <span>// 从最新的记录开始</span>
myConsumer<span>.</span><span>setStartFromTimestamp</span><span>(</span><span>.</span><span>.</span><span>.</span><span>)</span><span>;</span> <span>// 从指定的时间开始（毫秒）</span>
myConsumer<span>.</span><span>setStartFromGroupOffsets</span><span>(</span><span>)</span><span>;</span> <span>// 默认的方法</span>

<span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> stream <span>=</span> env<span>.</span><span>addSource</span><span>(</span>myConsumer<span>)</span><span>;</span>
<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p>在<code>StreamX</code>中不推荐这种方式进行设定,提供了更方便的方式,只需要在配置里指定 <mark><code>auto.offset.reset</code></mark> 即可</p>
<ul>
<li><code>earliest</code> 从最早的记录开始</li>
<li><code>latest</code> 从最新的记录开始</li>
</ul>
<h3 id="指定分区offset"> 指定分区Offset</h3>
<p>你也可以为每个分区指定 consumer 应该开始消费的具体 offset,只需要按照如下的配置文件配置<code>start.from</code>相关的信息即可</p>
<div><pre><code><span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> topic1<span>,</span>topic2<span>,</span>topic3<span>...</span>
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest <span># (earliest | latest)</span>
  <span>start.from</span><span>:</span>
    <span>timestamp</span><span>:</span> <span>1591286400000</span> <span>#指定timestamp,针对所有的topic生效</span>
    <span>offset</span><span>:</span> <span># 给topic的partition指定offset</span>
      <span>topic</span><span>:</span> topic_abc<span>,</span>topic_123
      <span>topic_abc</span><span>:</span> 0<span>:</span><span>182</span><span>,</span>1<span>:</span><span>183</span><span>,</span>2<span>:</span><span>182</span> <span>#分区0从182开始消费,分区1从183开始,分区2从182开始...</span>
      <span>topic_123</span><span>:</span> 0<span>:</span><span>182</span><span>,</span>1<span>:</span><span>183</span><span>,</span>2<span>:</span><span>182</span>
  <span>...</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><h3 id="指定deserializer"> 指定deserializer</h3>
<p>默认不指定<code>deserializer</code>则在内部采用String的方式反序列化topic中的数据,可以手动指定<code>deserializer</code>,这样可以一步直接返回目标<code>DataStream</code>,具体完整代码如下</p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span>TypeInformation
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span>TypeExtractor<span>.</span>getForClass
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span>KafkaDeserializationSchema

<span>object</span> KafkaSourceApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        KafkaSource<span>(</span><span>)</span>
          <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span>deserializer <span>=</span> <span>new</span> UserSchema<span>)</span>
          <span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
          <span>.</span>print<span>(</span><span>)</span>
  <span>}</span>

<span>}</span>

<span>class</span> UserSchema <span>extends</span> KafkaDeserializationSchema<span>[</span>User<span>]</span> <span>{</span>
  <span>override</span> <span>def</span> isEndOfStream<span>(</span>nextElement<span>:</span> User<span>)</span><span>:</span> <span>Boolean</span> <span>=</span> <span>false</span>
  <span>override</span> <span>def</span> getProducedType<span>:</span> TypeInformation<span>[</span>User<span>]</span> <span>=</span> getForClass<span>(</span>classOf<span>[</span>User<span>]</span><span>)</span>
  <span>override</span> <span>def</span> deserialize<span>(</span>record<span>:</span> ConsumerRecord<span>[</span>Array<span>[</span><span>Byte</span><span>]</span><span>,</span> Array<span>[</span><span>Byte</span><span>]</span><span>]</span><span>)</span><span>:</span> User <span>=</span> <span>{</span>
    <span>val</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span>record<span>.</span>value<span>(</span><span>)</span><span>)</span>
    JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>value<span>)</span>
  <span>}</span>
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span><span>String</span><span>,</span>age<span>:</span><span>Int</span><span>,</span>gender<span>:</span><span>Int</span><span>,</span>address<span>:</span><span>String</span><span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span><span>KafkaDeserializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span><span>ConsumerRecord</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>import</span> <span>static</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span><span>TypeExtractor</span><span>.</span>getForClass<span>;</span>

<span>public</span> <span>class</span> <span>KafkaSourceJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>deserializer</span><span>(</span><span>new</span> <span>JavaUserSchema</span><span>(</span><span>)</span><span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span>
                <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUserSchema</span> <span>implements</span> <span>KafkaDeserializationSchema</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>{</span>
    <span>private</span> <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>boolean</span> <span>isEndOfStream</span><span>(</span><span>JavaUser</span> nextElement<span>)</span> <span>return</span> <span>false</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>TypeInformation</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>getProducedType</span><span>(</span><span>)</span> <span>return</span> <span>getForClass</span><span>(</span><span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>JavaUser</span> <span>deserialize</span><span>(</span><span>ConsumerRecord</span><span>&lt;</span><span>byte</span><span>[</span><span>]</span><span>,</span> <span>byte</span><span>[</span><span>]</span><span>></span> <span>record</span><span>)</span> <span>throws</span> <span>Exception</span> <span>{</span>
        <span>String</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span><span>record</span><span>.</span><span>value</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>return</span> mapper<span>.</span><span>readValue</span><span>(</span>value<span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="返回记录kafkarecord"> 返回记录KafkaRecord</h3>
<p>返回的对象被包装在<code>KafkaRecord</code>中,<code>kafkaRecord</code>中有当前的<code>offset</code>,<code>partition</code>,<code>timestamp</code>等诸多有用的信息供开发者使用,其中<code>value</code>即返回的目标对象,如下图:</p>
<p><img src="/streamx-docs/assets/img/doc-img/streamx_kafkaapi.jpeg" alt=""></p>
<h3 id="指定strategy"> 指定strategy</h3>
<p>在许多场景中,记录的时间戳是(显式或隐式)嵌入到记录本身中。此外,用户可能希望定期或以不规则的方式<code>Watermark</code>,例如基于<code>Kafka</code>流中包含当前事件时间的<code>watermark</code>的特殊记录。对于这些情况，<code>Flink Kafka Consumer</code>是允许指定<code>AssignerWithPeriodicWatermarks</code>或<code>AssignerWithPunctuatedWatermarks</code></p>
<p>在<code>StreamX</code>中运行传入一个<code>WatermarkStrategy</code>作为参数来分配<code>Watermark</code>,如下面的示例,解析<code>topic</code>中的数据为<code>user</code>对象,<code>user</code>中有个 <mark><code>orderTime</code></mark> 是时间类型,我们以这个为基准,为其分配一个<code>Watermark</code></p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>{</span>KafkaRecord<span>,</span> KafkaSource<span>}</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>eventtime<span>.</span></span><span>{</span>SerializableTimestampAssigner<span>,</span> WatermarkStrategy<span>}</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span>TypeInformation
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span>TypeExtractor<span>.</span>getForClass
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span>KafkaDeserializationSchema
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span>ConsumerRecord

<span>import</span> <span>java<span>.</span>time<span>.</span></span>Duration
<span>import</span> <span>java<span>.</span>util<span>.</span></span>Date

<span>object</span> KafkaSourceStrategyApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    KafkaSource<span>(</span><span>)</span>
      <span>.</span>getDataStream<span>[</span>User<span>]</span><span>(</span>
        deserializer <span>=</span> <span>new</span> UserSchema<span>,</span>
        strategy <span>=</span> WatermarkStrategy
          <span>.</span>forBoundedOutOfOrderness<span>[</span>KafkaRecord<span>[</span>User<span>]</span><span>]</span><span>(</span>Duration<span>.</span>ofMinutes<span>(</span><span>1</span><span>)</span><span>)</span>
          <span>.</span>withTimestampAssigner<span>(</span><span>new</span> SerializableTimestampAssigner<span>[</span>KafkaRecord<span>[</span>User<span>]</span><span>]</span> <span>{</span>
            <span>override</span> <span>def</span> extractTimestamp<span>(</span>element<span>:</span> KafkaRecord<span>[</span>User<span>]</span><span>,</span> recordTimestamp<span>:</span> <span>Long</span><span>)</span><span>:</span> <span>Long</span> <span>=</span> <span>{</span>
              element<span>.</span>value<span>.</span>orderTime<span>.</span>getTime
            <span>}</span>
          <span>}</span><span>)</span>
      <span>)</span><span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
      <span>.</span>print<span>(</span><span>)</span>
  <span>}</span>

<span>}</span>

<span>class</span> UserSchema <span>extends</span> KafkaDeserializationSchema<span>[</span>User<span>]</span> <span>{</span>
  <span>override</span> <span>def</span> isEndOfStream<span>(</span>nextElement<span>:</span> User<span>)</span><span>:</span> <span>Boolean</span> <span>=</span> <span>false</span>
  <span>override</span> <span>def</span> getProducedType<span>:</span> TypeInformation<span>[</span>User<span>]</span> <span>=</span> getForClass<span>(</span>classOf<span>[</span>User<span>]</span><span>)</span>
  <span>override</span> <span>def</span> deserialize<span>(</span>record<span>:</span> ConsumerRecord<span>[</span>Array<span>[</span><span>Byte</span><span>]</span><span>,</span> Array<span>[</span><span>Byte</span><span>]</span><span>]</span><span>)</span><span>:</span> User <span>=</span> <span>{</span>
    <span>val</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span>record<span>.</span>value<span>(</span><span>)</span><span>)</span>
    JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>value<span>)</span>
  <span>}</span>
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span> <span>String</span><span>,</span> age<span>:</span> <span>Int</span><span>,</span> gender<span>:</span> <span>Int</span><span>,</span> address<span>:</span> <span>String</span><span>,</span> orderTime<span>:</span> Date<span>)</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>eventtime<span>.</span></span><span>SerializableTimestampAssigner</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>eventtime<span>.</span></span><span>WatermarkStrategy</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>typeinfo<span>.</span></span><span>TypeInformation</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>connectors<span>.</span>kafka<span>.</span></span><span>KafkaDeserializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>kafka<span>.</span>clients<span>.</span>consumer<span>.</span></span><span>ConsumerRecord</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>
<span>import</span> <span>java<span>.</span>time<span>.</span></span><span>Duration</span><span>;</span>
<span>import</span> <span>java<span>.</span>util<span>.</span></span><span>Date</span><span>;</span>

<span>import</span> <span>static</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>java<span>.</span>typeutils<span>.</span></span><span>TypeExtractor</span><span>.</span>getForClass<span>;</span>

<span>/**
 * @author benjobs
 */</span>
<span>public</span> <span>class</span> <span>KafkaSourceStrategyJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>deserializer</span><span>(</span><span>new</span> <span>JavaUserSchema</span><span>(</span><span>)</span><span>)</span>
                <span>.</span><span>strategy</span><span>(</span>
                        <span>WatermarkStrategy</span><span>.</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>></span></span><span>forBoundedOutOfOrderness</span><span>(</span><span>Duration</span><span>.</span><span>ofMinutes</span><span>(</span><span>1</span><span>)</span><span>)</span>
                                <span>.</span><span>withTimestampAssigner</span><span>(</span>
                                        <span>(</span><span>SerializableTimestampAssigner</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>></span></span><span>)</span>
                                                <span>(</span>element<span>,</span> recordTimestamp<span>)</span> <span>-></span> element<span>.</span><span>value</span><span>(</span><span>)</span><span>.</span>orderTime<span>.</span><span>getTime</span><span>(</span><span>)</span>
                                <span>)</span>
                <span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>JavaUser</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span>
                <span>.</span><span>print</span><span>(</span><span>)</span><span>;</span>


        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUserSchema</span> <span>implements</span> <span>KafkaDeserializationSchema</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>{</span>
    <span>private</span> <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>boolean</span> <span>isEndOfStream</span><span>(</span><span>JavaUser</span> nextElement<span>)</span> <span>return</span> <span>false</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>TypeInformation</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> <span>getProducedType</span><span>(</span><span>)</span> <span>return</span> <span>getForClass</span><span>(</span><span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>@Override</span> <span>public</span> <span>JavaUser</span> <span>deserialize</span><span>(</span><span>ConsumerRecord</span><span>&lt;</span><span>byte</span><span>[</span><span>]</span><span>,</span> <span>byte</span><span>[</span><span>]</span><span>></span> <span>record</span><span>)</span> <span>throws</span> <span>Exception</span> <span>{</span>
        <span>String</span> value <span>=</span> <span>new</span> <span>String</span><span>(</span><span>record</span><span>.</span><span>value</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>return</span> mapper<span>.</span><span>readValue</span><span>(</span>value<span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
    <span>Date</span> orderTime<span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br><span>59</span><br><span>60</span><br><span>61</span><br><span>62</span><br><span>63</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>注意事项</p>
<p>如果<code>watermark assigner</code>依赖于从<code>Kafka</code>读取的消息来上涨其<code>watermark</code>(通常就是这种情况),那么所有主题和分区都需要有连续的消息流。否则, <mark>整个应用程序的<code>watermark</code>将无法上涨</mark> ，所有基于时间的算子(例如时间窗口或带有计时器的函数)也无法运行。单个的<code>Kafka</code>分区也会导致这种反应。考虑设置适当的 <mark><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/event_timestamps_watermarks.html#dealing-with-idle-sources" target="_blank" rel="noopener noreferrer"><code>idelness timeouts</code></a></mark> 来缓解这个问题。</p>
</div>
<h2 id="kafka-sink-producer"> Kafka Sink (Producer)</h2>
<p>在<code>StreamX</code>中<code>Kafka Producer</code> 被称为<code>KafkaSink</code>,它允许将消息写入一个或多个<code>Kafka topic中</code></p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code> <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span><span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
 KafkaSink<span>(</span><span>)</span><span>.</span>sink<span>(</span>source<span>)</span>     
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code> <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
 <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
 <span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
         <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
         <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>String</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span><span>;</span>
 
 <span>new</span> <span>KafkaSink</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span><span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>
 
 context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p><code>sink</code>是具体的写入数据的方法,参数列表如下</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">参数类型</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">默认值</th>
<th style="text-align:left">必须</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>stream</code></td>
<td style="text-align:left">DataStream[T]</td>
<td style="text-align:left">要写的数据流</td>
<td style="text-align:left">无</td>
<td style="text-align:left"><i style="color:green"></i></td>
</tr>
<tr>
<td style="text-align:left"><code>alias</code></td>
<td style="text-align:left">String</td>
<td style="text-align:left"><code>kafka</code>的实例别名</td>
<td style="text-align:left">无</td>
<td style="text-align:left"><i style="color:red"></i></td>
</tr>
<tr>
<td style="text-align:left"><code>serializationSchema</code></td>
<td style="text-align:left">SerializationSchema[T]</td>
<td style="text-align:left">写入的序列化器</td>
<td style="text-align:left">SimpleStringSchema</td>
<td style="text-align:left"><i style="color:red"></i></td>
</tr>
<tr>
<td style="text-align:left"><code>partitioner</code></td>
<td style="text-align:left">FlinkKafkaPartitioner[T]</td>
<td style="text-align:left">kafka分区器</td>
<td style="text-align:left">KafkaEqualityPartitioner[T]</td>
<td style="text-align:left"><i style="color:red"></i></td>
</tr>
</tbody>
</table>
<h3 id="容错和语义"> 容错和语义</h3>
<p>启用 Flink 的 <code>checkpointing</code> 后，<code>KafkaSink</code> 可以提供<code>精确一次</code>的语义保证,具体开启<code>checkpointing</code>的设置请参考第二章关于<a href="/doc/guide/quickstart/conf/#checkpoints">项目配置</a>部分</p>
<p>除了启用 Flink 的 checkpointing，你也可以通过将适当的 <code>semantic</code> 参数传递给 <code>KafkaSink</code> 来选择三种不同的操作模式</p>
<div>
<ul>
<li>EXACTLY_ONCE  使用 Kafka 事务提供精确一次语义</li>
<li>AT_LEAST_ONCE 至少一次,可以保证不会丢失任何记录(但是记录可能会重复)</li>
<li>NONE Flink 不会有任何语义的保证，产生的记录可能会丢失或重复</li>
</ul>
</div>
<p>具体操作如下,只需要在<code>kafka.sink</code>下配置<code>semantic</code>即可</p>
<div><pre><code><span>kafka.sink</span><span>:</span>
    <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
    <span>topic</span><span>:</span> kfk_sink
    <span>transaction.timeout.ms</span><span>:</span> <span>1000</span>
    <span>semantic</span><span>:</span> AT_LEAST_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
    <span>batch.size</span><span>:</span> <span>1</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><div><p>注意事项</p>
<p><code>Semantic.EXACTLY_ONCE</code>模式依赖于事务提交的能力。事务提交发生于触发 checkpoint 之前，以及从 checkpoint 恢复之后。如果从 Flink 应用程序崩溃到完全重启的时间超过了 Kafka 的事务超时时间，那么将会有数据丢失（Kafka 会自动丢弃超出超时时间的事务）。考虑到这一点，请根据预期的宕机时间来合理地配置事务超时时间。</p>
<p>默认情况下，Kafka broker 将 transaction.max.timeout.ms 设置为 15 分钟。此属性不允许为大于其值的 producer 设置事务超时时间。 默认情况下，FlinkKafkaProducer 将 producer config 中的 transaction.timeout.ms 属性设置为 1 小时，因此在使用 Semantic.EXACTLY_ONCE 模式之前应该增加 transaction.max.timeout.ms 的值。</p>
<p>在 KafkaConsumer 的 read_committed 模式中，任何未结束（既未中止也未完成）的事务将阻塞来自给定 Kafka topic 的未结束事务之后的所有读取数据。 换句话说，在遵循如下一系列事件之后：</p>
<div>
<ul>
<li>用户启动了 transaction1 并使用它写了一些记录</li>
<li>用户启动了 transaction2 并使用它编写了一些其他记录</li>
<li>用户提交了 transaction2</li>
</ul>
</div>
<p>即使 transaction2 中的记录已提交，在提交或中止 transaction1 之前，消费者也不会看到这些记录。这有 2 层含义：</p>
<ul>
<li>首先，在 Flink 应用程序的正常工作期间，用户可以预料 Kafka 主题中生成的记录的可见性会延迟，相当于已完成 checkpoint 之间的平均时间。</li>
<li>其次，在 Flink 应用程序失败的情况下，此应用程序正在写入的供消费者读取的主题将被阻塞，直到应用程序重新启动或配置的事务超时时间过去后，才恢复正常。此标注仅适用于有多个 agent 或者应用程序写入同一 Kafka 主题的情况。</li>
</ul>
<p>注意：<code>Semantic.EXACTLY_ONCE</code> 模式为每个 FlinkKafkaProducer 实例使用固定大小的 KafkaProducer 池。每个 checkpoint 使用其中一个 producer。如果并发 checkpoint 的数量超过池的大小，FlinkKafkaProducer 将抛出异常，并导致整个应用程序失败。请合理地配置最大池大小和最大并发 checkpoint 数量。</p>
<p>注意：<code>Semantic.EXACTLY_ONCE</code> 会尽一切可能不留下任何逗留的事务，否则会阻塞其他消费者从这个 Kafka topic 中读取数据。但是，如果 Flink 应用程序在第一次 checkpoint 之前就失败了，那么在重新启动此类应用程序后，系统中不会有先前池大小（pool size）相关的信息。因此，在第一次 checkpoint 完成前对 Flink 应用程序进行缩容，且并发数缩容倍数大于安全系数 FlinkKafkaProducer.SAFE_SCALE_DOWN_FACTOR 的值的话，是不安全的。</p>
</div>
<h3 id="多实例kafka指定alias"> 多实例kafka指定alias</h3>
<p>如果写时有多个不同实例的kafka需要配置,同样采用<code>alias</code>来区别不用的kafka实例,配置如下:</p>
<div><pre><code><span>kafka.sink</span><span>:</span>
    <span>kafka_cluster1</span><span>:</span>
        <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
        <span>topic</span><span>:</span> kfk_sink
        <span>transaction.timeout.ms</span><span>:</span> <span>1000</span>
        <span>semantic</span><span>:</span> AT_LEAST_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
        <span>batch.size</span><span>:</span> <span>1</span>
    <span>kafka_cluster2</span><span>:</span>
        <span>bootstrap.servers</span><span>:</span> kfk6<span>:</span><span>9092</span><span>,</span>kfk7<span>:</span><span>9092</span><span>,</span>kfk8<span>:</span><span>9092</span>
        <span>topic</span><span>:</span> kfk_sink
        <span>transaction.timeout.ms</span><span>:</span> <span>1000</span>
        <span>semantic</span><span>:</span> AT_LEAST_ONCE <span># EXACTLY_ONCE|AT_LEAST_ONCE|NONE</span>
        <span>batch.size</span><span>:</span> <span>1</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><p>在写入的时候,需要手动指定<code>alias</code>,注意下<code>scala</code> api和<code>java</code> api在代码上稍有不同,<code>scala</code>直接在<code>sink</code>方法里指定参数,<code>java</code> api则是通过<code>alias()</code>方法来设置,其底层实现是完全一致的</p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code> <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span><span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span><span>.</span>map<span>(</span>_<span>.</span>value<span>)</span>
 KafkaSink<span>(</span><span>)</span><span>.</span>sink<span>(</span>source<span>,</span>alias <span>=</span> <span>"kafka_cluster1"</span><span>)</span>     
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code> <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
 <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
 <span>DataStream</span><span><span>&lt;</span><span>String</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
         <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
         <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>String</span><span>></span></span><span>)</span> <span>KafkaRecord</span><span>::</span><span>value</span><span>)</span><span>;</span>
 
 <span>new</span> <span>KafkaSink</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span><span>.</span><span>alias</span><span>(</span><span>"kafka_cluster1"</span><span>)</span><span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>
 
 context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="指定serializationschema"> 指定SerializationSchema</h3>
<p><code>Flink Kafka Producer</code> 需要知道如何将 Java/Scala 对象转化为二进制数据。 KafkaSerializationSchema 允许用户指定这样的schema, 相关操作方式和文档请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html#the-serializationschema" target="_blank" rel="noopener noreferrer">官网文档</a></p>
<p>在<code>KafkaSink</code>里默认不指定序列化方式,采用的是<code>SimpleStringSchema</code>来进行序列化,这里开发者可以显示的指定一个自定义的序列化器,通过<code>serializationSchema</code>参数指定即可,例如,将<code>user</code>对象安装自定义的格式写入<code>kafka</code></p>
<CodeGroup>
<CodeGroupItem title="scala">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>serialization<span>.</span></span>SerializationSchema
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> KafkaSinkApp <span>extends</span> FlinkStreaming <span>{</span>
  
  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span>
      <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
      <span>.</span>map<span>(</span>x <span>=></span> JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>x<span>.</span>value<span>)</span><span>)</span>
      
    KafkaSink<span>(</span><span>)</span><span>.</span>sink<span>[</span>User<span>]</span><span>(</span>source<span>,</span> serialization <span>=</span> <span>new</span> SerializationSchema<span>[</span>User<span>]</span><span>(</span><span>)</span> <span>{</span>
      <span>override</span> <span>def</span> serialize<span>(</span>user<span>:</span> User<span>)</span><span>:</span> Array<span>[</span><span>Byte</span><span>]</span> <span>=</span> <span>{</span>
        s<span>"${user.name},${user.age},${user.gender},${user.address}"</span><span>.</span>getBytes
      <span>}</span>
    <span>}</span><span>)</span>
    
  <span>}</span>
  
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span> <span>String</span><span>,</span> age<span>:</span> <span>Int</span><span>,</span> gender<span>:</span> <span>Int</span><span>,</span> address<span>:</span> <span>String</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>sink<span>.</span></span><span>KafkaSink</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>FilterFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>serialization<span>.</span></span><span>SerializationSchema</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>api<span>.</span>datastream<span>.</span></span><span>DataStream</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>public</span> <span>class</span> kafkaSinkJavaApp <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>

        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>
        <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>

        <span>DataStream</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span>
                        mapper<span>.</span><span>readValue</span><span>(</span>value<span>.</span><span>value</span><span>(</span><span>)</span><span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>)</span><span>;</span>

        <span>new</span> <span>KafkaSink</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>serializer</span><span>(</span>
                        <span>(</span><span>SerializationSchema</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> element <span>-></span>
                                <span>String</span><span>.</span><span>format</span><span>(</span><span>"%s,%d,%d,%s"</span><span>,</span> element<span>.</span>name<span>,</span> element<span>.</span>age<span>,</span> element<span>.</span>gender<span>,</span> element<span>.</span>address<span>)</span><span>.</span><span>getBytes</span><span>(</span><span>)</span>
                <span>)</span><span>.</span><span>sink</span><span>(</span>source<span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="指定partitioner"> 指定partitioner</h3>
<p><code>KafkaSink</code>允许显示的指定一个kafka分区器,不指定默认使用<code>StreamX</code>内置的 <mark>KafkaEqualityPartitioner</mark> 分区器,顾名思义,该分区器可以均匀的将数据写到各个分区中去,<code>scala</code> api是通过<code>partitioner</code>参数来设置分区器,
<code>java</code> api中是通过<code>partitioner()</code>方法来设置的</p>
<div><p>注意事项</p>
<p>Flink Kafka Connector中默认使用的是 <mark>FlinkFixedPartitioner</mark> 分区器,该分区器需要特别注意<code>sink</code>的并行度和<code>kafka</code>的分区数,不然会出现往一个分区写</p>
</div>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/streamx_kafkaapi.jpeg" type="image/jpeg"/>
    </item>
    <item>
      <title>MongoDb Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/mongo/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/mongo/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">MongoDb Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Redis Connector</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/connector/redis/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/connector/redis/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Redis Connector</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
    <item>
      <title>安装部署</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/console/deploy/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/console/deploy/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">安装部署</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<blockquote>
<p>streamx-console 是一个综合实时数据平台，低代码 ( Low Code ) ,Flink Sql 平台，可以较好的管理 Flink 任务，集成了项目编译、发布、参数配置、启动、savepoint,火焰图 ( flame graph ) ,Flink SQL,监控等诸多功能于一体，大大简化了 Flink 任务的日常操作和维护，融合了诸多最佳实践。其最终目标是打造成一个实时数仓，流批一体的一站式大数据解决方案</p>
</blockquote>
<p>streamx-console 提供了开箱即用的安装包，安装之前对环境有些要求，具体要求如下：</p>
<h2 id="环境要求"> 环境要求</h2>
<ClientOnly>
  <table-data name="envs"></table-data>
</ClientOnly>
<p>目前 StreamX 对 Flink 的任务发布，同时支持 <code>Flink on YARN</code> 和 <code>Flink on Kubernetes</code> 两种模式。</p>
<h3 id="hadoop"> Hadoop</h3>
<p>使用 <code>Flink on YARN</code>，需要部署的集群安装并配置 Hadoop的相关环境变量，如你是基于 CDH 安装的 hadoop 环境，
相关环境变量可以参考如下配置:</p>
<div><pre><code><span>export</span> <span>HADOOP_HOME</span><span>=</span>/opt/cloudera/parcels/CDH/lib/hadoop <span>#hadoop 安装目录</span>
<span>export</span> <span>HADOOP_CONF_DIR</span><span>=</span>/etc/hadoop/conf
<span>export</span> <span>HIVE_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hive
<span>export</span> <span>HBASE_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hbase
<span>export</span> <span>HADOOP_HDFS_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hadoop-hdfs
<span>export</span> <span>HADOOP_MAPRED_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hadoop-mapreduce
<span>export</span> <span>HADOOP_YARN_HOME</span><span>=</span><span>$HADOOP_HOME</span>/<span>..</span>/hadoop-yarn
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><h3 id="kubernetes"> Kubernetes</h3>
<p>使用 <code>Flink on Kubernetes</code>，需要额外部署/或使用已经存在的 Kubernetes 集群，请参考条目： <a href="./../flink-k8s/1-deployment.html"><strong>StreamX Flink-K8s 集成支持</strong></a>。</p>
<h2 id="编译-安装"> 编译 &amp; 安装</h2>
<p>你可以选择手动编译安装也可以直接下载编译好的安装包，手动编译安装步骤如下</p>
<h3 id="编译"> 编译</h3>
<ul>
<li>Maven 3.6+</li>
<li>npm 7.11.2 ( https://nodejs.org/en/ )</li>
<li>JDK 1.8+</li>
</ul>
<div><pre><code><span>git</span> clone https://github.com/streamxhub/streamx.git
<span>cd</span> Streamx
mvn clean <span>install</span> -DskipTests -Denv<span>=</span>prod
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>安装完成之后就看到最终的工程文件，位于 <code>streamx/streamx-console/streamx-console-service/target/streamx-console-service-1.0.0-bin.tar.gz</code>,解包后安装目录如下</p>
<div><pre><code>.
streamx-console-service-1.0.0
├── bin
│    ├── flame-graph
│    ├──   └── *.py                                             //火焰图相关功能脚本 ( 内部使用，用户无需关注 )
│    ├── startup.sh                                             //启动脚本
│    ├── setclasspath.sh                                        //java 环境变量相关的脚本 ( 内部使用，用户无需关注 )
│    ├── shutdown.sh                                            //停止脚本
│    ├── yaml.sh                                                //内部使用解析 yaml 参数的脚本 ( 内部使用，用户无需关注 )
├── conf
│    ├── application.yaml                                       //项目的配置文件 ( 注意不要改动名称 )
│    ├── application-prod.yml                                   //项目的配置文件 ( 开发者部署需要改动的文件，注意不要改动名称 )
│    ├── flink-application.template                             //flink 配置模板 ( 内部使用，用户无需关注 )
│    ├── logback-spring.xml                                     //logback
│    └── ...
├── lib
│    └── *.jar                                                  //项目的 jar 包
├── plugins
│    ├── streamx-jvm-profiler-1.0.0.jar                         //jvm-profiler,火焰图相关功能 ( 内部使用，用户无需关注 )
│    └── streamx-flink-sqlclient-1.0.0.jar                      //Flink SQl 提交相关功能 ( 内部使用，用户无需关注 )
├── logs                                                        //程序 log 目录
├── temp                                                        //内部使用到的零时路径，不要删除
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div><h3 id="修改配置"> 修改配置</h3>
<p>安装解包已完成，接下来准备数据相关的工作</p>
<ul>
<li>新建数据库 <code>streamx</code>
确保在部署机可以连接的 mysql 里新建数据库 <code>streamx</code></li>
<li>修改连接信息
进入到 <code>conf</code> 下，修改 <code>conf/application-prod.yml</code>,找到 datasource 这一项，找到 mysql 的配置，修改成对应的信息即可，如下</li>
</ul>
<div><pre><code>  <span>datasource</span><span>:</span>
    <span>dynamic</span><span>:</span>
      <span># 是否开启 SQL 日志输出，生产环境建议关闭，有性能损耗</span>
      <span>p6spy</span><span>:</span> <span>false</span>
      <span>hikari</span><span>:</span>
        <span>connection-timeout</span><span>:</span> <span>30000</span>
        <span>max-lifetime</span><span>:</span> <span>1800000</span>
        <span>max-pool-size</span><span>:</span> <span>15</span>
        <span>min-idle</span><span>:</span> <span>5</span>
        <span>connection-test-query</span><span>:</span> select 1
        <span>pool-name</span><span>:</span> HikariCP<span>-</span>DS<span>-</span>POOL
      <span># 配置默认数据源</span>
      <span>primary</span><span>:</span> primary
      <span>datasource</span><span>:</span>
        <span># 数据源-1，名称为 primary</span>
        <span>primary</span><span>:</span>
          <span>username</span><span>:</span> $user
          <span>password</span><span>:</span> $password
          <span>driver-class-name</span><span>:</span> com.mysql.cj.jdbc.Driver
          <span>url</span><span>:</span> <span>jdbc</span><span>:</span> mysql<span>:</span>//$host<span>:</span>$port/streamx<span>?</span>useUnicode=true<span>&amp;characterEncoding=UTF-8&amp;useJDBCCompliantTimezoneShift=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=GMT%2B8</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br></div></div><div><p>提示</p>
<p>特别提示: 安装过程中不需要手动做数据初始化，只需要设置好数据库信息即可，会自动完成建表和数据初始化等一些列操作</p>
</div>
<h3 id="启动"> 启动</h3>
<p>进入到 <code>bin</code> 下直接执行 startup.sh 即可启动项目，默认端口是 <mark>10000</mark>,如果没啥意外则会启动成功</p>
<div><pre><code><span>cd</span> streamx-console-service-1.0.0/bin
<span>bash</span> startup.sh
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><p>相关的日志会输出到 <mark>streamx-console-service-1.0.0/logs/streamx.out</mark> 里</p>
<p>打开浏览器 输入 <strong> <mark>http://$host:10000</mark> </strong> 即可登录，登录界面如下</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_login.jpeg"/>
<div><p>提示</p>
<p>默认密码: <strong> admin / streamx </strong></p>
</div>
<h2 id="系统配置"> 系统配置</h2>
<p>进入系统之后，第一件要做的事情就是修改系统配置，在菜单/StreamX/Setting 下，操作界面如下:</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_settings.png"/>
<p>主要配置项分为以下几类</p>
<div>
<ul>
<li>Flink Home</li>
<li>Maven Home</li>
<li>StreamX Env</li>
<li>Email</li>
</ul>
</div>
<h3 id="flink-home"> Flink Home</h3>
<p>这里配置全局的 Flink Home,此处是系统唯一指定 Flink 环境的地方，会作用于所有的作业</p>
<div><p>提示</p>
<p>特别提示: 最低支持的 Flink 版本为 1.11.1, 之后的版本都支持</p>
</div>
<h3 id="maven-home"> Maven Home</h3>
<p>指定 maven Home, 目前暂不支持，下个版本实现</p>
<h3 id="streamx-env"> StreamX Env</h3>
<ul>
<li>StreamX Webapp address <br>
这里配置 StreamX Console 的 web url 访问地址，主要火焰图功能会用到，具体任务会将收集到的信息通过此处暴露的 url 发送 http 请求到系统，进行收集展示<br></li>
<li>StreamX Console Workspace <br>
配置系统的工作空间，用于存放项目源码，编译后的项目等</li>
</ul>
<h3 id="email"> Email</h3>
<p>Alert Email 相关的配置是配置发送者邮件的信息，具体配置请查阅相关邮箱资料和文档进行配置</p>
]]></content:encoded>
    </item>
    <item>
      <title>开发环境</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/console/deployment/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/console/deployment/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">开发环境</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p><span style="background-color:#ffffff; color:#333333">&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span><a href="https://github.com/streamxhub/streamx" target="_blank">StreamX</a><span style="background-color:#ffffff; color:#333333">&nbsp;</span>遵循 Apache-2.0 开源协议，将会是个长期更新的活跃项目，欢迎大家提交<a href="https://github.com/streamxhub/streamx/pulls">PR</a> 或 <a href="https://github.com/streamxhub/streamx/issues/new/choose">Issue</a>。喜欢请给个 <a href="https://github.com/streamxhub/streamx/stargazers">Star</a>。您的支持是我们最大的动力， 该项目从开源以来受到不少朋友的关注和认可，表示感谢，已陆续有来自金融，数据分析，车联网，智能广告，地产等公司的朋友在使用或二开，也不乏来自一线大厂的朋友在研究使用，欢迎更多的开发者加入一块贡献，不只是代码的贡献，还寻求使用文档，体验报告，问答等方面的贡献</p>
<p>StreamX 总体组件栈架构如下， 由 streamx-core 和 streamx-console 两个大的部分组成，其中 streamx-core 是开发时框架，这里不做讲解，本章节具体讲讲如何在本地搭建 streamx-console 流批一体平台的开发环境，为了方便讲解，本文中所说的 <code>streamx-console</code> 均指 <code>streamx-console 平台</code></p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_archite.png"/><br>
</center>
<p>StreamX Console 从 1.2.0 开始实现了 Flink-Runtime 的解耦，即<strong>不强制依赖 Hadoop 或 Kubernetes 环境</strong>，可以根据实际开发/使用需求自行安装 Hadoop 或 Kubernetes。</p>
<br/>
<h2 id="安装-hadoop-可选-yarn-runtime"> 安装 Hadoop（可选，YARN Runtime）</h2>
<p>关于 hadoop 环境有两种方式解决，<code>本地安装 hadoop 环境</code> 和 <code>使用已有的 hadoop 环境</code>,不论是本地安装 hadoop 环境还是使用已有的 hadoop 环境，都需要确保以下条件</p>
<ul>
<li>安装并且配置好 <code>hadoop</code>,<code>yarn</code></li>
<li>已配置 <code>HADOOP_HOME</code> 和 <code>HADOOP_CONF_DIR</code></li>
<li>已成功启动 <code>hadoop</code> 和 <code>yarn</code></li>
</ul>
<h3 id="本地安装-hadoop-环境"> 本地安装 Hadoop 环境</h3>
<p>关于如何在本地安装 Hadoop 环境可自行查阅相关资料，这里不作过多讲解。</p>
<h3 id="使用已有-hadoop-集群"> 使用已有 Hadoop 集群</h3>
<p>推荐使用已有的 Hadoop 集群 ( 测试环境 ) ,如使用已有 hadoop 集群需要将以下配置 copy 到开发机器</p>
<ul>
<li><code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code> 这三个配置文件 copy 到开发机器</li>
<li>如果开启了 kerberos 认证，需要将 <code>keytab</code> 文件和 <code>krb5.conf</code> copy 到开发机器</li>
</ul>
<p>需要注意的是，<code>core-site.xml</code>, <code>hdfs-site.xml</code>, <code>yarn-site.xml</code> 这些配置文件里的一系列主机地址 ( host ) 需要在当前的开发机器上配置出来，需要确保本机可以连接集群里的机器。</p>
<h2 id="安装-kubernetes-可选-k8s-runtime"> 安装 Kubernetes （可选，K8s Runtime）</h2>
<p>本地开发可以通过 MiniKube 或 KubeSphere 等项目快速安装 Kubernetes 环境，当然选择现有的 K8s Cluster 设施更加推荐。此外按时计费的腾讯云 TKE / 阿里云 ACK 也是快速开发很好的选择。</p>
<p>额外配置需求请参考： <a href="./../flink-k8s/1-deployment.html"><strong>StreamX Flink-K8s 集成支持</strong></a></p>
<h2 id="安装-flink-可选-standalone-runtime"> 安装 Flink（可选，Standalone Runtime）</h2>
<p>从官网下载 Flink,并且启动测试，配置 FLINK_HOME</p>
<div><pre><code><span>wget</span> https://mirrors.bfsu.edu.cn/apache/flink/flink-1.13.1/flink-1.13.1-bin-scala_2.11.tgz
<span>tar</span> xzf flink-1.13.1-bin-scala_2.11.tgz /opt/
<span>cd</span> /opt/flink-1.13.1
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>启动本地 Flink 群集，可以检查下 flink 是否正常启动</p>
<div><pre><code>./bin/start-cluster.sh
</code></pre>
<div><span>1</span><br></div></div><h2 id="安装-maven"> 安装 Maven</h2>
<p>最新的 Maven 下载地址：<code>http://maven.apache.org/download.cgi</code>，我们创建一个连接，以便 mvn 可以在任何地方运行。</p>
<div><pre><code><span>cd</span> ~
<span>wget</span> https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.8.1/binaries/apache-maven-3.8.1-bin.tar.gz
<span>tar</span> -xzvf apache-maven-3.8.1-bin.tar.gz
<span>ln</span> -s /root/apache-maven-3.8.1/bin/mvn /usr/bin/mvn
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></div></div><h2 id="安装-mysql"> 安装 MySQL</h2>
<p><code>console</code> 用到了 MySQL,因此需要准备 MySQL 环境，你可以本地安装 MySQL,也可以直接使用已有的 MySQL,关于 MySQL 的安装配置，请自行查阅资料，这里不作过多讲解</p>
<h2 id="安装-nodejs"> 安装 Nodejs</h2>
<p><code>console</code> 前端部分采用 nodejs 开发，需要 nodejs 环境，下载安装最新的 nodejs 即可</p>
<h2 id="安装配置-streamx"> 安装配置 StreamX</h2>
<p>如果以上准备工作都已经就绪，此时就可以安装配置 <code>streamx-console</code> 了，<code>streamx-console</code> 是前后端分离的项目，在项目最终打包部署时为了方便快捷，减少用户的使用和学习成本，使用了前后端混合打包部署模式，但在开发阶段建议使用前后端分离模式进行开发调试，具体步骤如下</p>
<h3 id="后端"> 后端</h3>
<p><code>streamx-console</code> 后端采用 springBoot + Mybatis 开发， JWT 权限验证，非常常见的后端技术栈。下面来看看后端按照部署具体流程</p>
<h4 id="编译"> 编译</h4>
<p>首先将 <code>StreamX</code> 工程下载到本地并且编译</p>
<div><pre><code><span>git</span> clone https://github.com/streamxhub/streamx.git
<span>cd</span> streamx
mvn clean <span>install</span> -DskipTests -Denv<span>=</span>prod
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><h4 id="解包"> 解包</h4>
<p>安装完成之后就看到最终的工程文件解包，位于 <code>streamx/streamx-console/streamx-console-service/target/streamx-console-service-${version}-bin.tar.gz</code>,解包之后的目录如下:</p>
<div><pre><code>.
streamx-console-service-${version}
├── bin
│    ├── flame-graph
│    ├──   └── *.py
│    ├── startup.sh
│    ├── setclasspath.sh
│    ├── shutdown.sh
│    ├── yaml.sh
├── conf
│    ├── application.yaml
│    ├── application-prod.yml
│    ├── flink-application.template
│    ├── logback-spring.xml
│    └── ...
├── lib
│    └── *.jar
├── plugins
│    ├── streamx-jvm-profiler-1.0.0.jar
│    └── streamx-flink-sqlclient-1.0.0.jar
├── logs
├── temp
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br></div></div><p>将解包后的整个工程文件 copy 到 target 之外的其他任意位置即可完成此步骤，该步主要是防止下次执行 mvn clean 被清理，如放到 <code>/opt/streamx/</code>,则此时该文件的完整路径是 <code>/opt/streamx/streamx-console-service-${version}</code>,记住这个路径，后面会用到</p>
<h4 id="配置"> 配置</h4>
<p>用 IDE 导入刚从 git 上 clone 下来的 StreamX 源码 ( 推荐使用 <code>IntelliJ IDEA</code> ) ,进入到 <code>resources</code> 下，编辑 application-prod.xml,找到 <code>datasource</code>,修改下 jdbc 的连接信息，具体可参考安装部署章节 <a href="http://www.streamxhub.com/zh/doc/console/deploy/#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE" target="_blank" rel="noopener noreferrer">修改配置</a> 部分</p>
<img src="/streamx-docs/assets/img/doc-img/console_conf.jpg" />
<p>如果你要连接的目标集群开启了 kerberos 认证，则需要配置 kerberos 信息，在 <code>resources</code> 下找到 <code>kerberos.xml</code> 配置上相关信息即可，默认 kerberos 是关闭状态，要启用需将 <code>enable</code> 设置为 true, 如下:</p>
<div><pre><code><span>security</span><span>:</span>
  <span>kerberos</span><span>:</span>
    <span>login</span><span>:</span>
      <span>enable</span><span>:</span> <span>false</span>
      <span>principal</span><span>:</span>
      <span>krb5</span><span>:</span>
      <span>keytab</span><span>:</span>
<span>java</span><span>:</span>
  <span>security</span><span>:</span>
    <span>krb5</span><span>:</span>
      <span>conf</span><span>:</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></div></div><h4 id="启动"> 启动</h4>
<p><code>streamx-console</code> 是基于 springBoot 开发的 web 应用，<code>com.streamxhub.streamx.console.StreamXConsole</code> 为主类， 在启动主类之前，需要设置下 <code>VM options</code> 和 <code>Environment variables</code></p>
<h5 id="vm-options"> VM options</h5>
<p>在 <code>VM options</code> 需要设置 <code>app.home</code>:值为上面解包后的 streamx-console 的完整路径:</p>
<div><pre><code>-Dapp.home<span>=</span>/opt/streamx/streamx-console-service-<span>${version}</span>
</code></pre>
<div><span>1</span><br></div></div><p><br><br>
如果开发机使用的 jdk 版本是 jdk1.8 以上版本， 则需要加上如下参数: <br></p>
<div><pre><code><span>-</span><span>-</span>add<span>-</span>opens java.base/jdk.internal.loader=ALL<span>-</span>UNNAMED <span>-</span><span>-</span>add<span>-</span>opens jdk.zipfs/jdk.nio.zipfs=ALL<span>-</span>UNNAMED
</code></pre>
<div><span>1</span><br></div></div><h5 id="environment-variables"> Environment variables</h5>
<p>如使用非本地安装的 hadoop 集群 ( 测试 hadoop ) <code>Environment variables</code> 中需要配置 <code>HADOOP_USER_NAME</code> 和 <code>HADOOP_CONF_DIR</code>,
<code>HADOOP_USER_NAME</code> 为 hdfs 或者有读写权限的 hadoop 用户名，<code>HADOOP_CONF_DIR</code> 为上面第一步安装 hadoop 步骤中从测试集群 copy 相关配置文件在开发机器上的存放位置，如果是本地安装的 hadoop 则不需要配置该项，</p>
<img src="/streamx-docs/assets/img/doc-img/streamx_ideaopt.jpg" />
<p>如果一切准假就绪，就可以直接启动 <code>StreamXConsole</code> 主类启动项目，后端就启动成功了。会看到有相关的启动信息打印输出</p>
<h3 id="前端"> 前端</h3>
<p>streamx web 前端部分采用 nodejs + vue 开发，因此需要在机器上按照 node 环境，完整流程如下:</p>
<h4 id="修改请求-url"> 修改请求 URL</h4>
<p>由于是前后端分离项目，前端需要知道后端 ( streamx-console ) 的访问地址，才能前后配合工作，因此需要后端的 URL,具体位置在:
<code>streamx-console/streamx-console-webapp/src/api/baseUrl.js</code></p>
<p>配置默认如下:</p>
<div><pre><code><span>export</span> <span>function</span> <span>baseUrl</span><span>(</span><span>)</span> <span>{</span>
    <span>let</span> url <span>=</span> <span>''</span>
    <span>switch</span> <span>(</span>process<span>.</span>env<span>.</span><span>NODE_ENV</span><span>)</span> <span>{</span>
        <span>//混合打包 ( production,不用配置，maven 编译项目阶段-Denv=prod 自动将环境参数透传到这里 )</span>
        <span>case</span> <span>'production'</span><span>:</span>
            url <span>=</span> <span>(</span>arguments<span>[</span><span>0</span><span>]</span> <span>||</span> <span>null</span><span>)</span> <span>?</span> <span>(</span>location<span>.</span>protocol <span>+</span> <span>'//'</span> <span>+</span> location<span>.</span>host<span>)</span> <span>:</span> <span>'/'</span>
            <span>break</span>
        <span>//开发测试阶段采用前后端分离，这里配置后端的请求 URI</span>
        <span>case</span> <span>'development'</span><span>:</span>
            url <span>=</span> <span>'http://localhost:10000'</span>
            <span>break</span>
    <span>}</span>
    <span>return</span> url
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></div></div><p>将 <code>development</code> 下的 URL 连接改为后端的 URI 即可</p>
<h4 id="编译项目"> 编译项目</h4>
<p>接下来需要编译项目，具体步骤如下:</p>
<div><pre><code><span>cd</span> streamx-console/streamx-console-webapp
<span>npm</span> <span>install</span>
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><h4 id="启动项目"> 启动项目</h4>
<p>以上步骤执行完毕即可启动项目即可</p>
<div><pre><code><span>cd</span> streamx-console/streamx-console-webapp
<span>npm</span> run serve
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div>]]></content:encoded>
    </item>
    <item>
      <title>快速开始</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/console/quickstart/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/console/quickstart/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">快速开始</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="如何使用"> 如何使用</h2>
<p>streamx-console 定位是流批一体的大数据平台，一站式解决方案，使用起来非常简单，没有复杂的概念和繁琐的操作，标准的 Flink 程序 ( 安装 Flink 官方要去的结构和规范 ) 和用 <code>streamx</code> 开发的项目都做了很好的支持，下面我们使用 <code>streamx-quickstart</code> 来快速开启 streamx-console 之旅</p>
<p><code>streamx-quickstart</code> 是 StreamX 开发 Flink 的上手示例程序，具体请查阅</p>
<ul>
<li>Github: <a href="https://github.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">https://github.com/streamxhub/streamx-quickstart.git</a></li>
<li>Gitee: <a href="https://gitee.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">https://gitee.com/streamxhub/streamx-quickstart.git</a></li>
</ul>
<h3 id="部署-datastream-任务"> 部署 DataStream 任务</h3>
<p>下面的示例演示了如何部署一个 DataStream 应用</p>
<p><video src="/streamx-docs/assets/video/datastream.mp4" controls="controls" width="100%" height="100%"></video></p>
<h3 id="部署-flinksql-任务"> 部署 FlinkSql 任务</h3>
<p>下面的示例演示了如何部署一个 FlinkSql 应用</p>
<p><video src="/streamx-docs/assets/video/flinksql.mp4" controls="controls" width="100%" height="100%"></video></p>
<ul>
<li>项目演示使用到的 flink sql 如下</li>
</ul>
<div><pre><code><span>CREATE</span> <span>TABLE</span> user_log <span>(</span>
    user_id <span>VARCHAR</span><span>,</span>
    item_id <span>VARCHAR</span><span>,</span>
    category_id <span>VARCHAR</span><span>,</span>
    behavior <span>VARCHAR</span><span>,</span>
    ts <span>TIMESTAMP</span><span>(</span><span>3</span><span>)</span>
 <span>)</span> <span>WITH</span> <span>(</span>
<span>'connector.type'</span> <span>=</span> <span>'kafka'</span><span>,</span> <span>-- 使用 kafka connector</span>
<span>'connector.version'</span> <span>=</span> <span>'universal'</span><span>,</span>  <span>-- kafka 版本，universal 支持 0.11 以上的版本</span>
<span>'connector.topic'</span> <span>=</span> <span>'user_behavior'</span><span>,</span>  <span>-- kafka topic</span>
<span>'connector.properties.bootstrap.servers'</span><span>=</span><span>'kafka-1:9092,kafka-2:9092,kafka-3:9092'</span><span>,</span>
<span>'connector.startup-mode'</span> <span>=</span> <span>'earliest-offset'</span><span>,</span> <span>-- 从起始 offset 开始读取</span>
<span>'update-mode'</span> <span>=</span> <span>'append'</span><span>,</span>
<span>'format.type'</span> <span>=</span> <span>'json'</span><span>,</span>  <span>-- 数据源格式为 json</span>
<span>'format.derive-schema'</span> <span>=</span> <span>'true'</span> <span>-- 从 DDL schema 确定 json 解析规则</span>
 <span>)</span><span>;</span>

<span>CREATE</span> <span>TABLE</span> pvuv_sink <span>(</span>
    dt <span>VARCHAR</span><span>,</span>
    pv <span>BIGINT</span><span>,</span>
    uv <span>BIGINT</span>
 <span>)</span> <span>WITH</span> <span>(</span>
<span>'connector.type'</span> <span>=</span> <span>'jdbc'</span><span>,</span> <span>-- 使用 jdbc connector</span>
<span>'connector.url'</span> <span>=</span> <span>'jdbc:mysql://test-mysql:3306/test'</span><span>,</span> <span>-- jdbc url</span>
<span>'connector.table'</span> <span>=</span> <span>'pvuv_sink'</span><span>,</span> <span>-- 表名</span>
<span>'connector.username'</span> <span>=</span> <span>'root'</span><span>,</span> <span>-- 用户名</span>
<span>'connector.password'</span> <span>=</span> <span>'123456'</span><span>,</span> <span>-- 密码</span>
<span>'connector.write.flush.max-rows'</span> <span>=</span> <span>'1'</span> <span>-- 默认 5000 条，为了演示改为 1 条</span>
 <span>)</span><span>;</span>

<span>INSERT</span> <span>INTO</span> pvuv_sink
<span>SELECT</span>
  DATE_FORMAT<span>(</span>ts<span>,</span> <span>'yyyy-MM-dd HH:00'</span><span>)</span> dt<span>,</span>
  <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>AS</span> pv<span>,</span>
  <span>COUNT</span><span>(</span><span>DISTINCT</span> user_id<span>)</span> <span>AS</span> uv
<span>FROM</span> user_log
<span>GROUP</span> <span>BY</span> DATE_FORMAT<span>(</span>ts<span>,</span> <span>'yyyy-MM-dd HH:00'</span><span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br></div></div><ul>
<li>使用到 maven 依赖如下</li>
</ul>
<div><pre><code><span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>mysql<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>mysql-connector-java<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>5.1.48<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>

<span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-sql-connector-kafka_2.12<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>

<span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-connector-jdbc_2.11<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>

<span><span><span>&lt;</span>dependency</span><span>></span></span>
    <span><span><span>&lt;</span>groupId</span><span>></span></span>org.apache.flink<span><span><span>&lt;/</span>groupId</span><span>></span></span>
    <span><span><span>&lt;</span>artifactId</span><span>></span></span>flink-json<span><span><span>&lt;/</span>artifactId</span><span>></span></span>
    <span><span><span>&lt;</span>version</span><span>></span></span>1.12.0<span><span><span>&lt;/</span>version</span><span>></span></span>
<span><span><span>&lt;/</span>dependency</span><span>></span></span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br></div></div><ul>
<li>Kafka 模拟发送的数据如下</li>
</ul>
<div><pre><code><span>{</span><span>"user_id"</span><span>:</span> <span>"543462"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"1715"</span><span>,</span> <span>"category_id"</span><span>:</span> <span>"1464116"</span><span>,</span> <span>"behavior"</span><span>:</span> <span>"pv"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
<span>{</span><span>"user_id"</span><span>:</span> <span>"662867"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"2244074"</span><span>,</span><span>"category_id"</span><span>:</span><span>"1575622"</span><span>,</span><span>"behavior"</span><span>:</span> <span>"pv"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
<span>{</span><span>"user_id"</span><span>:</span> <span>"662867"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"2244074"</span><span>,</span><span>"category_id"</span><span>:</span><span>"1575622"</span><span>,</span><span>"behavior"</span><span>:</span> <span>"pv"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
<span>{</span><span>"user_id"</span><span>:</span> <span>"662867"</span><span>,</span> <span>"item_id"</span><span>:</span><span>"2244074"</span><span>,</span><span>"category_id"</span><span>:</span><span>"1575622"</span><span>,</span><span>"behavior"</span><span>:</span> <span>"learning flink"</span><span>,</span> <span>"ts"</span><span>:</span><span>"2021-02-01T01:00:00Z"</span><span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></div></div><h3 id="任务启动流程"> 任务启动流程</h3>
<p>任务启动流程图如下</p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_start.png"/><br>
<strong>streamx-console 提交任务流程</strong>
</center>
<p>关于项目的概念，<code>Development Mode</code>,<code>savepoint</code>,<code>NoteBook</code>,自定义 jar 管理，任务发布，任务恢复，参数配置，参数对比，多版本管理等等更多使用教程和文档后续持续更新。..</p>
]]></content:encoded>
    </item>
    <item>
      <title>Flink K8s 集成支持</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/flink-k8s/1-deployment/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/flink-k8s/1-deployment/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Flink K8s 集成支持</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>StreamX Flink Kubernetes 基于 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/resource-providers/native_kubernetes/" target="_blank" rel="noopener noreferrer">Flink Native Kubernetes</a> 实现，支持以下 Flink 运行模式：</p>
<ul>
<li>Native-Kubernetes Application</li>
<li>Native-Kubernetes Session</li>
</ul>
<p>单个 StreamX 实例当前只支持单个 Kubernetes 集群，如果您有多 Kubernetes 支持的诉求，欢迎提交相关的 <a href="https://github.com/streamxhub/streamx/issues" target="_blank" rel="noopener noreferrer">Fearure Request Issue</a> : )</p>
<br/>
<h2 id="额外环境要求"> 额外环境要求</h2>
<p>StreamX Flink-K8s 需要具备以下额外的运行环境：</p>
<ul>
<li>Kubernetes</li>
<li>Maven（StreamX 运行节点具备）</li>
<li>Docker（StreamX 运行节点是具备）</li>
</ul>
<p>StreamX 实例并不需要强制部署在 Kubernetes 所在节点上，可以部署在 Kubernetes 集群外部节点，但是需要该 StreamX 部署节点与 Kubernetes 集群<strong>保持网络通信畅通</strong>。</p>
<br/>
<h2 id="集成准备"> 集成准备</h2>
<h3 id="kubernetes-连接配置"> Kubernetes 连接配置</h3>
<p>StreamX 直接使用系统 <code>～/.kube/config</code> 作为 Kubernetes 集群的连接凭证，最为简单的方式是直接拷贝 Kubernetes 节点的 <code>.kube/config</code> 到 StreamX 节点用户目录，各云服务商 Kubernetes 服务也都提供了相关配置的快速下载。当然为了权限约束，也可以自行生成对应 k8s 自定义账户的 config。</p>
<p>完成后，可以通过 StreamX 所在机器的 kubectl 快速检查目标 Kubernetes 集群的连通性：</p>
<div><pre><code>kubectl cluster-info
</code></pre>
<div><span>1</span><br></div></div><h3 id="kubernetes-rbac-配置"> Kubernetes RBAC 配置</h3>
<p>同样的，需要准备 Flink 所使用 K8s Namespace 的 RBAC 资源，请参考 Flink-Docs：https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/resource-providers/native_kubernetes/#rbac</p>
<p>假设使用 Flink Namespace 为 <code>flink-dev</code>，不明确指定 K8s 账户，可以如下创建简单 clusterrolebinding 资源：</p>
<div><pre><code>kubectl create clusterrolebinding flink-role-binding-default --clusterrole=edit --serviceaccount=flink-dev:default
</code></pre>
<div><span>1</span><br></div></div><h3 id="docker-远程容器服务配置"> Docker 远程容器服务配置</h3>
<p>在 StreamX Setting 页面，配置目标 Kubernetes 集群所使用的 Docker 容器服务的连接信息。</p>
<p><img src="/streamx-docs/assets/img/doc-img/docker_register_setting.png" alt="docker register setting"></p>
<p>在远程 Docker 容器服务创建一个名为 <code>streamx</code> 的 Namespace ，为 StreamX 自动构建的 Flink image 推送空间，请确保使用的 Docker Register User 具有该  Namespace 的 <code>pull</code>/<code>push</code> 权限。</p>
<p>可以在 StreamX 所在节点通过 docker command 简单测试权限：</p>
<div><pre><code><span># verify access</span>
docker login --username<span>=</span><span>&lt;</span>your_username<span>></span> <span>&lt;</span>your_register_addr<span>></span>
<span># verify push permission</span>
docker pull busybox
docker tag busybox <span>&lt;</span>your_register_addr<span>></span>/streamx/busybox
docker push <span>&lt;</span>your_register_addr<span>></span>/streamx/busybox
<span># verify pull permission</span>
docker pull <span>&lt;</span>your_register_addr<span>></span>/streamx/busybox
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br></div></div><br/>
<h2 id="任务提交"> 任务提交</h2>
<h3 id="application-任务发布"> Application 任务发布</h3>
<p><img src="/streamx-docs/assets/img/doc-img/k8s_application_submit.png" alt="k8s application submit"></p>
<p>其中需要说明的参数如下：</p>
<ul>
<li><strong>Flink Base Docker Image</strong>： 基础 Flink Docker 镜像的 Tag，可以直接从 <a href="https://hub.docker.com/_/flink" target="_blank" rel="noopener noreferrer">DockerHub - offical/flink</a> 获取，也支持用户私有的底层镜像，此时在 setting 设置 Docker Register Account 需要具备该私有镜像 	<code>pull</code> 权限。</li>
<li><strong>Rest-Service Exposed Type</strong>：对应 Flink 原生 <a href="https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/config/#kubernetes" target="_blank" rel="noopener noreferrer">kubernetes.rest-service.exposed.type</a> 配置，各个候选值说明：
<ul>
<li><code>ClusterIP</code>：需要 StreamX 可直接访问 K8s 内部网络；</li>
<li><code>LoadBalancer</code>：需要 K8s 提前创建 LoadBalancer 资源，且 Flink Namespace 具备自动绑定权限，同时 StreamX 可以访问该 LoadBalancer 网关；</li>
<li><code>NodePort</code>：需要 StreamX 可以直接连通所有 K8s 节点；</li>
</ul>
</li>
<li><strong>Kubernetes Pod Template</strong>： Flink 自定义 pod-template 配置。</li>
</ul>
<p>任务启动后，支持在该任务的 Detail 页直接访问对应的 Flink Web UI 页面：</p>
<p><img src="/streamx-docs/assets/img/doc-img/k8s_app_detail.png" alt="k8s app detail"></p>
<h3 id="session-任务发布"> Session 任务发布</h3>
<p>Flink-Native-Kubernetes Session 任务 K8s 额外的配置（pod-template 等）完全由提前部署的 Flink-Session 集群决定，请直接参考 Flink-Doc：https://ci.apache.org/projects/flink/flink-docs-stable/docs/deployment/resource-providers/native_kubernetes</p>
<br/>
<h2 id="相关参数配置"> 相关参数配置</h2>
<p>StreamX 在 <code>applicaton.yml</code>  Flink-K8s 相关参数如下，默认情况下不需要额外调整默认值。</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>描述</th>
<th>默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td>streamx.docker.register.image-namespace</td>
<td>远程 docker 容器服务仓库命名空间，构建的 flink-job 镜像会推送到该命名空间。</td>
<td>steramx</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-task-timeout-sec.job-status</td>
<td>每组 flink 状态追踪任务的运行超时秒数</td>
<td>120</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-task-timeout-sec.cluster-metric</td>
<td>每组 flink 指标追踪任务的运行超时秒数</td>
<td>120</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-interval-sec.job-status</td>
<td>flink 状态追踪任务运行间隔秒数，为了维持准确性，请设置在 5s 以下，最佳设置在 2-3s</td>
<td>5</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.polling-interval-sec.cluster-metric</td>
<td>flink 指标追踪任务运行间隔秒数</td>
<td>10</td>
</tr>
<tr>
<td>streamx.flink-k8s.tracking.silent-state-keep-sec</td>
<td>silent 追踪容错时间秒数</td>
<td>60</td>
</tr>
</tbody>
</table>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/docker_register_setting.png" type="image/png"/>
    </item>
    <item>
      <title>K8s PVC 资源使用</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/flink-k8s/2-k8s-pvc-integration/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/flink-k8s/2-k8s-pvc-integration/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">K8s PVC 资源使用</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="k8s-pvc-资源使用说明"> K8s PVC 资源使用说明</h2>
<p>当前版本 StreamX Flink-K8s 任务对 PVC 资源（挂载 checkpoint/savepoint/logs 等文件资源）的支持基于 pod-template。</p>
<p>Native-Kubernetes Session 由创建 Session Cluster 时控制，这里不再赘述。Native-Kubernetes Application 支持在 StreamX 页面上直接编写 <code>pod-template</code>，<code>jm-pod-template</code>，<code>tm-pod-template</code> 配置。</p>
<br/>
<p>以下是一个简要的示例，假设已经提前创建 <code>flink-checkpoint</code>， <code>flink-savepoint</code> 两个 PVC ：</p>
<p><img src="/streamx-docs/assets/img/doc-img/k8s_pvc.png" alt="k8s pvc"></p>
<p>pod-template 配置文本如下：</p>
<div><pre><code><span>apiVersion</span><span>:</span> v1
<span>kind</span><span>:</span> Pod
<span>metadata</span><span>:</span>
  <span>name</span><span>:</span> pod<span>-</span>template
<span>spec</span><span>:</span>
  <span>containers</span><span>:</span>
    <span>-</span> <span>name</span><span>:</span> flink<span>-</span>main<span>-</span>container
      <span>volumeMounts</span><span>:</span>
        <span>-</span> <span>name</span><span>:</span> checkpoint<span>-</span>pvc
          <span>mountPath</span><span>:</span> /opt/flink/checkpoints
        <span>-</span> <span>name</span><span>:</span> savepoint<span>-</span>pvc
          <span>mountPath</span><span>:</span> /opt/flink/savepoints
  <span>volumes</span><span>:</span>
    <span>-</span> <span>name</span><span>:</span> checkpoint<span>-</span>pvc
      <span>persistentVolumeClaim</span><span>:</span>
        <span>claimName</span><span>:</span> flink<span>-</span>checkpoint
    <span>-</span> <span>name</span><span>:</span> savepoint<span>-</span>pvc
      <span>persistentVolumeClaim</span><span>:</span>
        <span>claimName</span><span>:</span> flink<span>-</span>savepoint
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br></div></div><p>由于使用了 <code>rockdb-backend</code>，该依赖可以由 3 种方式提供：</p>
<ol>
<li>
<p>提供的 Flink Base Docker Image 已经包含该依赖（用户自行解决依赖冲突）；</p>
</li>
<li>
<p>在 StreamX 本地 <code>Workspace/jars</code> 目录下放置 <code>flink-statebackend-rocksdb_xx.jar</code> 依赖；</p>
</li>
<li>
<p>在 StreamX Dependency 配置中加入 rockdb-backend 依赖（此时 StreamX 会自动解决依赖冲突）：</p>
<p><img src="/streamx-docs/assets/img/doc-img/rocksdb_dependency.png" alt="rocksdb dependency"></p>
</li>
</ol>
<br/>
<p>在随后版本中，我们会提供一种优雅的 pod-template 配置自动生成的方式，来简化 k8s-pvc 挂载这一过程 : )</p>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/k8s_pvc.png" type="image/png"/>
    </item>
    <item>
      <title>Hadoop 资源集成</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/flink-k8s/3-hadoop-resource-integration/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/flink-k8s/3-hadoop-resource-integration/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Hadoop 资源集成</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h2 id="在-k8s-上使用-hadoop-资源"> 在 K8s 上使用 Hadoop 资源</h2>
<p>在 StreamX Flink-K8s runtime 下使用 Hadoop 资源，如 checkpoint 挂载 HDFS、读写 Hive 等，目前用户需要自行构建相关 Flink Base   Docker Image，Image 中需要包含以下内容：</p>
<ul>
<li>包含 Hadoop Lib， 并设置 <code>HADOOP_CLASSPATH</code> 到该目录；</li>
<li>包含 Hadoop Config，并设置 <code>HADOOP_CONF_DIR</code> 到该目录；</li>
<li>如果使用 Hive， 需要包含 Hive Config；</li>
</ul>
<br/>
<p>这其实挺不优雅的 🥲，我们将在随后的版本里支持<strong>自动集成 Hadoop</strong> 的功能支持， Plz look forward to !</p>
]]></content:encoded>
    </item>
    <item>
      <title>Flink SQL</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/flinksql/flinksql/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/flinksql/flinksql/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">Flink SQL</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
    </item>
    <item>
      <title>框架介绍</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/guide/intro/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/guide/intro/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">框架介绍</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h1 id="streamx"> StreamX</h1>
<p>let flink|spark easy</p>
<blockquote>
<p>一个神奇的框架,让Flink开发更简单</p>
</blockquote>
<h2 id="🚀-什么是streamx"> 🚀 什么是StreamX</h2>
<p>    大数据技术如今发展的如火如荼,已经呈现百花齐放欣欣向荣的景象,实时处理流域 <code>Apache Spark</code> 和 <code>Apache Flink</code> 更是一个伟大的进步,尤其是<code>Apache Flink</code>被普遍认为是下一代大数据流计算引擎,
我们在使用 <code>Flink</code> 时发现从编程模型, 启动配置到运维管理都有很多可以抽象共用的地方, 我们将一些好的经验固化下来并结合业内的最佳实践, 通过不断努力终于诞生了今天的框架 —— <code>StreamX</code>, 项目的初衷是 —— 让 <code>Flink</code> 开发更简单,
使用<code>StreamX</code>开发,可以极大降低学习成本和开发门槛, 让开发者只用关心最核心的业务,<code>StreamX</code> 规范了项目的配置,鼓励函数式编程,定义了最佳的编程方式,提供了一系列开箱即用的<code>Connectors</code>,标准化了配置、开发、测试、部署、监控、运维的整个过程, 提供<code>scala</code>和<code>java</code>两套api,
其最终目的是打造一个一站式大数据平台,流批一体,湖仓一体的解决方案</p>
<p><video src="/streamx-docs/assets/video/streamx-video.mp4" controls="controls" autoplay="autoplay" width="100%" height="100%"></video></p>
<h2 id="🎉-features"> 🎉 Features</h2>
<ul>
<li>开发脚手架</li>
<li>多版本Flink支持(1.11,x, 1.12.x, 1.13 )</li>
<li>一系列开箱即用的connectors</li>
<li>支持项目编译功能(maven 编译)</li>
<li>在线参数配置</li>
<li>支持<code>Applicaion</code> 模式, <code>Yarn-Per-Job</code>模式启动</li>
<li>快捷的日常操作(任务<code>启动</code>、<code>停止</code>、<code>savepoint</code>,从<code>savepoint</code>恢复)</li>
<li>支持火焰图</li>
<li>支持<code>notebook</code>(在线任务开发)</li>
<li>项目配置和依赖版本化管理</li>
<li>支持任务备份、回滚(配置回滚)</li>
<li>在线管理依赖(maven pom)和自定义jar</li>
<li>自定义udf、连接器等支持</li>
<li>Flink SQL WebIDE</li>
<li>支持catalog、hive</li>
<li>任务运行失败发送告警邮件</li>
<li>支持失败重启重试</li>
<li>从任务<code>开发</code>阶段到<code>部署管理</code>全链路支持</li>
<li>...</li>
</ul>
<h2 id="🏳‍🌈-组成部分"> 🏳‍🌈 组成部分</h2>
<p><code>Streamx</code>有三部分组成,分别是<code>streamx-core</code>,<code>streamx-pump</code> 和 <code>streamx-console</code></p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_archite.png"/><br>
</center>
<h3 id="_1️⃣-streamx-core"> 1️⃣ streamx-core</h3>
<p><code>streamx-core</code> 定位是一个开发时框架,关注编码开发,规范了配置文件,按照约定优于配置的方式进行开发,提供了一个开发时 <code>RunTime Content</code>和一系列开箱即用的<code>Connector</code>,扩展了<code>DataStream</code>相关的方法,融合了<code>DataStream</code>和<code>Flink sql</code> api,简化繁琐的操作,聚焦业务本身,提高开发效率和开发体验</p>
<h3 id="_2️⃣-streamx-pump"> 2️⃣ streamx-pump</h3>
<p><code>pump</code> 是抽水机,水泵的意思,<code>streamx-pump</code>的定位是一个数据抽取的组件,类似于<code>flinkx</code>,基于<code>streamx-core</code>中提供的各种<code>connector</code>开发,目的是打造一个方便快捷,开箱即用的大数据实时数据抽取和迁移组件,并且集成到<code>streamx-console</code>中,解决实时数据源获取问题,目前在规划中</p>
<h3 id="_3️⃣-streamx-console"> 3️⃣ streamx-console</h3>
<p><code>streamx-console</code> 是一个综合实时数据平台,低代码(<code>Low Code</code>)平台,可以较好的管理<code>Flink</code>任务,集成了项目编译、发布、参数配置、启动、<code>savepoint</code>,火焰图(<code>flame graph</code>),<code>Flink SQL</code>,
监控等诸多功能于一体,大大简化了<code>Flink</code>任务的日常操作和维护,融合了诸多最佳实践。旧时王谢堂前燕,飞入寻常百姓家,让大公司有能力研发使用的项目,现在人人可以使用,
其最终目标是打造成一个实时数仓,流批一体的一站式大数据解决方案,该平台使用但不仅限以下技术:</p>
<ul>
<li><a href="http://flink.apache.org" target="_blank" rel="noopener noreferrer">Apache Flink</a></li>
<li><a href="http://hadoop.apache.org" target="_blank" rel="noopener noreferrer">Apache YARN</a></li>
<li><a href="https://spring.io/projects/spring-boot/" target="_blank" rel="noopener noreferrer">Spring Boot</a></li>
<li><a href="http://www.mybatis.org" target="_blank" rel="noopener noreferrer">Mybatis</a></li>
<li><a href="http://mp.baomidou.com" target="_blank" rel="noopener noreferrer">Mybatis-Plus</a></li>
<li><a href="http://www.brendangregg.com/FlameGraphs" target="_blank" rel="noopener noreferrer">Flame Graph</a></li>
<li><a href="https://github.com/uber-common/jvm-profiler" target="_blank" rel="noopener noreferrer">JVM-Profiler</a></li>
<li><a href="https://cn.vuejs.org/" target="_blank" rel="noopener noreferrer">Vue</a></li>
<li><a href="https://vuepress.vuejs.org/" target="_blank" rel="noopener noreferrer">VuePress</a></li>
<li><a href="https://antdv.com/" target="_blank" rel="noopener noreferrer">Ant Design of Vue</a></li>
<li><a href="https://pro.antdv" target="_blank" rel="noopener noreferrer">ANTD PRO VUE</a></li>
<li><a href="https://xtermjs.org/" target="_blank" rel="noopener noreferrer">xterm.js</a></li>
<li><a href="https://microsoft.github.io/monaco-editor/" target="_blank" rel="noopener noreferrer">Monaco Editor</a></li>
<li>...</li>
</ul>
<p>感谢以上优秀的开源项目和很多未提到的优秀开源项目,给予最大的respect,特别感谢<a href="http://zeppelin.apache.org" target="_blank" rel="noopener noreferrer">Apache Zeppelin</a>,<a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener noreferrer">IntelliJ IDEA</a>,
感谢<a href="https://github.com/GuoNingNing/fire-spark" target="_blank" rel="noopener noreferrer">fire-spark</a>项目,早期给予的灵感和帮助, 感谢我老婆在项目开发时给予的支持,悉心照顾我的生活和日常,给予我足够的时间开发这个项目</p>
<h2 id="👻-为什么不是-❓"> 👻 为什么不是...❓</h2>
<h3 id="apache-zeppelin"> Apache Zeppelin</h3>
<p><a href="http://zeppelin.apache.org" target="_blank" rel="noopener noreferrer">Apache Zeppelin</a>是一个非常优秀的开源项目👏 对<code>Flink</code>做了很好的支持,<code>Zeppelin</code>创新型的<code>notebook</code>功能,让开发者非常方便的<code>On-line</code>编程,快捷的提交任务,语言层面同时支持<code>java</code>,<code>scala</code>,<code>python</code>,国内阿里的章剑峰大佬也在积极推动该项目,向剑峰大佬致以崇高的敬意🙏🙏🙏,
但该项目目前貌似没有解决项目的管理和运维方面的痛点,针对比较复杂的项目和大量的作业管理就有些力不从心了,一般来讲不论是<code>DataStream</code>作业还是<code>Flink SQL</code>作业,大概都会经历作业的<code>开发阶段</code>,<code>测试阶段</code>,<code>打包阶段</code>,<code>上传服务器阶段</code>,<code>启动任务阶段</code>等这些步骤,这是一个链路很长的步骤,且整个过程耗时比较长,体验不好,
即使修改了一个符号,项目改完上线都得走上面的流程,我们期望这些步骤能够动动鼠标一键式解决,还希望至少能有一个任务列表的功能,能够方便的管理任务,可以清楚的看到哪些任务正在运行,哪些停止了,任务的资源消耗情况,可以在任务列表页面一键<code>启动</code>或<code>停止</code>任务,并且自动管理<code>savePoint</code>,这些问题也是开发者实际开发中会遇到了问题,
<code>streamx-console</code>很好的解决了这些痛点,定位是一个一站式实时数据平台,并且开发了更多令人激动的功能(诸如<code>Flink SQL WebIDE</code>,<code>依赖隔离</code>,<code>任务回滚</code>,<code>火焰图</code>等)</p>
<h3 id="flinkx"> FlinkX</h3>
<p><a href="http://github.com/DTStack/flinkx" target="_blank" rel="noopener noreferrer">FlinkX</a> 是基于flink的分布式数据同步工具,实现了多种异构数据源之间高效的数据迁移,定位比较明确,专门用来做数据抽取和迁移,可以作为一个服务组件来使用,<code>StreamX</code>关注开发阶段和任务后期的管理,定位有所不同,<code>streamx-pump</code>模块也在规划中,
致力于解决数据源抽取和迁移,最终会集成到<code>streamx-console</code>中</p>
]]></content:encoded>
    </item>
    <item>
      <title>快速开始</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/guide/quickstart/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/guide/quickstart/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">快速开始</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>千里之行始于足下,从这一刻开始即将进入<code>StreamX</code> 开发的奇妙旅程,准备好了吗? 让我们开始吧</p>
<h2 id="起步"> 起步</h2>
<p>我们要做的第一件事就是将项目clone到本地,执行编译,在编译前请确保以下事项</p>
<div>
<ul>
<li>确保本机安装的JDK<code>1.8</code>及以上的版本</li>
<li>确保本机已经安装了maven</li>
</ul>
</div>
<p>如果准备就绪,就可以clone项目并且执行编译了</p>
<div><pre><code><span>git</span> clone https://github.com/streamxhub/streamx.git
<span>cd</span> Streamx
mvn clean <span>install</span> -DskipTests
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>顺利的话就会看到编译成功.</p>
<div><pre><code>[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Streamx 1.0.0:
[INFO]
[INFO] Streamx ............................................ SUCCESS [  1.882 s]
[INFO] Streamx : Common ................................... SUCCESS [ 15.700 s]
[INFO] Streamx : Flink Parent ............................. SUCCESS [  0.032 s]
[INFO] Streamx : Flink Common ............................. SUCCESS [  8.243 s]
[INFO] Streamx : Flink Core ............................... SUCCESS [ 17.332 s]
[INFO] Streamx : Flink Test ............................... SUCCESS [ 42.742 s]
[INFO] Streamx : Spark Parent ............................. SUCCESS [  0.018 s]
[INFO] Streamx : Spark Core ............................... SUCCESS [ 12.028 s]
[INFO] Streamx : Spark Test ............................... SUCCESS [  5.828 s]
[INFO] Streamx : Spark Cli ................................ SUCCESS [  0.016 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  01:43 min
[INFO] Finished at: 2021-03-15T17:02:22+08:00
[INFO] ------------------------------------------------------------------------
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br></div></div><h2 id="datastream"> DataStream</h2>
<p>从这里开始用StreamX开发第一个<code>DataStream</code> 程序,从最简单的需求入手,这里的需求是从<code>kafka</code>读取用户数据写入<code>mysql</code>,需求如下:</p>
<div>
<ul>
<li>从kafka里读取用户的数据写入到mysql中</li>
<li>只要年龄小于30岁的用户数据</li>
<li><code>kafka</code> 的 <code>topic</code>为<code>test_user</code></li>
<li>要写入的<code>mysql</code>的表为<code>t_user</code></li>
</ul>
</div>
<p><code>kafka</code>的<code>topic</code> 的数据格式如下</p>
<div><pre><code><span>{</span>
  <span>"name"</span> <span>:</span> <span>"$name"</span><span>,</span>
  <span>"age"</span> <span>:</span> $age<span>,</span>
  <span>"gender"</span> <span>:</span> $gender<span>,</span>
  <span>"address"</span> <span>:</span> <span>"$address"</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><p><code>mysql</code>表<code>t_user</code>结构如下:</p>
<div><pre><code><span>create</span> <span>table</span> <span>user</span><span>(</span>
    <span>`</span>name<span>`</span> <span>varchar</span><span>(</span><span>32</span><span>)</span><span>,</span>
    <span>`</span>age<span>`</span> <span>int</span><span>(</span><span>3</span><span>)</span><span>,</span>
    <span>`</span>gender<span>`</span> <span>int</span><span>(</span><span>1</span><span>)</span><span>,</span>
    <span>`</span>address<span>`</span> <span>varchar</span><span>(</span><span>255</span><span>)</span>
<span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><h3 id="编码开发"> 编码开发</h3>
<p>用Streamx开发一个这样的需求非常简单,只需要几行代码就搞定了</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>quickstart</span>

<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>common<span>.</span>util<span>.</span></span>JsonUtils
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>sink<span>.</span></span>JdbcSink
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span>KafkaSource
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_
<span>/**
 * @author benjobs
 */</span>
<span>object</span> QuickStartApp <span>extends</span> FlinkStreaming <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>val</span> source <span>=</span> KafkaSource<span>(</span><span>)</span>
      <span>.</span>getDataStream<span>[</span><span>String</span><span>]</span><span>(</span><span>)</span>
      <span>.</span>map<span>(</span>x <span>=></span> JsonUtils<span>.</span>read<span>[</span>User<span>]</span><span>(</span>x<span>.</span>value<span>)</span><span>)</span>
      <span>.</span>filter<span>(</span>_<span>.</span>age <span>&lt;</span> <span>30</span><span>)</span>

    JdbcSink<span>(</span><span>)</span><span>.</span>sink<span>[</span>User<span>]</span><span>(</span>source<span>)</span><span>(</span>user <span>=></span>
    s<span>"""
        |insert into t_user(`name`,`age`,`gender`,`address`)
        |value('${user.name}',${user.age},${user.gender},'${user.address}')
        |"""</span><span>.</span>stripMargin
    <span>)</span>
  <span>}</span>
<span>}</span>

<span>case</span> <span>class</span> User<span>(</span>name<span>:</span> <span>String</span><span>,</span> age<span>:</span> <span>Int</span><span>,</span> gender<span>:</span> <span>Int</span><span>,</span> address<span>:</span> <span>String</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>fasterxml<span>.</span>jackson<span>.</span>databind<span>.</span></span><span>ObjectMapper</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>SQLFromFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>function<span>.</span></span><span>StreamEnvConfigFunction</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>sink<span>.</span></span><span>JdbcSink</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>java<span>.</span>source<span>.</span></span><span>KafkaSource</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamingContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>source<span>.</span></span><span>KafkaRecord</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamEnvConfig</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>FilterFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>common<span>.</span>functions<span>.</span></span><span>MapFunction</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>streaming<span>.</span>api<span>.</span>datastream<span>.</span></span><span>DataStream</span><span>;</span>

<span>import</span> <span>java<span>.</span>io<span>.</span></span><span>Serializable</span><span>;</span>

<span>/**
 * @author benjobs
 */</span>
<span>public</span> <span>class</span> <span>QuickStartJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>

        <span>StreamEnvConfig</span> envConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>

        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>envConfig<span>)</span><span>;</span>

        <span>ObjectMapper</span> mapper <span>=</span> <span>new</span> <span>ObjectMapper</span><span>(</span><span>)</span><span>;</span>

        <span>DataStream</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span> source <span>=</span> <span>new</span> <span>KafkaSource</span><span><span>&lt;</span><span>String</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>getDataStream</span><span>(</span><span>)</span>
                <span>.</span><span>map</span><span>(</span><span>(</span><span>MapFunction</span><span><span>&lt;</span><span>KafkaRecord</span><span>&lt;</span><span>String</span><span>></span><span>,</span> <span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span>
                        mapper<span>.</span><span>readValue</span><span>(</span>value<span>.</span><span>value</span><span>(</span><span>)</span><span>,</span> <span>JavaUser</span><span>.</span><span>class</span><span>)</span><span>)</span>
                <span>.</span><span>filter</span><span>(</span><span>(</span><span>FilterFunction</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> value <span>-></span> value<span>.</span>age <span>&lt;</span> <span>30</span><span>)</span><span>;</span>

        <span>new</span> <span>JdbcSink</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>(</span>context<span>)</span>
                <span>.</span><span>sql</span><span>(</span><span>(</span><span>SQLFromFunction</span><span><span>&lt;</span><span>JavaUser</span><span>></span></span><span>)</span> <span>JavaUser</span><span>::</span><span>toSql</span><span>)</span>
                <span>.</span><span>towPCSink</span><span>(</span>source<span>)</span><span>;</span>

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>

<span>}</span>

<span>class</span> <span>JavaUser</span> <span>implements</span> <span>Serializable</span> <span>{</span>
    <span>String</span> name<span>;</span>
    <span>Integer</span> age<span>;</span>
    <span>Integer</span> gender<span>;</span>
    <span>String</span> address<span>;</span>

    <span>public</span> <span>String</span> <span>toSql</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>format</span><span>(</span>
                <span>"insert into t_user(`name`,`age`,`gender`,`address`) value('%s',%d,%d,'%s')"</span><span>,</span>
                name<span>,</span>
                age<span>,</span>
                gender<span>,</span>
                address<span>)</span><span>;</span>
    <span>}</span>

<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>提示</p>
<p>项目工程和代码已经提供好了,位于 <a href="https://github.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">streamx-quickstart</a> 开箱即用,不需要写一行代码</p>
</div>
<h3 id="配置文件"> 配置文件</h3>
<p><code>kafka</code> 和 <code>mysql</code>相关准备工作就绪了,接下来要改项目的配置,找到项目路径下的<code>assembly/conf/application.yml</code>
这里有很多配置项,先不用管,也不用改,现在只关注 <code>kafka</code> 和 <code>mysql</code> 两项配置</p>
<div><pre><code><span># kafka source</span>
<span>kafka.source</span><span>:</span>
  <span>bootstrap.servers</span><span>:</span> kfk1<span>:</span><span>9092</span><span>,</span>kfk2<span>:</span><span>9092</span><span>,</span>kfk3<span>:</span><span>9092</span>
  <span>topic</span><span>:</span> test_user
  <span>group.id</span><span>:</span> user_01
  <span>auto.offset.reset</span><span>:</span> earliest

<span># jdbc</span>
<span>jdbc</span><span>:</span>
  <span>semantic</span><span>:</span> EXACTLY_ONCE
  <span>driverClassName</span><span>:</span> com.mysql.cj.jdbc.Driver
  <span>jdbcUrl</span><span>:</span> jdbc<span>:</span>mysql<span>:</span>//localhost<span>:</span>3306/test<span>?</span>useSSL=false<span>&amp;allowPublicKeyRetrieval=true</span>
  <span>username</span><span>:</span> root
  <span>password</span><span>:</span> <span>123456</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br></div></div><p>找到上面两项配置修改相关的 <code>kafka</code> 和 <code>mysql</code> 信息即可.</p>
<div><p>提示</p>
<p><code>配置</code>是非常重要的概念,会在下一章节进行介绍说明</p>
</div>
<h3 id="本地测试"> 本地测试</h3>
<p>准备工作完毕现在启动项目进行本地测试,找到项目主类<code>com.streamxhub.streamx.quickstart.QuickStartApp</code> 运行启动,并且加上参数
<code>--conf $path</code>,path即为当前项目的配置文件的路径(上面修改的配置文件),如果没啥意外的话,项目就会在本地启动成功,会看到<code>kafka</code>里的数据按照需求成功的写入到<code>mysql</code>中</p>
<div><p>注意事项</p>
<p>项目启动时配置文件必须要指定,如不指定该参数会报错如下:</p>
<div><pre><code>Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError: 
[Streamx] Usage:can&#39;t fond config,please set &quot;--conf $path &quot; in main arguments
	at com.streamxhub.streamx.flink.core.scala.util.FlinkStreamingInitializer.initParameter(FlinkStreamingInitializer.scala:126)
	at com.streamxhub.streamx.flink.core.scala.util.FlinkStreamingInitializer.&lt;init&gt;(FlinkStreamingInitializer.scala:85)
	at com.streamxhub.streamx.flink.core.scala.util.FlinkStreamingInitializer$.initStream(FlinkStreamingInitializer.scala:55)
	at com.streamxhub.streamx.flink.core.scala.FlinkStreaming$class.main(FlinkStreaming.scala:94)
	at com.streamxhub.streamx.quickstart.QuickStartApp$.main(QuickStartApp.scala:9)
	at com.streamxhub.streamx.quickstart.QuickStartApp.main(QuickStartApp.scala)

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></div>
<h3 id="部署上线"> 部署上线</h3>
<p>当程序开发完成并且通过本地测试后,需要往测试或生产环境发布,一般的方式是将jar包上传到集群的某一台部署机,进行<code>flink run</code>命令启动项目,然后跟上各种资源参数,
虽然flink启动的时候官方提供了很多短参数的方式进行设置(-c -m -jar等),但是这种方式,不仅可读性差,还极容易出错,稍不注意一个参数不合法就会导致任务启动失败,
在StreamX里这些都简化了,StreamX启动项目如下</p>
<div>
<ul>
<li>解项目压缩包</li>
<li>执行启动脚本</li>
</ul>
</div>
<h3 id="项目结构"> 项目结构</h3>
<p>解包<code>Streamx-flink-quickstart-1.0.0.tar.gz</code>后项目的目录结构如下:</p>
<div><pre><code>.
Streamx-flink-quickstart-1.0.0
├── bin
│   ├── startup.sh                             //启动脚本  
│   ├── setclasspath.sh                        //java环境变量相关的脚本(内部使用的,用户无需关注)
│   ├── shutdown.sh                            //任务停止脚本(不建议使用)
│   ├── flink.sh                               //启动时内部使用到的脚本(内部使用的,用户无需关注)
├── conf                           
|   ├── application.yaml                       //项目的配置文件
├── lib
│   └── Streamx-flink-quickstart-1.0.0.jar     //项目的jar包
└── temp
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><h3 id="启动命令"> 启动命令</h3>
<p>启动之前要确定<code>conf/application.yaml</code>下的配置文件,在当前<code>Streamx-flink-quickstart</code>项目里都已经配置好了,不需改动,直接启动项目即可:</p>
<div><pre><code>bin/startup.sh --conf conf/application.yaml 
</code></pre>
<div><span>1</span><br></div></div><h2 id="flink-sql"> Flink Sql</h2>
<p>从这里开始用StreamX开发第一个Flink Sql 程序,利用Flink内置的 <code>DataGen</code>和<code>print</code> Connectors 来完成</p>
<div><p>提示</p>
<p>项目工程和代码已经提供好了,位于 <a href="https://github.com/streamxhub/streamx-quickstart.git" target="_blank" rel="noopener noreferrer">streamx-quickstart</a> 开箱即用,不需要写一行代码</p>
</div>
<h3 id="编码开发-2"> 编码开发</h3>
<p>下面的代码演示了如何用<code>scala</code>和<code>java</code>两种api开发这样一个需求,全部代码如下</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>test<span>.</span>tablesql</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreamTable

<span>object</span> HelloFlinkSQL <span>extends</span> FlinkStreamTable <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>/**
     * 一行胜千言
     */</span>
    context<span>.</span>sql<span>(</span><span>"myflinksql"</span><span>)</span>
  <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java" active>
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>TableContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>TableEnvConfig</span><span>;</span>

<span>public</span> <span>class</span> <span>HelloFlinkSQL</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>TableEnvConfig</span> tableEnvConfig <span>=</span> <span>new</span> <span>TableEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>tableConfig<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
            <span>System</span><span>.</span>out<span>.</span><span>println</span><span>(</span><span>"set tableConfig..."</span><span>)</span><span>;</span>
        <span>}</span><span>)</span><span>;</span>

        <span>TableContext</span> context <span>=</span> <span>new</span> <span>TableContext</span><span>(</span>tableEnvConfig<span>)</span><span>;</span>
        context<span>.</span><span>sql</span><span>(</span><span>"myflinksql"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br></div></div></CodeGroupItem>
</CodeGroup>
<h3 id="配置文件-2"> 配置文件</h3>
<p>找到项目路径下的assembly/conf/sql.yml,内容如下</p>
<div><pre><code>myflinksql: <span>|</span>
  <span>CREATE</span> <span>TABLE</span> datagen <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING<span>,</span>
    ts <span>AS</span> localtimestamp<span>,</span>
    WATERMARK <span>FOR</span> ts <span>AS</span> ts
  <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'datagen'</span><span>,</span>
    <span>-- optional options --</span>
    <span>'rows-per-second'</span><span>=</span><span>'5'</span><span>,</span>
    <span>'fields.f_sequence.kind'</span><span>=</span><span>'sequence'</span><span>,</span>
    <span>'fields.f_sequence.start'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_sequence.end'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random.min'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_random.max'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random_str.length'</span><span>=</span><span>'10'</span>
  <span>)</span><span>;</span>

  <span>CREATE</span> <span>TABLE</span> print_table <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING
    <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'print'</span>
  <span>)</span><span>;</span>

  <span>INSERT</span> <span>INTO</span> print_table <span>select</span> f_sequence<span>,</span>f_random<span>,</span>f_random_str <span>from</span> datagen<span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div><h3 id="本地测试-2"> 本地测试</h3>
<p>找到项目主类<code>com.streamxhub.streamx.quickstart.HelloFlinkSQL</code> 运行启动,并且加上参数
<code>--sql $path</code>,path即为当前项目的sql文件的路径,如果没啥意外的话,项目就会在本地启动成功,生成的数据会打印到控制台</p>
]]></content:encoded>
    </item>
    <item>
      <title>项目配置</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/model/conf/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/model/conf/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">项目配置</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>配置在<code>StreamX</code>中是非常重要的概念,先说说为什么需要配置</p>
<h2 id="为什么需要配置"> 为什么需要配置</h2>
<p>开发<code>DataStream</code>程序,大体流程都可以抽象为以下4步</p>
<div>
<ul>
<li>StreamExecutionEnvironment初始并配置</li>
<li>Source接入数据</li>
<li>Transformation逻辑处理</li>
<li>Sink结果数据落地</li>
</ul>
</div>
<p><img src="/streamx-docs/assets/img/doc-img/process_steps.png" alt=""></p>
<p>开发<code>DataStream</code>程序都需要定义<code>Environment</code>初始化并且配置环境相关的参数,一般我们都会在第一步初始化<code>Environment</code>并配置各种参数,配置的参数大概有以下几类</p>
<div>
<ul>
<li>Parallelism 默认并行度配置</li>
<li>TimeCharacteristic 时间特征配置</li>
<li>checkpoint 检查点的相关配置</li>
<li>Watermark 相关配置</li>
<li>State Backend 状态后端配置</li>
<li>Restart Strategy 重启策略配置</li>
<li>其他配置...</li>
</ul>
</div>
<p>以上的配置基本都是比较普遍且通用的,是每个程序上来第一步就要定义的,是一项重复的工作</p>
<p>当程序写好后,要上线运行,任务启动提交都差不多用下面的命令行的方式,设置各种启动参数,
这时就得开发者清楚的知道每个参数的含义,如果再设置几个运行时资源参数,那启动命名会很长,可读性很差,参数解析用到了强校验,一旦设置错误,会直接报错,导致任务启动失败,最直接的异常是 <mark>找不到程序的jar</mark></p>
<div><pre><code>flink run -m yarn-cluster -p <span>1</span> -c com.xx.Main job.jar
</code></pre>
<div><span>1</span><br></div></div><p>开发<code>Flink Sql</code>程序,也需要设置一系列环境参数,除此之外,如果要使用纯sql的方式开发,举一个最简单的例子,代码如下</p>
<div><pre><code><span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>table<span>.</span>api<span>.</span></span><span>EnvironmentSettings</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>table<span>.</span>api<span>.</span></span><span>Table</span><span>;</span>
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>table<span>.</span>api<span>.</span></span><span>TableEnvironment</span><span>;</span>

<span>public</span> <span>class</span> <span>JavaTableApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>EnvironmentSettings</span> bbSettings <span>=</span> <span>EnvironmentSettings</span>
        <span>.</span><span>newInstance</span><span>(</span><span>)</span>
        <span>.</span><span>useBlinkPlanner</span><span>(</span><span>)</span>
        <span>.</span><span>build</span><span>(</span><span>)</span><span>;</span>
        
        <span>TableEnvironment</span> bsTableEnv <span>=</span> <span>TableEnvironment</span><span>.</span><span>create</span><span>(</span>bbSettings<span>)</span><span>;</span>

        <span>String</span> sourceDDL <span>=</span> <span>"CREATE TABLE datagen (  "</span> <span>+</span>
                <span>" f_random INT,  "</span> <span>+</span>
                <span>" f_random_str STRING,  "</span> <span>+</span>
                <span>" ts AS localtimestamp,  "</span> <span>+</span>
                <span>" WATERMARK FOR ts AS ts  "</span> <span>+</span>
                <span>") WITH (  "</span> <span>+</span>
                <span>" 'connector' = 'datagen',  "</span> <span>+</span>
                <span>" 'rows-per-second'='10',  "</span> <span>+</span>
                <span>" 'fields.f_random.min'='1',  "</span> <span>+</span>
                <span>" 'fields.f_random.max'='5',  "</span> <span>+</span>
                <span>" 'fields.f_random_str.length'='10'  "</span> <span>+</span>
                <span>")"</span><span>;</span>

        bsTableEnv<span>.</span><span>executeSql</span><span>(</span>sourceDDL<span>)</span><span>;</span>

        <span>String</span> sinkDDL <span>=</span> <span>"CREATE TABLE print_table ("</span> <span>+</span>
                <span>" f_random int,"</span> <span>+</span>
                <span>" c_val bigint, "</span> <span>+</span>
                <span>" wStart TIMESTAMP(3) "</span> <span>+</span>
                <span>") WITH ('connector' = 'print') "</span><span>;</span>
        
        bsTableEnv<span>.</span><span>executeSql</span><span>(</span>sinkDDL<span>)</span><span>;</span>
    <span>}</span>

<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br></div></div><p>我们会看到除了设置EnvironmentSettings参数之外,剩下的几乎大段大段的代码都是在写<code>sql</code>,用java代码拼接各种sql,这种编码的方式,极不优雅,如果业务复杂,更是难以维护,而且会发现,整个编码的模式是统一的,
都是声明一段sql,然后调用<code>executeSql</code>方法</p>
<p><strong>我们的设想是</strong>:能不能以一种更好的方式将这种重复的工作简单化,将<code>DataStream</code>和<code>Flink Sql</code>任务中的一些环境初始化相关的参数和启动相关参数简化,最好一行代码都不写,针对<code>Flink Sql</code>作业,也不想在代码里写大段的sql,能不能以一种更优雅的方式解决?</p>
<p><strong>答案是肯定的</strong></p>
<p>针对参数设置的问题,在<code>StreamX</code>中提出统一程序配置的概念,把程序的一系列参数从开发到部署阶段按照特定的格式配置到<code>application.yml</code>里,抽象出
一个通用的配置模板,按照这种规定的格式将上述配置的各项参数在配置文件里定义出来,在程序启动的时候将这个项目配置传入到程序中即可完成环境的初始化工作,在任务启动的时候也会自动识别启动时的参数,于是就有了<code>配置文件</code>这一概念</p>
<p>针对Flink Sql作业在代码里写sql的问题,<code>StreamX</code>针对<code>Flink Sql</code>作业做了更高层级封装和抽象,开发者只需要将sql按照一定的规范要求定义到<code>sql.yaml</code>文件中,在程序启动时将该sql文件传入到主程序中,
就会自动按照要求加载执行sql,于是就有了<code>sql文件</code>的概念</p>
<h2 id="相关术语"> 相关术语</h2>
<p>为了方便开发者理解和相互交流,我们把上面引出的,把程序的一系列参数从开发到部署阶段按照特定的格式配置到文件里,这个有特定作用的文件就是项目的 <strong> <mark><code>配置文件</code></mark> </strong></p>
<p>Flink Sql任务中将提取出来的sql放到<code>sql.yaml</code>中,这个有特定作用的文件就是项目的 <strong> <mark><code>sql文件</code></mark> </strong></p>
<h2 id="配置文件"> 配置文件</h2>
<p>在StreamX中,<code>DataStream</code>作业和<code>Flink Sql</code>作业配置文件是通用的,换言之,这个配置文件既能定义<code>DataStream</code>的各项配置,也能定义<code>Flink Sql</code>的各项配置(Flink Sql作业中配置文件是可选的), 配置文件的格式必须是<code>yaml</code>格式, 必须得符合yaml的格式规范</p>
<p>下面我们来详细看看这个配置文件的各项配置都是如何进行配置的,有哪些注意事项</p>
<div><pre><code><span>flink</span><span>:</span>
  <span>deployment</span><span>:</span>
    <span>option</span><span>:</span>
      <span>target</span><span>:</span> application
      <span>detached</span><span>:</span>
      <span>shutdownOnAttachedExit</span><span>:</span>
      <span>jobmanager</span><span>:</span>
    <span>property</span><span>:</span> <span>#@see: https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html</span>
      <span>$internal.application.main</span><span>:</span> com.streamxhub.streamx.flink.quickstart.QuickStartApp
      <span>yarn.application.name</span><span>:</span> Streamx QuickStart App
      <span>yarn.application.queue</span><span>:</span> 
      <span>taskmanager.numberOfTaskSlots</span><span>:</span> <span>1</span>
      <span>parallelism.default</span><span>:</span> <span>2</span>
      <span>jobmanager.memory</span><span>:</span>
        <span>flink.size</span><span>:</span>
        <span>heap.size</span><span>:</span>
        <span>jvm-metaspace.size</span><span>:</span>
        <span>jvm-overhead.max</span><span>:</span>
        <span>off-heap.size</span><span>:</span>
        <span>process.size</span><span>:</span>
      <span>taskmanager.memory</span><span>:</span>
        <span>flink.size</span><span>:</span>
        <span>framework.heap.size</span><span>:</span>
        <span>framework.off-heap.size</span><span>:</span>
        <span>managed.size</span><span>:</span>
        <span>process.size</span><span>:</span>
        <span>task.heap.size</span><span>:</span>
        <span>task.off-heap.size</span><span>:</span>
        <span>jvm-metaspace.size</span><span>:</span>
        <span>jvm-overhead.max</span><span>:</span>
        <span>jvm-overhead.min</span><span>:</span>
        <span>managed.fraction</span><span>:</span> <span>0.4</span>
  <span>checkpoints</span><span>:</span>
    <span>enable</span><span>:</span> <span>true</span>
    <span>interval</span><span>:</span> <span>30000</span>
    <span>mode</span><span>:</span> EXACTLY_ONCE
    <span>timeout</span><span>:</span> <span>300000</span>
    <span>unaligned</span><span>:</span> <span>true</span>
  <span>watermark</span><span>:</span>
    <span>interval</span><span>:</span> <span>10000</span>
  <span># 状态后端</span>
  <span>state</span><span>:</span>
    <span>backend</span><span>:</span> <span># see https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html</span>
      <span>value</span><span>:</span> filesystem <span># 保存类型('jobmanager', 'filesystem', 'rocksdb')</span>
      <span>memory</span><span>:</span> <span>5242880</span> <span># 针对jobmanager有效,最大内存</span>
      <span>async</span><span>:</span> <span>false</span>    <span># 针对(jobmanager,filesystem)有效,是否开启异步</span>
      <span>incremental</span><span>:</span> <span>true</span> <span>#针对rocksdb有效,是否开启增量</span>
      <span>#rocksdb 的配置参考 https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#rocksdb-state-backend</span>
      <span>#rocksdb配置key的前缀去掉:state.backend</span>
      <span>#rocksdb.block.blocksize:</span>
    <span>checkpoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
    <span>savepoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
  <span># 重启策略</span>
  <span>restart-strategy</span><span>:</span>
    <span>value</span><span>:</span> fixed<span>-</span>delay  <span>#重启策略[(fixed-delay|failure-rate|none)共3个可配置的策略]</span>
    <span>fixed-delay</span><span>:</span>
      <span>attempts</span><span>:</span> <span>3</span>
      <span>delay</span><span>:</span> <span>5000</span>
    <span>failure-rate</span><span>:</span>
      <span>max-failures-per-interval</span><span>:</span>
      <span>failure-rate-interval</span><span>:</span>
      <span>delay</span><span>:</span>
  <span># table</span>
  <span>table</span><span>:</span>
    <span>planner</span><span>:</span> blink <span># (blink|old|any)</span>
    <span>mode</span><span>:</span> streaming <span>#(batch|streaming)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br><span>40</span><br><span>41</span><br><span>42</span><br><span>43</span><br><span>44</span><br><span>45</span><br><span>46</span><br><span>47</span><br><span>48</span><br><span>49</span><br><span>50</span><br><span>51</span><br><span>52</span><br><span>53</span><br><span>54</span><br><span>55</span><br><span>56</span><br><span>57</span><br><span>58</span><br><span>59</span><br><span>60</span><br><span>61</span><br><span>62</span><br><span>63</span><br><span>64</span><br><span>65</span><br><span>66</span><br></div></div><p>上面是关于<code>开发时</code>和<code>项目部署</code>时需要关注的环境相关的完整的配置,这些配置是在<code>flink</code>的namespace下进行配置的,主要分为2大类</p>
<ul>
<li><code>deployment</code>下的配置是项目部署相关的配置(<code>即项目启动时的一系列资源相关的配置参数</code>)</li>
<li>其他是开发时需要关注的环境相关的配置</li>
</ul>
<p>开发时需要关注的环境相关的配置有5项</p>
<div>
<ul>
<li><code>checkpoints</code></li>
<li><code>watermark</code></li>
<li><code>state</code></li>
<li><code>restart-strategy</code></li>
<li><code>table</code></li>
</ul>
</div>
<h3 id="deployment"> Deployment</h3>
<p>deployment下放的是部署相关的参数和配置项,具体又分为两类</p>
<ul>
<li><code>option</code></li>
<li><code>property</code></li>
</ul>
<h4 id="option"> option</h4>
<p><code>option</code>下放的参数是flink run 下支持的参数,目前支持的参数如下
<ClientOnly>
<table-data name="option"></table-data>
</ClientOnly>
<code>parallelism</code> (-p) 并行度不支持在option里配置,会在后面的property里配置
<code>class</code> (-c) 程序main不支持在option里配置,会在后面的property里配置</p>
<div><p>注意事项</p>
<p>option下的参数必须是 <code>完整参数名</code></p>
</div>
<h4 id="property"> property</h4>
<p><code>property</code>下放的参数是标准参数-D下的参数,可以分为两类</p>
<ul>
<li>基础参数</li>
<li>Memory参数</li>
</ul>
<h5 id="基础参数"> 基础参数</h5>
<p>基础参数可以配置的选项非常之多,这里举例5个最基础的设置
<ClientOnly>
<table-data name="property"></table-data>
</ClientOnly></p>
<div><p>注意事项</p>
<p><code>$internal.application.main</code> 和 <code>yarn.application.name</code> 这两个参数是必须的</p>
</div>
<p>如您需要设置更多的参数,可参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html" target="_blank" rel="noopener noreferrer"><code>这里</code></a>
一定要将这些参数放到<code>property</code>下,并且参数名称要正确,<code>StreamX</code>会自动解析这些参数并生效</p>
<h5 id="memory参数"> Memory参数</h5>
<p>Memory相关的参数设置也非常之多,一般常见的配置如下</p>
<ClientOnly>
  <table-data name="memory"></table-data>
</ClientOnly>
<p>同样,如你想配置更多的内存相关的参数,请参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup.html" target="_blank" rel="noopener noreferrer"><code>这里</code></a> 查看<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup.html" target="_blank" rel="noopener noreferrer"><code>Flink Process Memory</code></a> , <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_tm.html" target="_blank" rel="noopener noreferrer"><code>jobmanager</code></a> 及 <a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/memory/mem_setup_jobmanager.html" target="_blank" rel="noopener noreferrer"><code>taskmanager</code></a>
相关的内存配置将这些参数放到<code>property</code>下,保证参数正确即可生效</p>
<h5 id="配置总内存"> 配置总内存</h5>
<p>Flink JVM 进程的进程总内存（Total Process Memory）包含了由 Flink 应用使用的内存（Flink 总内存）以及由运行 Flink 的 JVM 使用的内存。 Flink 总内存（Total Flink Memory）包括 JVM 堆内存（Heap Memory）和堆外内存（Off-Heap Memory）。 其中堆外内存包括直接内存（Direct Memory）和本地内存（Native Memory）</p>
<center>
<img src="/streamx-docs/assets/img/doc-img/process_mem_model.svg" width="340px"/>
</center>
<p>配置 Flink 进程内存最简单的方法是指定以下两个配置项中的任意一个：</p>
<ClientOnly>
<table-data name="totalMem"></table-data>
</ClientOnly>
<div><p>注意事项</p>
<p>不建议同时设置进程总内存和 Flink 总内存。 这可能会造成内存配置冲突，从而导致部署失败。 额外配置其他内存部分时，同样需要注意可能产生的配置冲突。</p>
</div>
<h3 id="checkpoints"> Checkpoints</h3>
<p>Checkpoints 的配置比较简单,按照下面的方式进行配置即可</p>
<ClientOnly>
  <table-data name="checkpoints"></table-data>
</ClientOnly>
<h3 id="watermark"> Watermark</h3>
<p><code>watermark</code> 配置只需要设置下Watermark的生成周期<code>interval</code>即可</p>
<h3 id="state"> State</h3>
<p><code>state</code>是设置状态相关的配置</p>
<div><pre><code><span>state</span><span>:</span>
  <span>backend</span><span>:</span> <span># see https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html</span>
    <span>value</span><span>:</span> filesystem <span># 保存类型('jobmanager', 'filesystem', 'rocksdb')</span>
    <span>memory</span><span>:</span> <span>5242880</span> <span># 针对jobmanager有效,最大内存</span>
    <span>async</span><span>:</span> <span>false</span>    <span># 针对(jobmanager,filesystem)有效,是否开启异步</span>
    <span>incremental</span><span>:</span> <span>true</span> <span>#针对rocksdb有效,是否开启增量</span>
    <span>#rocksdb 的配置参考 https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#rocksdb-state-backend</span>
    <span>#rocksdb配置key的前缀去掉:state.backend</span>
    <span>#rocksdb.block.blocksize:</span>
  <span>checkpoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
  <span>savepoints.dir</span><span>:</span> file<span>:</span>///tmp/chkdir
  <span>checkpoints.num-retained</span><span>:</span> <span>1</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div><p>我们可以看到大体可以分为两类</p>
<ul>
<li>backend 相关的配置</li>
<li>checkpoints 相关的配置</li>
</ul>
<h4 id="backend"> backend</h4>
<p>很直观的,<code>backend</code>下是设置状态后端相关的配置,状态后台的配置遵照<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html" target="_blank" rel="noopener noreferrer"><code>官网文档</code></a>的配置规则,在这里支持以下配置</p>
<ClientOnly>
  <table-data name="backend"></table-data>
</ClientOnly>
<p>如果<code>backend</code>的保存类型为<code>rocksdb</code>,则可能要进一步设置<code>rocksdb</code>相关的配置,可以参考<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/config.html#rocksdb-state-backend" target="_blank" rel="noopener noreferrer"><code>官网</code></a>来进行相关配置,
需要注意的是官网关于<code>rocksdb</code>的配置都是以<code>state.backend</code>为前缀,而当前的命名空间就是在<code>state.backend</code>下,注意要保证参数名正确</p>
<div><p>注意事项</p>
<p><code>value</code>项非标准配置,该项用来设置状态的保存类型(<code>jobmanager</code> | <code>filesystem</code> | <code>rocksdb</code>),其他项均为标准配置,遵守官网的规范</p>
</div>
<h3 id="restart-strategy"> Restart Strategy</h3>
<p>重启策略的配置非常直观,在flink中有三种重启策略,对应了这里的三种配置,如下:</p>
<div><pre><code>  <span>restart-strategy</span><span>:</span>
    <span>value</span><span>:</span> fixed<span>-</span>delay  <span>#重启策略[(fixed-delay|failure-rate|none)共3个可配置的策略]</span>
    <span>fixed-delay</span><span>:</span>
      <span>attempts</span><span>:</span> <span>3</span>
      <span>delay</span><span>:</span> <span>5000</span>
    <span>failure-rate</span><span>:</span>
      <span>max-failures-per-interval</span><span>:</span>
      <span>failure-rate-interval</span><span>:</span>
      <span>delay</span><span>:</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div><p><code>value</code>下配置具体的选择哪种重启策略</p>
<div>
<ul>
<li>fixed-delay</li>
<li>failure-rate</li>
<li>none</li>
</ul>
</div>
<h4 id="fixed-delay-固定间隔"> fixed-delay(固定间隔)</h4>
<ClientOnly>
<table-data name="fixed-delay"></table-data>
</ClientOnly>
<div><p>示例</p>
<div><pre><code><span>attempts</span><span>:</span> <span>5</span>
<span>delay</span><span>:</span> 3 s
</code></pre>
<div><span>1</span><br><span>2</span><br></div></div><p>即:任务最大的失败重试次数是<code>5次</code>,每次任务重启的时间间隔是<code>3秒</code>,如果失败次数到达5次,则任务失败退出</p>
</div>
<h4 id="failure-rate-失败率"> failure-rate(失败率)</h4>
<ClientOnly>
<table-data name="failure-rate"></table-data>
</ClientOnly>
<div><p>示例</p>
<div><pre><code> <span>max-failures-per-interval</span><span>:</span> <span>10</span>
 <span>failure-rate-interval</span><span>:</span> 5 min
 <span>delay</span><span>:</span> 2 s
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br></div></div><p>即:每次异常重启的时间间隔是<code>2秒</code>,如果在<code>5分钟内</code>,失败总次数到达<code>10次</code> 则任务失败.</p>
</div>
<h4 id="none-无重启"> None (无重启)</h4>
<p>无重启无需配置任务参数</p>
<h4 id="单位后缀"> 单位后缀</h4>
<p>时间间隔和频率设置需注意,可以不带单位后缀,如果不带单位后缀则默认会当成<code>毫秒</code>来处理,可选的单位有</p>
<ul>
<li>s    秒</li>
<li>m    分钟</li>
<li>min  分钟</li>
<li>h    小时</li>
<li>d    日</li>
</ul>
<h3 id="table"> Table</h3>
<p>在<code>table</code>下是Flink Sql相关的配置,目前支持的配置项和作用如下</p>
<ul>
<li>planner</li>
<li>mode</li>
<li>catalog</li>
<li>database</li>
</ul>
<ClientOnly>
  <table-data name="tables"></table-data>
</ClientOnly>
<h2 id="sql-文件"> Sql 文件</h2>
<p>Sql 文件必须是yaml格式的文件,得遵循yaml文件的定义规则,具体内部sql格式的定义非常简单,如下:</p>
<div><pre><code><span>sql</span>: <span>|</span>
  <span>CREATE</span> <span>TABLE</span> datagen <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING<span>,</span>
    ts <span>AS</span> localtimestamp<span>,</span>
    WATERMARK <span>FOR</span> ts <span>AS</span> ts
  <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'datagen'</span><span>,</span>
    <span>-- optional options --</span>
    <span>'rows-per-second'</span><span>=</span><span>'5'</span><span>,</span>
    <span>'fields.f_sequence.kind'</span><span>=</span><span>'sequence'</span><span>,</span>
    <span>'fields.f_sequence.start'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_sequence.end'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random.min'</span><span>=</span><span>'1'</span><span>,</span>
    <span>'fields.f_random.max'</span><span>=</span><span>'1000'</span><span>,</span>
    <span>'fields.f_random_str.length'</span><span>=</span><span>'10'</span>
  <span>)</span><span>;</span>

  <span>CREATE</span> <span>TABLE</span> print_table <span>(</span>
    f_sequence <span>INT</span><span>,</span>
    f_random <span>INT</span><span>,</span>
    f_random_str STRING
    <span>)</span> <span>WITH</span> <span>(</span>
    <span>'connector'</span> <span>=</span> <span>'print'</span>
  <span>)</span><span>;</span>

  <span>INSERT</span> <span>INTO</span> print_table <span>select</span> f_sequence<span>,</span>f_random<span>,</span>f_random_str <span>from</span> datagen<span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br></div></div><p><code>sql</code>为当前sql的id,必须是唯一的,后面的内容则是具体的sql</p>
<div><p>特别注意</p>
<p>上面内容中 <strong>sql:</strong> 后面的 <strong>|</strong> 是必带的, 加上 <strong>|</strong> 会保留整段内容的格式,重点是保留了换行符, StreamX封装了Flink Sql的提交,可以直接将多个Sql一次性定义出来,每个Sql必须用 <strong>;</strong> 分割,每段 Sql也必须遵循Flink Sql规定的格式和规范</p>
</div>
<h2 id="总结"> 总结</h2>
<p>本章节详细介绍了<code>配置文件</code>和<code>sql文件</code>的由来和具体配置,相信你已经有了一个初步的印象和概念,具体使用请查后续章节</p>
]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/process_steps.png" type="image/png"/>
    </item>
    <item>
      <title>编程模型</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/model/model/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/model/model/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">编程模型</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<p>任何框架都有一些要遵循的规则和约定,我们只有遵循并掌握了这些规则,才能更加游刃有余的使用,使其发挥事半功倍的效果, 我们开发<code>flink</code>作业,其实就是利用<code>flink</code>提供的api,按照<code>flink</code>要求的开发方式,写一个可以执行的(必须有<code>main()</code>函数)的程序,在程序里接入各种<code>Connector</code>经过一系列的<code>算子</code>操作,最终将数据通过<code>Connector</code>sink到目标存储,
我们把这种按照某种约定的规则去逐步编程的方式称之为<code>编程模型</code>, 这一章节我们就来聊聊<code>StreamX</code>的<code>编程模型</code>以及开发注意事项</p>
<p>我们从这几个方面开始入手</p>
<div>
<ul>
<li>架构</li>
<li>编程模型</li>
<li>RunTime Context</li>
<li>生命周期</li>
<li>目录结构</li>
<li>打包部署</li>
</ul>
</div>
<h2 id="架构"> 架构</h2>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_archite.png"/><br>
</center>
<h2 id="编程模型"> 编程模型</h2>
<p><code>streamx-core</code>定位是编程时框架,快速开发脚手架,专门为简化Flink开发而生,开发者在开发阶段会使用到该模块,下面我们来看看<code>DataStrema</code>和<code>Flink sql</code>用<code>StreamX</code>来开发编程模型是什么样的,有什么规范和要求</p>
<h3 id="datastream"> DataStream</h3>
<p><code>StreamX</code>提供了<code>scala</code>和<code>java</code>两种api来开发<code>DataStream</code>程序,具体代码开发如下</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreaming
<span>import</span> <span>org<span>.</span>apache<span>.</span>flink<span>.</span>api<span>.</span>scala<span>.</span></span>_

<span>object</span> MyFlinkApp <span>extends</span> FlinkStreaming <span>{</span>

    <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
        <span>.</span><span>.</span><span>.</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>public</span> <span>class</span> <span>MyFlinkJavaApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>environment<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
            <span>//用户可以给environment设置参数...</span>
            <span>System</span><span>.</span>out<span>.</span><span>println</span><span>(</span><span>"environment argument set..."</span><span>)</span><span>;</span>
        <span>}</span><span>)</span><span>;</span>
        
        <span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>javaConfig<span>)</span><span>;</span>
            
        <span>.</span><span>.</span><span>.</span><span>.</span>    

        context<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>

</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br></div></div></CodeGroupItem>
</CodeGroup>
<p>用<code>scala</code> api开发,程序必须要继承<code>FlinkStreaming</code>,继承之后,会强制让开发者实现<code>handle()</code>方法,该方法就是用户写代码的入口, 同时<code>streamingContext</code>供开发者使用</p>
<p>用<code>java</code> api开发由于语言本身的限制没法省掉<code>main()</code>方法,所以会是一个标准的<code>main()</code>函数, 需要用户手动创建<code>StreamingContext</code>,<code>StreamingContext</code>是非常重要的一个类,稍后会介绍</p>
<div><p>特别注意</p>
<p>以上几行scala和java代码就是用StreamX开发DataStream必不可少的最基本的骨架代码,用StreamX开发DataStream程序,从这几行代码开始,
JAVA API开发需要开发者手动启动任务<code>start</code></p>
</div>
<h3 id="flink-sql"> Flink SQL</h3>
<p>TableEnvironment 是用来创建 Table &amp; SQL 程序的上下文执行环境,也是 Table &amp; SQL 程序的入口,Table &amp; SQL 程序的所有功能都是围绕 TableEnvironment 这个核心类展开的。TableEnvironment 的主要职能包括：对接外部系统，表及元数据的注册和检索，执行SQL语句，提供更详细的配置选项。</p>
<p>Flink社区一直在推进 DataStream 的批处理能力,统一流批一体,在Flink 1.12中流批一体真正统一运行,诸多历史API如DataSet API,BatchTableEnvironment API等被废弃,退出历史舞台,官方推荐使用 <mark>TableEnvironment</mark> 和 <mark>StreamTableEnvironment</mark></p>
<p><code>StreamX</code>针对 <mark>TableEnvironment</mark> 和 <mark>StreamTableEnvironment</mark> 这两种环境的开发,提供了对应的更方便快捷的API</p>
<h4 id="tableenvironment"> TableEnvironment</h4>
<p>开发Table &amp; SQL 作业,TableEnvironment 会是 Flink 推荐使用的入口类, 同时能支持 JAVA API 和 SCALA API,下面的代码演示了在<code>StreamX</code>如何开发一个TableEnvironment类型的作业</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkTable

<span>object</span> TableApp <span>extends</span> FlinkTable <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>.</span><span>.</span><span>.</span>
  <span>}</span>
  
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code><span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>TableContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>TableEnvConfig</span><span>;</span>

<span>public</span> <span>class</span> <span>JavaTableApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>TableEnvConfig</span> tableEnvConfig <span>=</span> <span>new</span> <span>TableEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>TableContext</span> context <span>=</span> <span>new</span> <span>TableContext</span><span>(</span>tableEnvConfig<span>)</span><span>;</span>
        <span>.</span><span>.</span><span>.</span>
        context<span>.</span><span>start</span><span>(</span><span>"Flink SQl Job"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>特别注意</p>
<p>以上几行scala和java代码就是用StreamX开发TableEnvironment必不可少的最基本的骨架代码,用StreamX开发TableEnvironment程序,从这几行代码开始,
SCALA API必须继承FlinkTable,JAVA API开发需要手动构造TableContext,需要开发者手动启动任务<code>start</code></p>
</div>
<h4 id="streamtableenvironment"> StreamTableEnvironment</h4>
<p>StreamTableEnvironment用于流计算场景,流计算的对象是DataStream。相比 TableEnvironment,StreamTableEnvironment 提供了 DataStream 和 Table 之间相互转换的接口,如果用户的程序除了使用 Table API &amp; SQL 编写外,还需要使用到 DataStream API,则需要使用 StreamTableEnvironment。
下面的代码演示了在<code>StreamX</code>如何开发一个StreamTableEnvironment类型的作业</p>
<CodeGroup>
<CodeGroupItem title="scala" active>
<div><pre><code><span>package</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>test<span>.</span>tablesql</span>

<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span>FlinkStreamTable

<span>object</span> StreamTableApp <span>extends</span> FlinkStreamTable <span>{</span>

  <span>override</span> <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    <span>.</span><span>.</span><span>.</span>
  <span>}</span>
  
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></div></div></CodeGroupItem>
<CodeGroupItem title="java">
<div><pre><code>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span></span><span>StreamTableContext</span><span>;</span>
<span>import</span> <span>com<span>.</span>streamxhub<span>.</span>streamx<span>.</span>flink<span>.</span>core<span>.</span>scala<span>.</span>util<span>.</span></span><span>StreamTableEnvConfig</span><span>;</span>

<span>public</span> <span>class</span> <span>JavaStreamTableApp</span> <span>{</span>

    <span>public</span> <span>static</span> <span>void</span> <span>main</span><span>(</span><span>String</span><span>[</span><span>]</span> args<span>)</span> <span>{</span>
        <span>StreamTableEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamTableEnvConfig</span><span>(</span>args<span>,</span> <span>null</span><span>,</span> <span>null</span><span>)</span><span>;</span>
        <span>StreamTableContext</span> context <span>=</span> <span>new</span> <span>StreamTableContext</span><span>(</span>javaConfig<span>)</span><span>;</span>

        <span>.</span><span>.</span><span>.</span>

        context<span>.</span><span>start</span><span>(</span><span>"Flink SQl Job"</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br></div></div></CodeGroupItem>
</CodeGroup>
<div><p>特别注意</p>
<p>以上几行scala和java代码就是用StreamX开发StreamTableEnvironment必不可少的最基本的骨架代码,用StreamX开发StreamTableEnvironment程序,从这几行代码开始,java代码需要手动构造StreamTableContext,<code>JAVA API</code>开发需要开发者手动启动任务<code>&quot;start&quot;</code></p>
</div>
<h2 id="runtime-context"> RunTime Context</h2>
<p><code>RunTime Context</code> —— <mark><code>StreamingContext</code></mark> , <mark><code>TableContext</code></mark> , <mark><code>StreamTableContext</code></mark> 是<code>StreamX</code>中几个非常重要三个对象,接下来我们具体看看这三个<code>Context</code>的定义和作用</p>
<center>
<img src="/streamx-docs/assets/img/doc-img/streamx_coreapi.png" width="60%"/>
</center>
<h3 id="streamingcontext"> StreamingContext</h3>
<p><code>StreamingContext</code>继承自<code>StreamExecutionEnvironment</code>,在<code>StreamExecutionEnvironment</code>的基础之上增加了<code>ParameterTool</code>,简单可以理解为</p>
<p><strong> <mark><code>StreamingContext</code></mark> </strong> = <strong> <mark><code>ParameterTool</code></mark> </strong> + <strong> <mark><code>StreamExecutionEnvironment</code></mark> </strong></p>
<p>具体定义如下</p>
<div><pre><code><span>class</span> StreamingContext<span>(</span><span>val</span> parameter<span>:</span> ParameterTool<span>,</span> <span>private</span> <span>val</span> environment<span>:</span> StreamExecutionEnvironment<span>)</span> 
    <span>extends</span> StreamExecutionEnvironment<span>(</span>environment<span>.</span>getJavaEnv<span>)</span> <span>{</span>

  <span>/**
   * for scala
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> <span>(</span>ParameterTool<span>,</span> StreamExecutionEnvironment<span>)</span><span>)</span> <span>=</span> <span>this</span><span>(</span>args<span>.</span>_1<span>,</span> args<span>.</span>_2<span>)</span>

  <span>/**
   * for Java
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkStreamingInitializer<span>.</span>initJavaStream<span>(</span>args<span>)</span><span>)</span>
  
  <span>.</span><span>.</span><span>.</span>  
<span>}</span>  
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br></div></div><div><p>特别注意</p>
<p>这个对象非常重要,在DataStream作业中会贯穿整个任务的生命周期,StreamingContext本身继承自StreamExecutionEnvironment,配置文件会完全融合到StreamingContext中,这样就可以非常方便的从StreamingContext中获取各种参数</p>
</div>
<p>在StreamX中,StreamingContext 也是 JAVA API编写DataStream作业的入口类,StreamingContext的构造方法中有一个是专门为JAVA API打造的,该构造函数定义如下:</p>
<div><pre><code><span>/**
 * for Java
 * @param args
*/</span>
<span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkStreamingInitializer<span>.</span>initJavaStream<span>(</span>args<span>)</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>由上面的构造方法可以看到创建<code>StreamingContext</code>,需要传入一个<code>StreamEnvConfig</code>对象,<code>StreamEnvConfig</code>定义如下:</p>
<div><pre><code><span>class</span> StreamEnvConfig<span>(</span><span>val</span> args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>,</span> <span>val</span> conf<span>:</span> StreamEnvConfigFunction<span>)</span>
</code></pre>
<div><span>1</span><br></div></div><p>StreamEnvConfig的构造方法中,其中</p>
<ul>
<li><code>args</code> 为启动参数,必须为<code>main</code>方法里的<code>args</code></li>
<li><code>conf</code> 为<code>StreamEnvConfigFunction</code>类型的<code>Function</code></li>
</ul>
<p><code>StreamEnvConfigFunction</code> 定义如下</p>
<div><pre><code><span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>StreamEnvConfigFunction</span> <span>{</span>
    <span>/**
     * 用于初始化StreamExecutionEnvironment的时候,用于可以实现该函数,自定义要设置的参数...
     *
     * @param environment
     * @param parameterTool
     */</span>
    <span>void</span> <span>configuration</span><span>(</span><span>StreamExecutionEnvironment</span> environment<span>,</span> <span>ParameterTool</span> parameterTool<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><p>该<code>Function</code>的作用是让开发者可以通过钩子的方式设置更多的参数,会将 <code>parameter</code>(解析配置文件里所有的参数)和初始化好的<code>StreamExecutionEnvironment</code>对象传给开发者去完成更多的参数设置,如:</p>
<div><pre><code><span>StreamEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>environment<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
    <span>System</span><span>.</span>out<span>.</span><span>println</span><span>(</span><span>"environment argument set..."</span><span>)</span><span>;</span>
    environment<span>.</span><span>getConfig</span><span>(</span><span>)</span><span>.</span><span>enableForceAvro</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>StreamingContext</span> context <span>=</span> <span>new</span> <span>StreamingContext</span><span>(</span>javaConfig<span>)</span><span>;</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><h3 id="tablecontext"> TableContext</h3>
<p><code>TableContext</code>继承自<code>TableEnvironment</code>,在<code>TableEnvironment</code>的基础之上增加了<code>ParameterTool</code>,用来创建 <code>Table</code> &amp; <code>SQL</code> 程序的上下文执行环境,简单可以理解为</p>
<p><strong> <mark><code>TableContext</code></mark> </strong> = <strong> <mark><code>ParameterTool</code></mark> </strong> + <strong> <mark><code>TableEnvironment</code></mark> </strong></p>
<p>具体定义如下</p>
<div><pre><code><span>class</span> TableContext<span>(</span><span>val</span> parameter<span>:</span> ParameterTool<span>,</span>
                   <span>private</span> <span>val</span> tableEnv<span>:</span> TableEnvironment<span>)</span> 
                   <span>extends</span> TableEnvironment 
                   <span>with</span> FlinkTableTrait <span>{</span>

  <span>/**
   * for scala
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> <span>(</span>ParameterTool<span>,</span> TableEnvironment<span>)</span><span>)</span> <span>=</span> <span>this</span><span>(</span>args<span>.</span>_1<span>,</span> args<span>.</span>_2<span>)</span>

  <span>/**
   * for java
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> TableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaTable<span>(</span>args<span>)</span><span>)</span>
  
  <span>.</span><span>.</span><span>.</span>
<span>}</span>  
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br></div></div><p>在StreamX中,<code>TableContext</code> 也是 JAVA API编写TableEnvironment类型的Table Sql作业的入口类,TableContext的构造方法中有一个是专门为JAVA API打造的,该构造函数定义如下:</p>
<div><pre><code>
<span>/**
* for java
* @param args
*/</span>
<span>def</span> <span>this</span><span>(</span>args<span>:</span> TableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaTable<span>(</span>args<span>)</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></div></div><p>由上面的构造方法可以看到创建<code>TableContext</code>,需要传入一个<code>TableEnvConfig</code>对象,<code>TableEnvConfig</code>定义如下:</p>
<div><pre><code><span>class</span> TableEnvConfig<span>(</span><span>val</span> args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>,</span> <span>val</span> conf<span>:</span> TableEnvConfigFunction<span>)</span>
</code></pre>
<div><span>1</span><br></div></div><p>TableEnvConfig的构造方法中,其中</p>
<ul>
<li><code>args</code> 为启动参数,必须为<code>main</code>方法里的<code>args</code></li>
<li><code>conf</code> 为<code>TableEnvConfigFunction</code>类型的<code>Function</code></li>
</ul>
<p><code>TableEnvConfigFunction</code> 定义如下</p>
<div><pre><code><span>@FunctionalInterface</span>
<span>public</span> <span>interface</span> <span>TableEnvConfigFunction</span> <span>{</span>
    <span>/**
     * 用于初始化TableEnvironment的时候,用于可以实现该函数,自定义要设置的参数...
     *
     * @param tableConfig
     * @param parameterTool
     */</span>
    <span>void</span> <span>configuration</span><span>(</span><span>TableConfig</span> tableConfig<span>,</span> <span>ParameterTool</span> parameterTool<span>)</span><span>;</span>

<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></div></div><p>该<code>Function</code>的作用是让开发者可以通过钩子的方式设置更多的参数,会将 <code>parameter</code>(解析配置文件里所有的参数)和初始化好的<code>TableEnvironment</code>中的<code>TableConfig</code>对象传给开发者去完成更多的参数设置,如:</p>
<div><pre><code><span>TableEnvConfig</span> config <span>=</span> <span>new</span> <span>TableEnvConfig</span><span>(</span>args<span>,</span><span>(</span>tableConfig<span>,</span>parameterTool<span>)</span><span>-></span><span>{</span>
    tableConfig<span>.</span><span>setLocalTimeZone</span><span>(</span><span>ZoneId</span><span>.</span><span>of</span><span>(</span><span>"Asia/Shanghai"</span><span>)</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>
<span>TableContext</span> context <span>=</span> <span>new</span> <span>TableContext</span><span>(</span>config<span>)</span><span>;</span>
<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><h3 id="streamtablecontext"> StreamTableContext</h3>
<p><code>StreamTableContext</code>继承自<code>StreamTableEnvironment</code>,用于流计算场景,流计算的对象是<code>DataStream</code>,相比<code>TableEnvironment</code>,<code>StreamTableEnvironment</code> 提供了 <code>DataStream</code> 和 <code>Table</code> 之间相互转换的接口,
<code>StreamTableContext</code>在<code>StreamTableEnvironment</code>的基础之上增加了<code>ParameterTool</code>,又直接接入了<code>StreamTableEnvironment</code>的API,简单可以理解为</p>
<p><strong> <mark><code>StreamTableContext</code></mark> </strong> = <strong> <mark><code>ParameterTool</code></mark> </strong> + <strong> <mark><code>StreamTableEnvironment</code></mark> </strong> + <strong> <mark><code>StreamExecutionEnvironment</code></mark> </strong></p>
<p>具体定义如下</p>
<div><pre><code>
<span>class</span> StreamTableContext<span>(</span><span>val</span> parameter<span>:</span> ParameterTool<span>,</span>
                         <span>private</span> <span>val</span> streamEnv<span>:</span> StreamExecutionEnvironment<span>,</span>
                         <span>private</span> <span>val</span> tableEnv<span>:</span> StreamTableEnvironment<span>)</span> 
                         <span>extends</span> StreamTableEnvironment 
                         <span>with</span> FlinkTableTrait <span>{</span>

  <span>/**
   * 一旦 Table 被转化为 DataStream,
   * 必须使用 StreamExecutionEnvironment 的 execute 方法执行该 DataStream 作业。
   */</span>
  <span>private</span><span>[</span>scala<span>]</span> <span>var</span> isConvertedToDataStream<span>:</span> <span>Boolean</span> <span>=</span> <span>false</span>

  <span>/**
   * for scala
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> <span>(</span>ParameterTool<span>,</span> StreamExecutionEnvironment<span>,</span> StreamTableEnvironment<span>)</span><span>)</span> <span>=</span> 
  <span>this</span><span>(</span>args<span>.</span>_1<span>,</span> args<span>.</span>_2<span>,</span> args<span>.</span>_3<span>)</span>

  <span>/**
   * for Java
   *
   * @param args
   */</span>
  <span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamTableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaStreamTable<span>(</span>args<span>)</span><span>)</span>
  <span>.</span><span>.</span><span>.</span>  
<span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br></div></div><p>在StreamX中,<code>StreamTableContext</code> 是 JAVA API编写StreamTableEnvironment类型的Table Sql作业的入口类,StreamTableContext的构造方法中有一个是专门为JAVA API打造的,该构造函数定义如下:</p>
<div><pre><code>
  <span>/**
   * for Java
   *
   * @param args
   */</span>
<span>def</span> <span>this</span><span>(</span>args<span>:</span> StreamTableEnvConfig<span>)</span> <span>=</span> <span>this</span><span>(</span>FlinkTableInitializer<span>.</span>initJavaStreamTable<span>(</span>args<span>)</span><span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br></div></div><p>由上面的构造方法可以看到创建<code>StreamTableContext</code>,需要传入一个<code>StreamTableEnvConfig</code>对象,<code>StreamTableEnvConfig</code>定义如下:</p>
<div><pre><code><span>class</span> StreamTableEnvConfig<span>(</span>
    <span>val</span> args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>,</span>
    <span>val</span> streamConfig<span>:</span> StreamEnvConfigFunction<span>,</span> 
    <span>val</span> tableConfig<span>:</span> TableEnvConfigFunction
<span>)</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br></div></div><p>StreamTableEnvConfig的构造方法中有三个参数,其中</p>
<ul>
<li><code>args</code> 为启动参数,必须为<code>main</code>方法里的<code>args</code></li>
<li><code>streamConfig</code> 为<code>StreamEnvConfigFunction</code>类型的<code>Function</code></li>
<li><code>tableConfig</code> 为<code>TableEnvConfigFunction</code>类型的<code>Function</code></li>
</ul>
<p><code>StreamEnvConfigFunction</code>和<code>TableEnvConfigFunction</code> 定义上面已经讲过,这里不再赘述</p>
<p>该<code>Function</code>的作用是让开发者可以通过钩子的方式设置更多的参数,和上面其他参数设置不同的是,该<code>Function</code>提供了同时设置<code>StreamExecutionEnvironment</code>和<code>TableEnvironment</code>的机会 ,会将 <code>parameter</code>和初始化好的<code>StreamExecutionEnvironment</code>和<code>TableEnvironment</code>中的<code>TableConfig</code>对象传给开发者去完成更多的参数设置,如:</p>
<div><pre><code>
<span>StreamTableEnvConfig</span> javaConfig <span>=</span> <span>new</span> <span>StreamTableEnvConfig</span><span>(</span>args<span>,</span> <span>(</span>environment<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
    environment<span>.</span><span>getConfig</span><span>(</span><span>)</span><span>.</span><span>enableForceAvro</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>,</span> <span>(</span>tableConfig<span>,</span> parameterTool<span>)</span> <span>-></span> <span>{</span>
    tableConfig<span>.</span><span>setLocalTimeZone</span><span>(</span><span>ZoneId</span><span>.</span><span>of</span><span>(</span><span>"Asia/Shanghai"</span><span>)</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>StreamTableContext</span> context <span>=</span> <span>new</span> <span>StreamTableContext</span><span>(</span>javaConfig<span>)</span><span>;</span>

<span>.</span><span>.</span><span>.</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br></div></div><div><p>特别提示</p>
<p>在<code>StreamTableContext</code>中可以直接使用<code>StreamExecutionEnvironment</code>的<code>API</code>, <mark>以$打头的方法</mark> 都是<code>StreamExecutionEnvironment</code>的API</p>
<p><img src="/streamx-docs/assets/img/doc-img/streamx_apis.jpeg" alt=""></p>
</div>
<h2 id="生命周期"> 生命周期</h2>
<p>生命周期的概念目前只针对<code>scala</code> API,该生命周期明确的定义了整个任务运行的全过程 ,但凡继承自<code>FlinkStreaming</code>或<code>FlinkTable</code>或<code>StreamingTable</code> 就会按这个生命周期执行,生命周期的核心方法如下</p>
<div><pre><code> <span>final</span> <span>def</span> main<span>(</span>args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    init<span>(</span>args<span>)</span>
    ready<span>(</span><span>)</span>
    handle<span>(</span><span>)</span>
    jobExecutionResult <span>=</span> context<span>.</span>start<span>(</span><span>)</span>
    destroy<span>(</span><span>)</span>
  <span>}</span>

  <span>private</span><span>[</span><span>this</span><span>]</span> <span>def</span> init<span>(</span>args<span>:</span> Array<span>[</span><span>String</span><span>]</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span>
    SystemPropertyUtils<span>.</span>setAppHome<span>(</span>KEY_APP_HOME<span>,</span> classOf<span>[</span>FlinkStreaming<span>]</span><span>)</span>
    context <span>=</span> <span>new</span> StreamingContext<span>(</span>FlinkStreamingInitializer<span>.</span>initStream<span>(</span>args<span>,</span> config<span>)</span><span>)</span>
  <span>}</span>

  <span>/**
   * 用户可覆盖次方法...
   *
   */</span>
  <span>def</span> ready<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span><span>}</span>

  <span>def</span> config<span>(</span>env<span>:</span> StreamExecutionEnvironment<span>,</span> parameter<span>:</span> ParameterTool<span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span><span>}</span>

  <span>def</span> handle<span>(</span><span>)</span><span>:</span> <span>Unit</span>

  <span>def</span> destroy<span>(</span><span>)</span><span>:</span> <span>Unit</span> <span>=</span> <span>{</span><span>}</span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br></div></div><p>生命周期如下</p>
<ul>
<li><strong>init</strong>          配置文件初始化阶段</li>
<li><strong>config</strong>        开发者手动设置参数阶段</li>
<li><strong>ready</strong>         启动之前执行自定义动作阶段</li>
<li><strong>handle</strong>        开发者代码接入阶段</li>
<li><strong>start</strong>         程序启动阶段</li>
<li><strong>destroy</strong>       销毁阶段</li>
</ul>
<p><img src="/streamx-docs/assets/img/doc-img/streamx_scala_life_cycle.png" alt="Life Cycle"></p>
<h3 id="生命周期之-init"> 生命周期之—— init</h3>
<p><strong>init</strong> 阶段,框架会自动解析传入的配置文件,按照里面的定义的各种参数初始化<code>StreamExecutionEnvironment</code>,这一步是框架自动执行,不需要开发者参与</p>
<h3 id="生命周期之-config"> 生命周期之—— config</h3>
<p><strong>config</strong> 阶段的目的是让开发者可以通过钩子的方式设置更多的参数(约定的配置文件以外的其他参数),在 <strong>config</strong> 阶段会将 <code>parameter</code>(<em>init</em> 阶段解析的配置文件里所有的参数)和<em>init</em> 阶段初始化好的<code>StreamExecutionEnvironment</code>对象传给开发者,
这样开发者就可以配置更多的参数</p>
<div><p>提示</p>
<p><strong>config</strong> 阶段是需要开发者参与的阶段,是可选的阶段</p>
</div>
<h3 id="生命周期之-ready"> 生命周期之—— ready</h3>
<p><strong>ready</strong> 阶段是在参数都设置完毕了,给开发者提供的一个用于做其他动作的入口, 该阶段是在<strong>初始化完成之后</strong>在<strong>程序启动之前</strong>进行</p>
<div><p>提示</p>
<p><strong>ready</strong> 阶段是需要开发者参与的阶段,是可选的阶段</p>
</div>
<h3 id="生命周期之-handle"> 生命周期之—— handle</h3>
<p><strong>handle</strong> 阶段是接入开发者编写的代码的阶段,是开发者编写代码的入口,也是最重要的一个阶段, 这个<code>handle</code> 方法会强制让开发者去实现</p>
<div><p>提示</p>
<p><strong>handle</strong> 阶段是需要开发者参与的阶段,是必须的阶段</p>
</div>
<h3 id="生命周期之-start"> 生命周期之—— start</h3>
<p><strong>start</strong> 阶段,顾名思义,这个阶段会启动任务,由框架自动执行</p>
<h3 id="生命周期之-destroy"> 生命周期之—— destroy</h3>
<p><strong>destroy</strong> 阶段,是程序运行完毕了,在jvm退出之前的最后一个阶段,一般用于收尾的工作</p>
<div><p>提示</p>
<p><strong>destroy</strong> 阶段是需要开发者参与的阶段,是可选的阶段</p>
</div>
<h2 id="目录结构"> 目录结构</h2>
<p>推荐的项目目录结构如下,具体可以参考<a href="https://github.com/streamxhub/streamx/streamx-flink/streamx-flink-quickstart" target="_blank" rel="noopener noreferrer">Streamx-flink-quickstart</a> 里的目录结构和配置</p>
<div><pre><code>.
|── assembly
│    ├── bin
│    │    ├── startup.sh                             //启动脚本  
│    │    ├── setclasspath.sh                        //java环境变量相关的脚本(框架内部使用,开发者无需关注)
│    │    ├── shutdown.sh                            //任务停止脚本(不建议使用)
│    │    └── flink.sh                               //启动时内部使用到的脚本(框架内部使用,开发者无需关注)
│    │── conf                           
│    │    ├── test
│    │    │    ├── application.yaml                  //测试(test)阶段的配置文件
│    │    │    └── sql.yaml                          //flink sql
│    │    │
│    │    ├── prod                      
│    │    │    ├── application.yaml                  //生产(prod)阶段的配置文件
│    │    │    └── sql.yaml                          //flink sql
│    │── logs                                        //logs目录
│    └── temp
│
│── src
│    └── main
│         ├── java 
│         ├── resources
│         └── scala 
│
│── assembly.xml
│
└── pom.xml
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br></div></div><p>assembly.xml 是assembly打包插件需要用到的配置文件,定义如下:</p>
<div><pre><code><span><span><span>&lt;</span>assembly</span><span>></span></span>
    <span><span><span>&lt;</span>id</span><span>></span></span>bin<span><span><span>&lt;/</span>id</span><span>></span></span>
    <span><span><span>&lt;</span>formats</span><span>></span></span>
        <span><span><span>&lt;</span>format</span><span>></span></span>tar.gz<span><span><span>&lt;/</span>format</span><span>></span></span>
    <span><span><span>&lt;/</span>formats</span><span>></span></span>
    <span><span><span>&lt;</span>fileSets</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/bin<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>bin<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>${project.build.directory}<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>lib<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
            <span><span><span>&lt;</span>includes</span><span>></span></span>
                <span><span><span>&lt;</span>include</span><span>></span></span>*.jar<span><span><span>&lt;/</span>include</span><span>></span></span>
            <span><span><span>&lt;/</span>includes</span><span>></span></span>
            <span><span><span>&lt;</span>excludes</span><span>></span></span>
                <span><span><span>&lt;</span>exclude</span><span>></span></span>original-*.jar<span><span><span>&lt;/</span>exclude</span><span>></span></span>
            <span><span><span>&lt;/</span>excludes</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/conf<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>conf<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/logs<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>logs<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
        <span><span><span>&lt;</span>fileSet</span><span>></span></span>
            <span><span><span>&lt;</span>directory</span><span>></span></span>assembly/temp<span><span><span>&lt;/</span>directory</span><span>></span></span>
            <span><span><span>&lt;</span>outputDirectory</span><span>></span></span>temp<span><span><span>&lt;/</span>outputDirectory</span><span>></span></span>
            <span><span><span>&lt;</span>fileMode</span><span>></span></span>0755<span><span><span>&lt;/</span>fileMode</span><span>></span></span>
        <span><span><span>&lt;/</span>fileSet</span><span>></span></span>
    <span><span><span>&lt;/</span>fileSets</span><span>></span></span>
<span><span><span>&lt;/</span>assembly</span><span>></span></span>
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br><span>37</span><br><span>38</span><br><span>39</span><br></div></div><h2 id="打包部署"> 打包部署</h2>
<p>推荐<a href="https://github.com/streamxhub/streamx/streamx-flink/streamx-flink-quickstart" target="_blank" rel="noopener noreferrer">Streamx-flink-quickstart</a>里的打包模式,直接运行<code>maven package</code>即可生成一个标准的StreamX推荐的项目包,解包后目录结构如下</p>
<div><pre><code>.
Streamx-flink-quickstart-1.0.0
├── bin
│   ├── startup.sh                             //启动脚本  
│   ├── setclasspath.sh                        //java环境变量相关的脚本(内部使用的,用户无需关注)
│   ├── shutdown.sh                            //任务停止脚本(不建议使用)
│   ├── flink.sh                               //启动时内部使用到的脚本(内部使用的,用户无需关注)
├── conf                           
│   ├── application.yaml                       //项目的配置文件
│   ├── sql.yaml                               // flink sql文件
├── lib
│   └── Streamx-flink-quickstart-1.0.0.jar     //项目的jar包
└── temp
</code></pre>
<div><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br></div></div><h2 id="启动命令"> 启动命令</h2>
<p>启动之前确定application.yaml和 sql.yaml 配置文件,如果要启动的任务是<code>DataStream</code>任务,直接在startup.sh后跟上配置文件即可</p>
<div><pre><code>bin/startup.sh --conf conf/application.yaml
</code></pre>
<div><span>1</span><br></div></div><p>如果要启动的任务是<code>Flink Sql</code>任务,则需要跟上配置文件和sql.yaml</p>
<div><pre><code>bin/startup.sh --conf conf/application.yaml --sql conf/sql.yaml
</code></pre>
<div><span>1</span><br></div></div>]]></content:encoded>
      <enclosure url="http://www.streamxhub.com/streamx-docs/streamx-docs/assets/img/doc-img/streamx_apis.jpeg" type="image/jpeg"/>
    </item>
    <item>
      <title>开发实例</title>
      <link>http://www.streamxhub.com/streamx-docs/zh/doc/usecase/usecase/</link>
      <guid>http://www.streamxhub.com/streamx-docs/zh/doc/usecase/usecase/</guid>
      <source url="http://www.streamxhub.com/streamx-docs/rss.xml">开发实例</source>
      <pubDate>Wed, 22 Dec 2021 00:45:12 GMT</pubDate>
      <content:encoded><![CDATA[<h3 id="待更新"> ---待更新---</h3>
]]></content:encoded>
    </item>
  </channel>
</rss>